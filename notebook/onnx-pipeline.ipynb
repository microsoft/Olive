{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository shows how to deploy and use ONNX pipeline with dockers including convert model, generate input and performance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull dockers from Azure. It should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sh build.sh # For Linux\n",
    "!build.sh # For Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the onnxpipeline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxpipeline\n",
    "\n",
    "# Initiate ONNX pipeline with local directory \"model\"\n",
    "pipeline = onnxpipeline.Pipeline('model')\n",
    "\n",
    "# onnx\n",
    "#pipeline = onnxpipeline.Pipeline('onnx')\n",
    "\n",
    "# tensorflow\n",
    "#pipeline = onnxpipeline.Pipeline('mnist/model')\n",
    "\n",
    "# pytorch\n",
    "#pipeline = onnxpipeline.Pipeline('pytorch')\n",
    "\n",
    "# cntk \n",
    "#pipeline = onnxpipeline.Pipeline('cntk')\n",
    "\n",
    "# keras\n",
    "#pipeline = onnxpipeline.Pipeline('KerasToONNX')\n",
    "\n",
    "# sklearn\n",
    "#pipeline = onnxpipeline.Pipeline('sklearn')\n",
    "\n",
    "# caffe\n",
    "#pipeline = onnxpipeline.Pipeline('caffe')\n",
    "\n",
    "# current directory\n",
    "#pipeline = onnxpipeline.Pipeline()\n",
    "\n",
    "# test mxnet fail\n",
    "#pipeline = onnxpipeline.Pipeline('mxnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) local_directory: string\n",
    "\n",
    "    Required. The path of local directory where would be mounted to the docker. All operations will be executed from this path.\n",
    "\n",
    "(2) mount_path: string\n",
    "\n",
    "    Optional. The path where the local_directory will be mounted in the docker. Default is \"/mnt/model\".\n",
    "\n",
    "(3) print_logs: boolean\n",
    "\n",
    "    Optional. Whether print the logs from the docker. Default is True.\n",
    "    \n",
    "(4) convert_directory: string\n",
    "\n",
    "    Optional. The directory path for converting model. Default is test/.    \n",
    "\n",
    "(5) convert_name: string\n",
    "\n",
    "    Optional. The model name for converting model. Default is model.onnx.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config information for ONNX pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------config----------------\n",
      "           Container information: <docker.client.DockerClient object at 0x000001B3542EB748>\n",
      " Local directory path for volume: E:\\onnx-pipeline\\notebook/model\n",
      "Volume directory path in dockers: /mnt/model\n",
      "                     Result path: result\n",
      "        Converted directory path: test\n",
      "        Converted model filename: model.onnx\n",
      "            Converted model path: test/model.onnx\n",
      "        Print logs in the docker: True\n"
     ]
    }
   ],
   "source": [
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is used to convert model from major model frameworks to onnx. Supported frameworks are - caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "\n",
    "You can run the docker image with customized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "Model Conversion\r\n",
      "\n",
      "\r\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL INPUT GENERATION(if needed)\r\n",
      "\n",
      "\r\n",
      "\n",
      "Input.pb already exists. Skipping dummy input generation. \r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL CORRECTNESS VERIFICATION\r\n",
      "\n",
      "\r\n",
      "\n",
      "\r\n",
      "\n",
      "Check the ONNX model for validity \r\n",
      "\n",
      "The ONNX model is valid.\r\n",
      "\n",
      "\r\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/model/test/output.json )\r\n",
      "\n",
      "\r\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\r\n",
      "\n",
      " 'correctness_verified': 'SKIPPED',\r\n",
      "\n",
      " 'error_message': '',\r\n",
      "\n",
      " 'input_folder': '/mnt/model/test/test_data_set_0',\r\n",
      "\n",
      " 'output_onnx_path': '/mnt/model/test/model.onnx'}\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = pipeline.convert_model(model='mnist.onnx', model_type='onnx')\n",
    "\n",
    "# test tensorflow\n",
    "#model = pipeline.convert_model(model_type='tensorflow')\n",
    "\n",
    "# test pytorch\n",
    "#model = pipeline.convert_model(model_type='pytorch', model='saved_model.pb', model_input_shapes='(1,3,224,224)')\n",
    "\n",
    "# test cntk\n",
    "#model = pipeline.convert_model(model_type='cntk', model='ResNet50_ImageNet_Caffe.model')\n",
    "\n",
    "# test keras\n",
    "#model = pipeline.convert_model(model_type='keras', model='keras_Average_ImageNet.keras')\n",
    "\n",
    "# test sklearn\n",
    "#model = pipeline.convert_model(model_type='scikit-learn', model='sklearn_svc.joblib', initial_types=(\"float_input\", \"FloatTensorType([1,4])\"))\n",
    "\n",
    "# test caffe\n",
    "#model = pipeline.convert_model(model_type='caffe', model='bvlc_alexnet.caffemodel')\n",
    "\n",
    "# test mxnet\n",
    "#model = pipeline.convert_model(model_type='mxnet', model='resnet.json', model_params='resnet.params', model_input_shapes='(1,3,224,224)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that needs to be converted.\n",
    "    \n",
    "    **IMPORTANT Only support the model path which is under the mounting directory (while initialization by Pipeline()).\n",
    "\n",
    "(2) output_onnx_path: string\n",
    "\n",
    "    Required. The path to store the converted onnx model. Should end with \".onnx\". e.g. output.onnx\n",
    "\n",
    "(3) model_type: string\n",
    "\n",
    "    Required. The name of original model framework. Available types are caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "(4) model_inputs_names: string\n",
    "\n",
    "    (tensorflow) Optional. The model's input names. Required for tensorflow frozen models and checkpoints.\n",
    "\n",
    "(5) model_outputs_names: string\n",
    "\n",
    "    (tensorflow) Optional. The model's output names. Required for tensorflow frozen models checkpoints.\n",
    "\n",
    "(6) model_params: string \n",
    "\n",
    "    (mxnet) Optional. The params of the model if needed.\n",
    "\n",
    "(7) model_input_shapes: list of tuple \n",
    "\n",
    "    (pytorch, mxnet) Optional. List of tuples. The input shape(s) of the model. Each dimension separated by ','.\n",
    "\n",
    "(8) target_opset: String\n",
    "\n",
    "    Optional. Specifies the opset for ONNX, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3. Defaults to '7'.\n",
    "    \n",
    "(9) caffe_model_prototxt: string\n",
    "\n",
    "    (caffe) Optional. The filename of deploy prototxt for the caffe madel. \n",
    "\n",
    "(10) initial_types: tuple (string, string)\n",
    "\n",
    "    (scikit-learn) Optional. A tuple consist two strings. The first is data type and the second is the size of tensor type e.g., ('float_input', 'FloatTensorType([1,4])')\n",
    "\n",
    "(11) input_json: string\n",
    "\n",
    "    Optional. Use JSON file as input parameters.\n",
    "    \n",
    "    **IMPORTANT Only support the path which is under the mounting directory (while initialization by Pipeline()).\n",
    "    \n",
    "(12) model_inputs_names: string\n",
    "\n",
    "    (tensorflow) Optional.\n",
    "    \n",
    "(13) model_outputs_names: string\n",
    "\n",
    "    (tensorflow) Optional.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance test tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run perf_tuning using command python perf_tuning.py [Your model path] [Output path on the docker]. You can use the same arguments as for onnxruntime_pert_test tool, e.g. -m for mode, -e to specify execution provider etc. By default it will try all providers available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0523975\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.61987 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0191098\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.95549 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0180704\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.90352 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0534911\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.67455 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0211078\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.05539 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0505198\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.52599 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0175324\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.87662 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0521924\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.60962 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0198271\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.991355 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0190172\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.95086 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0255277\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.27639 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0212756\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.06378 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0696818\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:3.48409 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0118875\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.594375 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.013423\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.67115 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0681074\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:3.40537 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.014876\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.7438 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.063621\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:3.18105 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0153054\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.76527 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0658572\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:3.29286 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.014825\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.74125 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0128891\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.644455 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0258362\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.29181 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0129193\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.645965 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0602524\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:3.01262 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.017044\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.8522 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0172331\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:0.861655 ms\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32637 ; hostname=f5dd61f60039 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32608 ; hostname=f5dd61f60039 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32656 ; hostname=f5dd61f60039 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32692 ; hostname=f5dd61f60039 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.180148\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:0.900741 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.21655\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:1.08275 ms\n",
      "\n",
      "Cores:  1\n",
      "\n",
      "No GPU found on current device. Cuda and TensorRT performance tuning might not be available. \n",
      "\n",
      "providers  ['mklml', 'cpu_openmp', 'mkldnn', 'mkldnn_openmp', 'cpu', 'tensorrt', 'ngraph', 'cuda']\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_tuning -P -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/f2028979-6380-461a-9569-38ef2473c9ce\n",
      "\n",
      "mklml_parallel_1_threads 2.619875\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_tuning -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/01cc797f-ec1a-4844-bf9a-f5a06ee00d98\n",
      "\n",
      "mklml_1_threads 0.9554900000000001\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d6f3a345-6b74-4a8f-a553-10406719d4e2\n",
      "\n",
      "mklml 0.90352\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e01b46e9-919b-454c-976f-58d20289cf09\n",
      "\n",
      "cpu_openmp_parallel_1_threads_OMP_WAIT_POLICY_active 2.674555\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/2c37d398-05c0-4a9b-bf5e-77b19f59f517\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_active 1.05539\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/687cf1c6-91ea-4fc4-a497-deb2b4ff63a8\n",
      "\n",
      "cpu_openmp_parallel_1_threads_OMP_WAIT_POLICY_passive 2.52599\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/f51f71b3-1bc2-4a4b-870b-054d8de161c4\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_passive 0.8766200000000001\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/550a7ebe-b9cb-49b7-8e5e-0561947cc6da\n",
      "\n",
      "cpu_openmp_parallel_1_threads 2.60962\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/fd37394f-3e3e-43a0-9d28-8e0adcfd001d\n",
      "\n",
      "cpu_openmp_1_threads 0.991355\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/8e480ab6-2668-426d-b4b3-8a4631d2ec34\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.95086\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/385b0da2-128f-43c3-91f0-5f43b62db67e\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 1.276385\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/0936e4c7-76db-43c1-a294-31c2578af8bf\n",
      "\n",
      "cpu_openmp 1.0637800000000002\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -P -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/75894cca-846c-4bac-ada2-9ae5b0d118b4\n",
      "\n",
      "mkldnn_parallel_1_threads 3.48409\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/0af828bb-1c7c-4210-a1e2-fae3977c26cb\n",
      "\n",
      "mkldnn_1_threads 0.594375\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/5157e6d6-4e37-46e1-b690-426da3036bf5\n",
      "\n",
      "mkldnn 0.67115\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/c0f86f78-dbc6-4a7c-b2b1-55d7d684027f\n",
      "\n",
      "mkldnn_openmp_parallel_1_threads_OMP_WAIT_POLICY_active 3.40537\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b85ec27e-a525-4daa-989d-802f7d83c377\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active 0.7438\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/16ff8b39-d928-497d-95d9-4f969774efba\n",
      "\n",
      "mkldnn_openmp_parallel_1_threads_OMP_WAIT_POLICY_passive 3.18105\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/dade4860-01e0-4c17-93d9-e30ac39a9281\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_passive 0.76527\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -P -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/6bbfe707-49d1-432e-8235-cb9791668ecc\n",
      "\n",
      "mkldnn_openmp_parallel_1_threads 3.2928599999999997\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/4dda93e8-d288-4351-b263-f9920f368de9\n",
      "\n",
      "mkldnn_openmp_1_threads 0.7412500000000001\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/69532ad9-7339-48b7-a599-12749ccd0d36\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_active 0.644455\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/010a8bfd-d781-4122-bddf-fc030a6b1faf\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_passive 1.29181\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7f868621-f6c9-42b1-9e10-7fceb1d73092\n",
      "\n",
      "mkldnn_openmp 0.645965\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -P -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/24f4ca53-3257-49c7-b019-86b58efd1908\n",
      "\n",
      "cpu_parallel_1_threads 3.01262\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/479c01c7-9f39-4283-8c24-f384065505ce\n",
      "\n",
      "cpu_1_threads 0.8522\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/2e63a6f7-594c-48a0-b56c-9e090d727c62\n",
      "\n",
      "cpu 0.8616550000000001\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e tensorrt -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/4216436b-d49e-453a-90eb-ca710f361552\n",
      "\n",
      "tensorrt_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e tensorrt -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/ec354273-70aa-4a17-a0af-b19659e1e6eb\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_tuning -e ngraph -P -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/dc262eb3-cc03-4b1d-ba8f-a8e2da7a306e\n",
      "\n",
      "ngraph_parallel_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_tuning -e ngraph -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/730eb238-cc58-4e1a-886c-c42e2dc814f0\n",
      "\n",
      "ngraph_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_tuning -e ngraph -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/1eae5faf-50f9-448a-b02e-9b26a3ec42e4\n",
      "\n",
      "ngraph None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e cuda -x 1 -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/986aa192-db8c-4a88-b67a-e6b86de086f5\n",
      "\n",
      "cuda_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e cuda -o 3 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/36cc772f-844d-4d1b-a99a-bb4a8b7cd343\n",
      "\n",
      "cuda None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_tuning -o 3 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/b9f62370-b3c1-4d0c-879c-47bb1647f5c0\n",
      "\n",
      "mklml 0.9007419999999999\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -o 3 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/8b686e09-33bb-4bda-9a3f-fc507f595924\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_passive 1.0827479999999998Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.141779\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:0.708894 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.146149\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:0.730747 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.192491\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:0.962453 ms\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -x 1 -o 3 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/0dc072e1-8cd3-4e2d-9c52-c6ec3bc122a6\n",
      "\n",
      "mkldnn_1_threads 0.7088939999999999\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -e mkldnn -o 3 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/d231523e-0633-43d1-84bf-e6d9b3c684d9\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_active 0.730747\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_tuning -x 1 -o 3 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/342413bd-4b68-4649-9858-881054337737\n",
      "\n",
      "cpu_1_threads 0.962453\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "mkldnn_1_threads 0.7088939999999999 ms\n",
      "\n",
      "mkldnn 0.67115 ms\n",
      "\n",
      "mkldnn_parallel_1_threads 3.48409 ms\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_active 0.730747 ms\n",
      "\n",
      "mkldnn_openmp 0.645965 ms\n",
      "\n",
      "mkldnn_openmp_1_threads 0.7412500000000001 ms\n",
      "\n",
      "mklml 0.9007419999999999 ms\n",
      "\n",
      "mklml_1_threads 0.9554900000000001 ms\n",
      "\n",
      "mklml_parallel_1_threads 2.619875 ms\n",
      "\n",
      "cpu_1_threads 0.962453 ms\n",
      "\n",
      "cpu 0.8616550000000001 ms\n",
      "\n",
      "cpu_parallel_1_threads 3.01262 ms\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_passive 1.0827479999999998 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.95086 ms\n",
      "\n",
      "cpu_openmp_1_threads 0.991355 ms\n",
      "\n",
      "ngraph_parallel_1_threads error\n",
      "\n",
      "ngraph_1_threads error\n",
      "\n",
      "ngraph error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.perf_tuning(model=model)\n",
    "#result = pipeline.perf_tuning()   # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that wants to be performed.\n",
    "    \n",
    "(2) result: string\n",
    "\n",
    "    Optional. The path of the result.\n",
    "    \n",
    "(3) config: string (choices=[\"Debug\", \"MinSizeRel\", \"Release\", \"RelWithDebInfo\"])\n",
    "\n",
    "    Optional. Configuration to run. Default is \"RelWithDebInfo\".\n",
    "    \n",
    "(4) mode: string (choices=[\"duration\", \"times\"])\n",
    "\n",
    "    Optional. Specifies the test mode. Value could be 'duration' or 'times'. Default is \"times\".\n",
    "\n",
    "(5) execution_provider: string (choices=[\"cpu\", \"cuda\", \"mkldnn\"])\n",
    "\n",
    "    Optional. help=\"Specifies the provider 'cpu','cuda','mkldnn'. Default is ''.\n",
    "    \n",
    "(6) repeated_times: integer\n",
    "\n",
    "    Optional. Specifies the repeated times if running in 'times' test mode. Default:20.\n",
    "    \n",
    "(7) duration_times: integer\n",
    "\n",
    "    Optional. Specifies the seconds to run for 'duration' mode. Default:10.\n",
    "    \n",
    "(8) parallel: boolean\n",
    "\n",
    "    Optional. Use parallel executor, default (without -x): sequential executor.\n",
    "    \n",
    "(9) threadpool_size: integer\n",
    "\n",
    "    Optional. Threadpool size if parallel executor (--parallel) is enabled. Default is the number of cores.\n",
    "    \n",
    "(10) num_threads: integer\n",
    "\n",
    "    Optional. OMP_NUM_THREADS value.\n",
    "    \n",
    "(11) top_n: integer\n",
    "\n",
    "    Optional. Show percentiles for top n runs. Default:5.\n",
    "    \n",
    "(12) runtime: boolean\n",
    "\n",
    "    Optional. Use this boolean flag to enable GPU if you have one.\n",
    "    \n",
    "(13) input_json: string\n",
    "\n",
    "    Optional. Use JSON file as input parameters.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkldnn_1_threads 0.7088939999999999 ms\n",
      "\n",
      "mkldnn 0.67115 ms\n",
      "\n",
      "mkldnn_parallel_1_threads 3.48409 ms\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_active 0.730747 ms\n",
      "\n",
      "mkldnn_openmp 0.645965 ms\n",
      "\n",
      "mkldnn_openmp_1_threads 0.7412500000000001 ms\n",
      "\n",
      "mklml 0.9007419999999999 ms\n",
      "\n",
      "mklml_1_threads 0.9554900000000001 ms\n",
      "\n",
      "mklml_parallel_1_threads 2.619875 ms\n",
      "\n",
      "cpu_1_threads 0.962453 ms\n",
      "\n",
      "cpu 0.8616550000000001 ms\n",
      "\n",
      "cpu_parallel_1_threads 3.01262 ms\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_passive 1.0827479999999998 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.95086 ms\n",
      "\n",
      "cpu_openmp_1_threads 0.991355 ms\n",
      "\n",
      "ngraph_parallel_1_threads error\n",
      "\n",
      "ngraph_1_threads error\n",
      "\n",
      "ngraph error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.print_performance(result)\n",
    "#pipeline.print_result() # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performance test, there would be a directory for results. \n",
    "\n",
    "This libray use Pandas.read_json to visualize JSON file. (orient is changeable.)\n",
    "\n",
    "\"latency.json\" contains the raw data of results ordered by the average time. \n",
    "\n",
    "Use .latency to obtain the original latency JSON; Use .profiling to obtain original top 5 profiling JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\onnx-pipeline\\notebook/model/result\n"
     ]
    }
   ],
   "source": [
    "r = pipeline.get_result(result)\n",
    "#r.latency\n",
    "#r.profiling\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print latency.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide parameters for top 5 performace. Use the parameter \"top\" to visualize more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>avg</th>\n",
       "      <th>p90</th>\n",
       "      <th>p95</th>\n",
       "      <th>cpu_usage</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>memory_util</th>\n",
       "      <th>code_snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mkldnn_1_threads</td>\n",
       "      <td>0.708894</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.0281</td>\n",
       "      <td>0.94483</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>OrderedDict([('execution_provider', 'mkldnn'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mkldnn_openmp_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.730747</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>1.0109</td>\n",
       "      <td>0.95975</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29121</td>\n",
       "      <td>OrderedDict([('execution_provider', 'mkldnn_op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mklml</td>\n",
       "      <td>0.900742</td>\n",
       "      <td>1.0600</td>\n",
       "      <td>1.1543</td>\n",
       "      <td>0.91482</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28926</td>\n",
       "      <td>OrderedDict([('execution_provider', 'mklml'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cpu_1_threads</td>\n",
       "      <td>0.962453</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>1.3482</td>\n",
       "      <td>0.89764</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29135</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu'), ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpu_openmp_1_threads_OMP_WAIT_POLICY_passive</td>\n",
       "      <td>1.082748</td>\n",
       "      <td>1.5606</td>\n",
       "      <td>1.6307</td>\n",
       "      <td>0.94206</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29095</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu_openm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           name       avg     p90     p95  \\\n",
       "0                              mkldnn_1_threads  0.708894  0.9633  1.0281   \n",
       "1          mkldnn_openmp_OMP_WAIT_POLICY_active  0.730747  0.9531  1.0109   \n",
       "2                                         mklml  0.900742  1.0600  1.1543   \n",
       "3                                 cpu_1_threads  0.962453  1.1300  1.3482   \n",
       "4  cpu_openmp_1_threads_OMP_WAIT_POLICY_passive  1.082748  1.5606  1.6307   \n",
       "\n",
       "   cpu_usage  gpu_usage  memory_util  \\\n",
       "0    0.94483          0      0.29085   \n",
       "1    0.95975          0      0.29121   \n",
       "2    0.91482          0      0.28926   \n",
       "3    0.89764          0      0.29135   \n",
       "4    0.94206          0      0.29095   \n",
       "\n",
       "                                        code_snippet  \n",
       "0  OrderedDict([('execution_provider', 'mkldnn'),...  \n",
       "1  OrderedDict([('execution_provider', 'mkldnn_op...  \n",
       "2  OrderedDict([('execution_provider', 'mklml'), ...  \n",
       "3  OrderedDict([('execution_provider', 'cpu'), ('...  \n",
       "4  OrderedDict([('execution_provider', 'cpu_openm...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print profiling.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only provide profiling JSON for top 5 performace by giving certain index of the result. The file name is profile_[name].json\n",
    "(1) index: integer\n",
    "    Required. The index for top 5 profiling files.\n",
    "(2) top: integer\n",
    "    The number for top Ops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>pid</th>\n",
       "      <th>tid</th>\n",
       "      <th>dur</th>\n",
       "      <th>ts</th>\n",
       "      <th>ph</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Node</td>\n",
       "      <td>12367</td>\n",
       "      <td>12367</td>\n",
       "      <td>1815</td>\n",
       "      <td>63814</td>\n",
       "      <td>X</td>\n",
       "      <td>sequential/dense/MatMul</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Node</td>\n",
       "      <td>12367</td>\n",
       "      <td>12367</td>\n",
       "      <td>652</td>\n",
       "      <td>73670</td>\n",
       "      <td>X</td>\n",
       "      <td>sequential/dense/MatMul</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Node</td>\n",
       "      <td>12367</td>\n",
       "      <td>12367</td>\n",
       "      <td>614</td>\n",
       "      <td>74898</td>\n",
       "      <td>X</td>\n",
       "      <td>sequential/dense/MatMul</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Node</td>\n",
       "      <td>12367</td>\n",
       "      <td>12367</td>\n",
       "      <td>608</td>\n",
       "      <td>80834</td>\n",
       "      <td>X</td>\n",
       "      <td>sequential/dense/MatMul</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Node</td>\n",
       "      <td>12367</td>\n",
       "      <td>12367</td>\n",
       "      <td>604</td>\n",
       "      <td>82160</td>\n",
       "      <td>X</td>\n",
       "      <td>sequential/dense/MatMul</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat    pid    tid   dur     ts ph                     name  \\\n",
       "0  Node  12367  12367  1815  63814  X  sequential/dense/MatMul   \n",
       "1  Node  12367  12367   652  73670  X  sequential/dense/MatMul   \n",
       "2  Node  12367  12367   614  74898  X  sequential/dense/MatMul   \n",
       "3  Node  12367  12367   608  80834  X  sequential/dense/MatMul   \n",
       "4  Node  12367  12367   604  82160  X  sequential/dense/MatMul   \n",
       "\n",
       "                                                args  \n",
       "0  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "1  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "2  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "3  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "4  {'provider': 'CPUExecutionProvider', 'op_name'...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.print_profiling(index=4, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import onnxruntime as ort\n",
      "so = rt.SessionOptions()\n",
      "so.set_graph_optimization_level(3)\n",
      "so.enable_sequential_execution = False\n",
      "so.session_thread_pool_size(0)\n",
      "session = rt.Session(\"/mnt/model/test/model.onnx\", so)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.get_code(ep='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'model/test/model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b366011cf8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only workable for notebook in the local server \n",
    "import netron\n",
    "netron.start(\"model/test/model.onnx\", browse=False) # 'model.onnx'\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=\"100%\", height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "netron.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
