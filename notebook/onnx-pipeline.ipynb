{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository shows how to deploy and use Onnx pipeline with dockers including convert model, generate input and performance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull dockers from Azure. It should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Error response from daemon: Get https://ziylregistry.azurecr.io/v2/: unauthorized: authentication required\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnx-converter\n",
      "Digest: sha256:43036294bac2bc2c88e5a42ff85a5cd38ac966004b02e4a25ca54f83ca970010\n",
      "Status: Image is up to date for ziylregistry.azurecr.io/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from perf_test\n",
      "Digest: sha256:0b93e6a1d4e4cd5e0057cf503fce53b8702d2445252b7837e844e50752d2a369\n",
      "Status: Image is up to date for ziylregistry.azurecr.io/perf_test:latest\n"
     ]
    }
   ],
   "source": [
    "!sh build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the onnxpipeline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxpipeline\n",
    "\n",
    "# test tensorflow\n",
    "pipeline = onnxpipeline.Pipeline('mnist/model')\n",
    "\n",
    "# test pytorch\n",
    "#pipeline = onnxpipeline.Pipeline('pytorch')\n",
    "\n",
    "# cntk \n",
    "#pipeline = onnxpipeline.Pipeline('cntk')\n",
    "\n",
    "# keras\n",
    "#pipeline = onnxpipeline.Pipeline('KerasToONNX')\n",
    "\n",
    "# sklearn\n",
    "#pipeline = onnxpipeline.Pipeline('sklearn')\n",
    "\n",
    "# caffe\n",
    "#pipeline = onnxpipeline.Pipeline('caffe')\n",
    "\n",
    "# empty\n",
    "#pipeline = onnxpipeline.Pipeline()\n",
    "\n",
    "# test mxnet fail\n",
    "#pipeline = onnxpipeline.Pipeline('mxnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) local_directory: string\n",
    "\n",
    "    Required. The path of local directory where would be mounted to the docker. All operations will be executed from this path.\n",
    "\n",
    "(2) mount_path: string\n",
    "\n",
    "    Optional. The path where the local_directory will be mounted in the docker. Default is \"/mnt/model\".\n",
    "\n",
    "(3) print_logs: boolean\n",
    "\n",
    "    Optional. Whether print the logs from the docker. Default is True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config information for ONNX pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------config----------------\n",
      "           Container information: <docker.client.DockerClient object at 0x7f5e940ca550>\n",
      " Local directory path for volume: /home/chuche/notebook/mnist/model\n",
      "Volume directory path in dockers: /mnt/model\n",
      "                     Result path: result\n",
      "        Converted directory path: test\n",
      "        Converted model filename: model.onnx\n",
      "            Converted model path: test/model.onnx\n",
      "        Print logs in the docker: True\n"
     ]
    }
   ],
   "source": [
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is used to convert model from major model frameworks to onnx. Supported frameworks are - caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "\n",
    "You can run the docker image with customized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-19 00:14:39,321 - INFO - Using tensorflow=1.12.0, onnx=1.5.0, tf2onnx=1.5.1/0c735a\n",
      "\n",
      "2019-06-19 00:14:39,321 - INFO - Using opset <onnx, 7>\n",
      "\n",
      "2019-06-19 00:14:39,446 - INFO - \n",
      "\n",
      "2019-06-19 00:14:39,481 - INFO - Optimizing ONNX model\n",
      "\n",
      "2019-06-19 00:14:39,515 - INFO - After optimization: Add -2 (4->2), Const +1 (12->13), Gather +1 (0->1), Identity -5 (5->0), Transpose -6 (8->2)\n",
      "\n",
      "2019-06-19 00:14:39,519 - INFO - \n",
      "\n",
      "2019-06-19 00:14:39,519 - INFO - Successfully converted TensorFlow model /mnt/model/ to ONNX\n",
      "\n",
      "2019-06-19 00:14:39,539 - INFO - ONNX model is saved at /mnt/model/test/model.onnx\n",
      "\n",
      "2019-06-19 00:14:39.880335: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "Model Conversion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL INPUT GENERATION(if needed)\n",
      "\n",
      "\n",
      "\n",
      "Input.pb already exists. Skipping dummy input generation. \n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CORRECTNESS VERIFICATION\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Check the ONNX model for validity \n",
      "\n",
      "The ONNX model is valid.\n",
      "\n",
      "\n",
      "\n",
      "Check ONNX model for correctness. \n",
      "\n",
      "Running inference on original model with specified or random inputs. \n",
      "\n",
      "...\n",
      "\n",
      "Running inference on the converted model with the same inputs\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Comparing the outputs from two models. \n",
      "\n",
      "The converted model achieves 5-decimal precision compared to the original model.\n",
      "\n",
      "MODEL CONVERSION SUCCESS. \n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/model/test/output.json )\n",
      "\n",
      "\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\n",
      "\n",
      " 'correctness_verified': 'SUCCESS',\n",
      "\n",
      " 'error_message': '',\n",
      "\n",
      " 'input_folder': '/mnt/model/test/test_data_set_0',\n",
      "\n",
      " 'output_onnx_path': '/mnt/model/test/model.onnx'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # test tensorflow\n",
    "model = pipeline.convert_model(model_type='tensorflow')\n",
    "\n",
    "# test pytorch\n",
    "#model = pipeline.convert_model(model_type='pytorch', model='saved_model.pb', input_json='input.json', convert_json=True)\n",
    "\n",
    "# test cntk\n",
    "#model = pipeline.convert_model(model_type='cntk', model='ResNet50_ImageNet_Caffe.model')\n",
    "\n",
    "# test keras\n",
    "#model = pipeline.convert_model(model_type='keras', model='keras_Average_ImageNet.keras', input_json='input.json', convert_json=True)\n",
    "\n",
    "# test sklearn\n",
    "#model = pipeline.convert_model(model_type='scikit-learn', model='sklearn_svc.joblib', initial_types=(\"float_input\", \"FloatTensorType([1,4])\"), input_json='input.json', convert_json=True)\n",
    "\n",
    "# test caffe\n",
    "#model = pipeline.convert_model(model_type='caffe', model='bvlc_alexnet.caffemodel', caffe_model_prototxt ='deploy.prototxt', input_json='input.json', convert_json=True)\n",
    "\n",
    "\n",
    "# test mxnet\n",
    "#model = pipeline.convert_model(model_type='mxnet', model='resnet.json', model_params='resnet.params', model_input_shapes='(1,3,224,224)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that needs to be converted.\n",
    "\n",
    "(2) output_onnx_path: string\n",
    "\n",
    "    Required. The path to store the converted onnx model. Should end with \".onnx\". e.g. output.onnx\n",
    "\n",
    "(3) model_type: string\n",
    "\n",
    "    Required. The name of original model framework. Available types are caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "(4) model_inputs: string\n",
    "\n",
    "    (tensorflow) Optional. The model's input names. Required for tensorflow frozen models and checkpoints.\n",
    "\n",
    "(5) model_outputs: string\n",
    "\n",
    "    (tensorflow) Optional. The model's output names. Required for tensorflow frozen models checkpoints.\n",
    "\n",
    "(6) model_params: string \n",
    "\n",
    "    (mxnet) Optional. The params of the model if needed.\n",
    "\n",
    "(7) model_input_shapes: list of tuple \n",
    "\n",
    "    (pytorch, mxnet) Optional. List of tuples. The input shape(s) of the model. Each dimension separated by ','.\n",
    "\n",
    "(8) target_opset: int\n",
    "\n",
    "    Optional. Specifies the opset for ONNX, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3. Defaults to 7.\n",
    "    \n",
    "(9) caffe_model_prototxt: string\n",
    "\n",
    "    (caffe) Optional. The filename of deploy prototxt for the caffe madel. \n",
    "\n",
    "(10) initial_types: tuple (string, string)\n",
    "\n",
    "    (scikit-learn) Optional. A tuple consist two strings. The first is data type and the second is the size of tensor type e.g., ('float_input', 'FloatTensorType([1,4])')\n",
    "\n",
    "(11) input_json: string\n",
    "\n",
    "    Optional. Use JSON file as input parameters.\n",
    "\n",
    "(12) convert_json: boolean\n",
    "    \n",
    "    Optional. Convert all parameters into JSON file for input parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance test tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run perf_test using command python perf_test.py [Your model path] [Output path on the docker]. You can use the same arguments as for onnxruntime_pert_test tool, e.g. -m for mode, -e to specify execution provider etc. By default it will try all providers available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  6\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/a9b68254-ff04-4eed-bf27-82421701bb29\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_1_passive 0.001039823\n",
      "\n",
      "lower: 1, mid: 2, upper: 3\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/perf_test/perf_test.py\", line 473, in <module>\n",
      "\n",
      "    json_record[\"cpu_usage\"] = test.cpu / 100\n",
      "\n",
      "AttributeError: 'PerfTestParams' object has no attribute 'cpu'\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/3af7c056-165e-4db8-9532-54a7c6881325\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_2_passive 0.000748495\n",
      "\n",
      "current latency:  0.000748495\n",
      "\n",
      "best latency:  0.001039823\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=4\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/3c9d7e98-8f3e-41ff-959d-ccf5d60286b6\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_4_passive 0.000761818\n",
      "\n",
      "lower: 4, mid: 5, upper: 6\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=5\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/d9777395-ea33-454f-b13f-2b509fa76eb9\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_5_passive 0.00075151\n",
      "\n",
      "current latency:  0.00075151\n",
      "\n",
      "best latency:  0.000761818\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/78c13832-6c9a-47cb-9cb3-334fc66f6c20\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_1_active 0.001044866\n",
      "\n",
      "lower: 1, mid: 2, upper: 3\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/6005a0cb-64c6-47c1-a4b4-0de5442f055d\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_2_active 0.000736118\n",
      "\n",
      "current latency:  0.000736118\n",
      "\n",
      "best latency:  0.001044866\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=4\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/214b12aa-a7c4-496d-bc64-adb101e63fbc\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_4_active 0.000578782\n",
      "\n",
      "lower: 4, mid: 5, upper: 6\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=5\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/3a86bbb4-f1cf-46ad-83b2-cf48f30b1d05\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_5_active 0.000574251\n",
      "\n",
      "current latency:  0.000574251\n",
      "\n",
      "best latency:  0.000578782\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/7b7bd96a-8bd5-48df-ba0a-83774360b4e8\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_1_passive 0.001042072\n",
      "\n",
      "lower: 1, mid: 2, upper: 3\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/03ad0952-052d-4359-bed6-39d4757f1a18\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_2_passive 0.000767542\n",
      "\n",
      "current latency:  0.000767542\n",
      "\n",
      "best latency:  0.001042072\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=4\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/5b794d9f-2239-4825-8609-c52e3fddeba1\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_4_passive 0.000740973\n",
      "\n",
      "lower: 4, mid: 5, upper: 6\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "OMP_NUM_THREADS=5\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/bffac9be-88c9-4ef2-8f7d-b9e8087d4643\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_5_passive 0.00073663\n",
      "\n",
      "current latency:  0.00073663\n",
      "\n",
      "best latency:  0.000740973\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/be727a32-9074-417a-93fa-4af8db934976\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_1_active 0.001016257\n",
      "\n",
      "lower: 1, mid: 2, upper: 3\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/cfe1e5c2-55e5-46c8-a7fd-c8f564253bf4\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_2_active 0.000715974\n",
      "\n",
      "current latency:  0.000715974\n",
      "\n",
      "best latency:  0.001016257\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=4\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/c6ce1dd8-ab82-4902-9e17-fd84b000bd6b\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_4_active 0.000579156\n",
      "\n",
      "lower: 4, mid: 5, upper: 6\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=5\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/5c1dd148-f097-443f-a36b-b68ad9dc91cb\n",
      "\n",
      "mkldnn_openmp_OMP_NUM_THREADS_5_active 0.000588691\n",
      "\n",
      "current latency:  0.000588691\n",
      "\n",
      "best latency:  0.000579156\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/tensorrt/onnxruntime_perf_test -e tensorrt -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/11ae6a3a-6307-4df5-85ed-83fcab0163e0\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/09501e4e-4e94-4ebf-91d5-156bb2f5af0d\n",
      "\n",
      "ngraph 0.000432653\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cuda/onnxruntime_perf_test -e cuda -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/14c83dfe-8c37-4237-a318-e6734de8b3a2\n",
      "\n",
      "cuda None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/a3e6adf0-f905-4b7c-b3e0-ab3f7ff314f7\n",
      "\n",
      "cpu_openmp_passive 0.001060006\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/518771fd-3ab9-4b3d-b222-cb6b09c5588e\n",
      "\n",
      "cpu_openmp_active 0.000481423\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/2fa8cc75-1139-4057-8668-37adfb5336ed\n",
      "\n",
      "mkldnn_openmp_passive 0.00168347\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/56371201-b8b8-43e5-87e2-b3fe661786a7\n",
      "\n",
      "mkldnn_openmp_active 0.000539688\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu/onnxruntime_perf_test -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/347c0341-6703-4d59-8c0d-cc7fded3abdf\n",
      "\n",
      "cpu 0.000475104\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn/onnxruntime_perf_test -e mkldnn -m times -r 99 /mnt/model/test/model.onnx /mnt/model/result/2e97ea27-1f22-4775-a73d-25cce9b179ad\n",
      "\n",
      "mkldnn 0.000601623\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -m times -r 100 /mnt/model/test/model.onnx /mnt/model/result/9583aadb-bd09-4800-b525-a37bfc4b1255\n",
      "\n",
      "ngraph 0.000430669\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu/onnxruntime_perf_test -m times -r 100 /mnt/model/test/model.onnx /mnt/model/result/d474a95b-f2c6-433b-a10b-b99c7a259984\n",
      "\n",
      "cpu 0.000508772\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 100 /mnt/model/test/model.onnx /mnt/model/result/08e6ca65-a8d0-48bc-aa65-8964f3a3251e\n",
      "\n",
      "cpu_openmp_active 0.00050408\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 100 /mnt/model/test/model.onnx /mnt/model/result/4267ed9c-0f7b-4a98-a79d-1841221a5e20\n",
      "\n",
      "mkldnn_openmp_active 0.000532876\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=ACTIVE\n",
      "\n",
      "OMP_NUM_THREADS=5\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 100 /mnt/model/test/model.onnx /mnt/model/result/83eb64ce-d3e5-488c-8a27-51a2d42e0af7\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_5_active 0.000598005\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "ngraph 0.000430669 s\n",
      "\n",
      "cpu_openmp_active 0.00050408 s\n",
      "\n",
      "cpu 0.000508772 s\n",
      "\n",
      "mkldnn_openmp_active 0.000532876 s\n",
      "\n",
      "cpu_openmp_OMP_NUM_THREADS_4_active 0.000578782 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.perf_test(model=model, r=99)\n",
    "#result = pipeline.perf_test()   # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that wants to be performed.\n",
    "    \n",
    "(2) result: string\n",
    "\n",
    "    Optional. The path of the result.\n",
    "    \n",
    "(3) config: string (choices=[\"Debug\", \"MinSizeRel\", \"Release\", \"RelWithDebInfo\"])\n",
    "\n",
    "    Optional. Configuration to run. Default is \"RelWithDebInfo\".\n",
    "    \n",
    "(4) m: string (choices=[\"duration\", \"times\"])\n",
    "\n",
    "    Optional. Specifies the test mode. Value could be 'duration' or 'times'. Default is \"times\".\n",
    "\n",
    "(5) e: string (choices=[\"cpu\", \"cuda\", \"mkldnn\"])\n",
    "\n",
    "    Optional. help=\"Specifies the provider 'cpu','cuda','mkldnn'. Default is ''.\n",
    "    \n",
    "(6) r: string\n",
    "\n",
    "    Optional. Specifies the repeated times if running in 'times' test mode. Default:20.\n",
    "    \n",
    "(7) t: string\n",
    "\n",
    "    Optional. Specifies the seconds to run for 'duration' mode. Default:10.\n",
    "    \n",
    "(8) x: string\n",
    "\n",
    "    Optional. Use parallel executor, default (without -x): sequential executor.\n",
    "    \n",
    "(9) n: string\n",
    "\n",
    "    Optional. OMP_NUM_THREADS value.\n",
    "    \n",
    "(10) s: string\n",
    "\n",
    "    Optional. Show percentiles for top n runs. Default:5.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.print_result(result)\n",
    "#pipeline.print_result() # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parameters\n",
    "\n",
    "(1) result: string\n",
    "Optional. The path of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n",
      "Serving 'model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"350\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xecdc6b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only workable for notebook in the local server \n",
    "import netron\n",
    "netron.start(model) # 'model.onnx'\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=700, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
