{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository shows how to deploy and use Onnx pipeline with dockers including convert model, generate input and performance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull dockers from Azure. It should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sh build.sh # For Linux\n",
    "!build.sh # For Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the onnxpipeline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxpipeline\n",
    "\n",
    "pipeline = onnxpipeline.Pipeline('model')\n",
    "\n",
    "# onnx\n",
    "#pipeline = onnxpipeline.Pipeline('onnx')\n",
    "\n",
    "# tensorflow\n",
    "#pipeline = onnxpipeline.Pipeline('mnist/model')\n",
    "\n",
    "# pytorch\n",
    "#pipeline = onnxpipeline.Pipeline('pytorch')\n",
    "\n",
    "# cntk \n",
    "#pipeline = onnxpipeline.Pipeline('cntk')\n",
    "\n",
    "# keras\n",
    "#pipeline = onnxpipeline.Pipeline('KerasToONNX')\n",
    "\n",
    "# sklearn\n",
    "#pipeline = onnxpipeline.Pipeline('sklearn')\n",
    "\n",
    "# caffe\n",
    "#pipeline = onnxpipeline.Pipeline('caffe')\n",
    "\n",
    "# current directory\n",
    "#pipeline = onnxpipeline.Pipeline()\n",
    "\n",
    "# test mxnet fail\n",
    "#pipeline = onnxpipeline.Pipeline('mxnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) local_directory: string\n",
    "\n",
    "    Required. The path of local directory where would be mounted to the docker. All operations will be executed from this path.\n",
    "\n",
    "(2) mount_path: string\n",
    "\n",
    "    Optional. The path where the local_directory will be mounted in the docker. Default is \"/mnt/model\".\n",
    "\n",
    "(3) print_logs: boolean\n",
    "\n",
    "    Optional. Whether print the logs from the docker. Default is True.\n",
    "    \n",
    "(4) convert_directory: string\n",
    "\n",
    "    Optional. The directory path for converting model. Default is test/.    \n",
    "\n",
    "(5) convert_name: string\n",
    "\n",
    "    Optional. The model name for converting model. Default is model.onnx.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config information for ONNX pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------config----------------\n",
      "           Container information: <docker.client.DockerClient object at 0x05E08450>\n",
      " Local directory path for volume: C:\\Users\\t-chuche\\Documents\\GitHub\\onnx-pipeline\\notebook/model\n",
      "Volume directory path in dockers: /mnt/model\n",
      "                     Result path: result\n",
      "        Converted directory path: test\n",
      "        Converted model filename: model.onnx\n",
      "            Converted model path: test/model.onnx\n",
      "        Print logs in the docker: True\n"
     ]
    }
   ],
   "source": [
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is used to convert model from major model frameworks to onnx. Supported frameworks are - caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "\n",
    "You can run the docker image with customized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "Model Conversion\r\n",
      "\n",
      "\r\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL INPUT GENERATION(if needed)\r\n",
      "\n",
      "\r\n",
      "\n",
      "Input.pb already exists. Skipping dummy input generation. \r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL CORRECTNESS VERIFICATION\r\n",
      "\n",
      "\r\n",
      "\n",
      "\r\n",
      "\n",
      "Check the ONNX model for validity \r\n",
      "\n",
      "The ONNX model is valid.\r\n",
      "\n",
      "\r\n",
      "\n",
      "Check ONNX model for correctness. \r\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \r\n",
      "\n",
      "\r\n",
      "\n",
      "-------------\r\n",
      "\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/model/test/output.json )\r\n",
      "\n",
      "\r\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\r\n",
      "\n",
      " 'correctness_verified': 'SKIPPED',\r\n",
      "\n",
      " 'error_message': '',\r\n",
      "\n",
      " 'input_folder': '/mnt/model/test/test_data_set_0',\r\n",
      "\n",
      " 'output_onnx_path': '/mnt/model/test/model.onnx'}\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = pipeline.convert_model(model='mnist.onnx', model_type='onnx')\n",
    "\n",
    "# test tensorflow\n",
    "#model = pipeline.convert_model(model_type='tensorflow')\n",
    "\n",
    "# test pytorch\n",
    "#model = pipeline.convert_model(model_type='pytorch', model='saved_model.pb', model_input_shapes='(1,3,224,224)')\n",
    "\n",
    "# test cntk\n",
    "#model = pipeline.convert_model(model_type='cntk', model='ResNet50_ImageNet_Caffe.model')\n",
    "\n",
    "# test keras\n",
    "#model = pipeline.convert_model(model_type='keras', model='keras_Average_ImageNet.keras')\n",
    "\n",
    "# test sklearn\n",
    "#model = pipeline.convert_model(model_type='scikit-learn', model='sklearn_svc.joblib', initial_types=(\"float_input\", \"FloatTensorType([1,4])\"))\n",
    "\n",
    "# test caffe\n",
    "#model = pipeline.convert_model(model_type='caffe', model='bvlc_alexnet.caffemodel')\n",
    "\n",
    "# test mxnet\n",
    "#model = pipeline.convert_model(model_type='mxnet', model='resnet.json', model_params='resnet.params', model_input_shapes='(1,3,224,224)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that needs to be converted.\n",
    "    \n",
    "    **IMPORTANT Only support the model path which is under the mounting directory (while initialization by Pipeline()).\n",
    "\n",
    "(2) output_onnx_path: string\n",
    "\n",
    "    Required. The path to store the converted onnx model. Should end with \".onnx\". e.g. output.onnx\n",
    "\n",
    "(3) model_type: string\n",
    "\n",
    "    Required. The name of original model framework. Available types are caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "(4) model_inputs_names: string\n",
    "\n",
    "    (tensorflow) Optional. The model's input names. Required for tensorflow frozen models and checkpoints.\n",
    "\n",
    "(5) model_outputs_names: string\n",
    "\n",
    "    (tensorflow) Optional. The model's output names. Required for tensorflow frozen models checkpoints.\n",
    "\n",
    "(6) model_params: string \n",
    "\n",
    "    (mxnet) Optional. The params of the model if needed.\n",
    "\n",
    "(7) model_input_shapes: list of tuple \n",
    "\n",
    "    (pytorch, mxnet) Optional. List of tuples. The input shape(s) of the model. Each dimension separated by ','.\n",
    "\n",
    "(8) target_opset: String\n",
    "\n",
    "    Optional. Specifies the opset for ONNX, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3. Defaults to '7'.\n",
    "    \n",
    "(9) caffe_model_prototxt: string\n",
    "\n",
    "    (caffe) Optional. The filename of deploy prototxt for the caffe madel. \n",
    "\n",
    "(10) initial_types: tuple (string, string)\n",
    "\n",
    "    (scikit-learn) Optional. A tuple consist two strings. The first is data type and the second is the size of tensor type e.g., ('float_input', 'FloatTensorType([1,4])')\n",
    "\n",
    "(11) input_json: string\n",
    "\n",
    "    Optional. Use JSON file as input parameters.\n",
    "    \n",
    "    **IMPORTANT Only support the path which is under the mounting directory (while initialization by Pipeline()).\n",
    "    \n",
    "(12) model_inputs_names: string\n",
    "\n",
    "    (tensorflow) Optional.\n",
    "    \n",
    "(13) model_outputs_names: string\n",
    "\n",
    "    (tensorflow) Optional.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance test tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run perf_test using command python perf_test.py [Your model path] [Output path on the docker]. You can use the same arguments as for onnxruntime_pert_test tool, e.g. -m for mode, -e to specify execution provider etc. By default it will try all providers available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32646 ; hostname=5306725f2124 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0307762\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.53881 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0243878\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.21939 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0238097\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.19049 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.159269\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:7.96343 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0244207\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.22104 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0478194\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.39097 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0381018\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.90509 ms\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.0922342\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:4.61171 ms\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32648 ; hostname=5306725f2124 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mklml/onnxruntime_perf_test: error while loading shared libraries: libmkldnn.so.0: cannot open shared object file: No such file or directory\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32683 ; hostname=5306725f2124 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.759284\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:37.9642 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0485586\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.42793 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.214199\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:10.7099 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.17056\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:8.52799 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0520268\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.60134 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0485211\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:2.42606 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0278393\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.39196 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.0395186\n",
      "\n",
      "Total iterations:20\n",
      "\n",
      "Average time cost:1.97593 ms\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32582 ; hostname=5306725f2124 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mklml/onnxruntime_perf_test: error while loading shared libraries: libmkldnn.so.0: cannot open shared object file: No such file or directory\n",
      "\n",
      "Setting thread pool size to 1\n",
      "\n",
      "Total time cost:0.293462\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:1.46731 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.297648\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:1.48824 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.295176\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:1.47588 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.433033\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:2.16517 ms\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "Total time cost:0.277646\n",
      "\n",
      "Total iterations:200\n",
      "\n",
      "Average time cost:1.38823 ms\n",
      "\n",
      "Cores:  1\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cuda/onnxruntime_perf_test -e cuda -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e2d93036-3be1-43e2-87f1-e832e2432e7f\n",
      "\n",
      "cuda_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/644b29d7-9403-41cd-b3b0-4aec6a015875\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active 0.001195805\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/0019fde7-0357-4ef0-9aaa-7907e831a973\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_passive 0.00137513\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/fac414a7-d17a-4f6f-942e-f94b6e699f42\n",
      "\n",
      "mkldnn_openmp_1_threads 0.00132002\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn/onnxruntime_perf_test -e mkldnn -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/4b745e85-6a50-4aff-a8a6-4d8d275861c5\n",
      "\n",
      "mkldnn_1_threads 0.008280075\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d40f4872-5139-40b9-b8d6-665cdb297ab6\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_active 0.00134276\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/39ee551e-1059-4517-b639-6a6e75ea1f4c\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_passive 0.00297085\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/735c2c63-1be9-4e84-923b-2c8a4518fb0d\n",
      "\n",
      "cpu_openmp_1_threads 0.002347575\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/10088b17-ce35-4b44-9222-2821b5ac4428\n",
      "\n",
      "ngraph_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu/onnxruntime_perf_test -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/0a6ea0ab-7f3e-4bae-8964-ba8b5cfbd753\n",
      "\n",
      "cpu_1_threads 0.001122835\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/tensorrt/onnxruntime_perf_test -e tensorrt -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7d3d343c-791b-490a-93af-6fb264b03fcf\n",
      "\n",
      "tensorrt_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e6ba1957-641a-4266-87ca-731710070fd2\n",
      "\n",
      "mklml_1_threads None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cuda/onnxruntime_perf_test -e cuda -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e3fa3248-229c-42db-ba94-cff031a3ed36\n",
      "\n",
      "cuda None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e1ee1778-e406-4fce-8b73-48763791f12d\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_active 0.05346786\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/a50a3cb8-6c0d-44d4-8af9-033cef9e73a2\n",
      "\n",
      "mkldnn_openmp_OMP_WAIT_POLICY_passive 0.002094445\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -e mkldnn -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/9b3f9e13-1410-42c6-973a-fe1a959ba873\n",
      "\n",
      "mkldnn_openmp 0.008712135\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn/onnxruntime_perf_test -e mkldnn -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b3e1dbcc-abec-4565-acbd-9737ef1f211c\n",
      "\n",
      "mkldnn 0.01117089\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7431ccbf-dde8-4383-af9a-f707618bdaef\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.00139207\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d529f2ba-2890-4e1a-b4e0-ed2203787f07\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 0.0017405\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/12d8794f-0935-4b28-ac5b-51f3ccb06aba\n",
      "\n",
      "cpu_openmp 0.0013846\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/f1f20820-650a-48d1-99e5-60d143604e5a\n",
      "\n",
      "ngraph None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d66083ba-c53a-4692-a542-d97f533a0fa9\n",
      "\n",
      "cpu 0.00196531\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/tensorrt/onnxruntime_perf_test -e tensorrt -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d9b58d77-0db5-4b6f-9018-066dc27984c7\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/c524bcd4-07ce-4209-ba74-166f8ec6900d\n",
      "\n",
      "mklml None\n",
      "\n",
      "\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu/onnxruntime_perf_test -x 1 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/eea3ede5-6aa5-4ff9-a28d-fc3be214a814\n",
      "\n",
      "cpu_1_threads 0.001448245\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/4add89a9-de41-415d-bdc1-9d90bdfceebc\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active 0.001443092\n",
      "\n",
      "\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/77f3cfa8-590d-42a2-bdb5-8f78f039aa88\n",
      "\n",
      "mkldnn_openmp_1_threads 0.001406933\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/cpu_openmp/onnxruntime_perf_test -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/9e23cf82-f19e-4d50-b492-1177d8784d77\n",
      "\n",
      "cpu_openmp_1_threads_OMP_WAIT_POLICY_active 0.001748473\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=1\n",
      "\n",
      "/perf_test/bin/RelWithDebInfo/mkldnn_openmp/onnxruntime_perf_test -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/8a38b798-442e-4b76-8f97-3e2d4cb36ff6\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_passive 0.001520645\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "cpu_openmp 0.0013846 s\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.00139207 s\n",
      "\n",
      "mkldnn_openmp_1_threads 0.001406933 s\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active 0.001443092 s\n",
      "\n",
      "cpu_1_threads 0.001448245 s\n",
      "\n",
      "cuda_1_threads error\n",
      "\n",
      "ngraph_1_threads error\n",
      "\n",
      "tensorrt_1_threads error\n",
      "\n",
      "mklml_1_threads error\n",
      "\n",
      "cuda error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "tensorrt error\n",
      "\n",
      "mklml error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.perf_test(model=model)\n",
    "#result = pipeline.perf_test()   # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "    Required. The path of the model that wants to be performed.\n",
    "    \n",
    "(2) result: string\n",
    "\n",
    "    Optional. The path of the result.\n",
    "    \n",
    "(3) config: string (choices=[\"Debug\", \"MinSizeRel\", \"Release\", \"RelWithDebInfo\"])\n",
    "\n",
    "    Optional. Configuration to run. Default is \"RelWithDebInfo\".\n",
    "    \n",
    "(4) mode: string (choices=[\"duration\", \"times\"])\n",
    "\n",
    "    Optional. Specifies the test mode. Value could be 'duration' or 'times'. Default is \"times\".\n",
    "\n",
    "(5) execution_provider: string (choices=[\"cpu\", \"cuda\", \"mkldnn\"])\n",
    "\n",
    "    Optional. help=\"Specifies the provider 'cpu','cuda','mkldnn'. Default is ''.\n",
    "    \n",
    "(6) repeated_times: integer\n",
    "\n",
    "    Optional. Specifies the repeated times if running in 'times' test mode. Default:20.\n",
    "    \n",
    "(7) duration_times: integer\n",
    "\n",
    "    Optional. Specifies the seconds to run for 'duration' mode. Default:10.\n",
    "    \n",
    "(8) parallel: boolean\n",
    "\n",
    "    Optional. Use parallel executor, default (without -x): sequential executor.\n",
    "    \n",
    "(9) threadpool_size: integer\n",
    "\n",
    "    Optional. Threadpool size if parallel executor (--parallel) is enabled. Default is the number of cores.\n",
    "    \n",
    "(10) num_threads: integer\n",
    "\n",
    "    Optional. OMP_NUM_THREADS value.\n",
    "    \n",
    "(11) top_n: integer\n",
    "\n",
    "    Optional. Show percentiles for top n runs. Default:5.\n",
    "    \n",
    "(12) runtime: boolean\n",
    "\n",
    "    Optional. Use this boolean flag to enable GPU if you have one.\n",
    "    \n",
    "(13) input_json: string\n",
    "\n",
    "    Optional. Use JSON file as input parameters.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_openmp 0.0013846 s\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.00139207 s\n",
      "\n",
      "mkldnn_openmp_1_threads 0.001406933 s\n",
      "\n",
      "mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active 0.001443092 s\n",
      "\n",
      "cpu_1_threads 0.001448245 s\n",
      "\n",
      "cuda_1_threads error\n",
      "\n",
      "ngraph_1_threads error\n",
      "\n",
      "tensorrt_1_threads error\n",
      "\n",
      "mklml_1_threads error\n",
      "\n",
      "cuda error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "tensorrt error\n",
      "\n",
      "mklml error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.print_performance(result)\n",
    "#pipeline.print_result() # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performance test, there would be a directory for results. \n",
    "\n",
    "This libray use Pandas.read_json to visualize JSON file. (orient is changeable.)\n",
    "\n",
    "\"latency.json\" contains the raw data of results ordered by the average time. \n",
    "\n",
    "Use .latency to obtain the original latency JSON; Use .profiling to obtain original top 5 profiling JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-chuche\\Documents\\GitHub\\onnx-pipeline\\notebook/model/result\n"
     ]
    }
   ],
   "source": [
    "r = pipeline.get_result(result)\n",
    "#r.latency\n",
    "#r.profiling\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print latency.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide parameters for top 5 performace. Use the parameter \"top\" to visualize more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>avg</th>\n",
       "      <th>p90</th>\n",
       "      <th>p95</th>\n",
       "      <th>cpu_usage</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>memory_util</th>\n",
       "      <th>code_snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpu_openmp</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>OrderedDict([('execution_provider', ''), ('env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cpu_openmp_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>OrderedDict([('execution_provider', ''), ('env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mkldnn_openmp_1_threads</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.873848</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341467</td>\n",
       "      <td>OrderedDict([('execution_provider', ''), ('env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.865487</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340511</td>\n",
       "      <td>OrderedDict([('execution_provider', ''), ('env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpu_1_threads</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.916762</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340514</td>\n",
       "      <td>OrderedDict([('execution_provider', ''), ('env...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             name       avg       p90  \\\n",
       "0                                      cpu_openmp  0.001385  0.001964   \n",
       "1               cpu_openmp_OMP_WAIT_POLICY_active  0.001392  0.001825   \n",
       "2                         mkldnn_openmp_1_threads  0.001407  0.001725   \n",
       "3  mkldnn_openmp_1_threads_OMP_WAIT_POLICY_active  0.001443  0.001790   \n",
       "4                                   cpu_1_threads  0.001448  0.001811   \n",
       "\n",
       "        p95  cpu_usage  gpu_usage  memory_util  \\\n",
       "0  0.002447   0.000000          0     0.000000   \n",
       "1  0.002018   0.000000          0     0.000000   \n",
       "2  0.001858   0.873848          0     0.341467   \n",
       "3  0.001881   0.865487          0     0.340511   \n",
       "4  0.001947   0.916762          0     0.340514   \n",
       "\n",
       "                                        code_snippet  \n",
       "0  OrderedDict([('execution_provider', ''), ('env...  \n",
       "1  OrderedDict([('execution_provider', ''), ('env...  \n",
       "2  OrderedDict([('execution_provider', ''), ('env...  \n",
       "3  OrderedDict([('execution_provider', ''), ('env...  \n",
       "4  OrderedDict([('execution_provider', ''), ('env...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print profiling.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only provide profiling JSON for top 5 performace by giving certain index of the result. The file name is profile_[name].json\n",
    "(1) index: integer\n",
    "    Required. The index for top 5 profiling files.\n",
    "(2) top: integer\n",
    "    The number for top Ops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dur</th>\n",
       "      <th>name</th>\n",
       "      <th>op_name</th>\n",
       "      <th>ph</th>\n",
       "      <th>pid</th>\n",
       "      <th>provider</th>\n",
       "      <th>tid</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Session</td>\n",
       "      <td>73070</td>\n",
       "      <td>model_loading_uri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Session</td>\n",
       "      <td>6711</td>\n",
       "      <td>session_initialization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>73081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Node</td>\n",
       "      <td>1</td>\n",
       "      <td>sequential/reshape/Shape_fence_before</td>\n",
       "      <td>Shape</td>\n",
       "      <td>X</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>84778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Node</td>\n",
       "      <td>61</td>\n",
       "      <td>sequential/reshape/Shape_kernel_time</td>\n",
       "      <td>Shape</td>\n",
       "      <td>X</td>\n",
       "      <td>136</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>136</td>\n",
       "      <td>84784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Node</td>\n",
       "      <td>0</td>\n",
       "      <td>sequential/reshape/Shape_fence_after</td>\n",
       "      <td>Shape</td>\n",
       "      <td>X</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>84848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat    dur                                   name op_name ph  pid  \\\n",
       "0  Session  73070                      model_loading_uri     NaN  X  136   \n",
       "1  Session   6711                 session_initialization     NaN  X  136   \n",
       "2     Node      1  sequential/reshape/Shape_fence_before   Shape  X  136   \n",
       "3     Node     61   sequential/reshape/Shape_kernel_time   Shape  X  136   \n",
       "4     Node      0   sequential/reshape/Shape_fence_after   Shape  X  136   \n",
       "\n",
       "               provider  tid     ts  \n",
       "0                   NaN  136      3  \n",
       "1                   NaN  136  73081  \n",
       "2                   NaN  136  84778  \n",
       "3  CPUExecutionProvider  136  84784  \n",
       "4                   NaN  136  84848  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.print_profiling(index=4, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get environment_variables in code_snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the index from latency.json, output the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LD_LIBRARY_PATH</th>\n",
       "      <td>/perf_test/bin/RelWithDebInfo/mkldnn_openmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMP_WAIT_POLICY</th>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0\n",
       "LD_LIBRARY_PATH  /perf_test/bin/RelWithDebInfo/mkldnn_openmp\n",
       "OMP_WAIT_POLICY                                       active"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.print_environment(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the index from latency.json, output the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import onnxruntime as ort\n",
      "so = rt.SessionOptions()\n",
      "so.set_graph_optimization_level(2)\n",
      "so.enable_sequential_execution = True\n",
      "so.session_thread_pool_size(0)\n",
      "session = rt.Session(\"/mnt/model/test/model.onnx\", so)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.get_code(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only workable for notebook in the local server \n",
    "import netron\n",
    "netron.start(model) # 'model.onnx'\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=700, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
