# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------
from argparse import ArgumentParser
from pathlib import Path
from typing import TYPE_CHECKING, Dict

from olive.command.base import BaseOliveCLICommand

if TYPE_CHECKING:
    from numpy.typing import NDArray


class ExportAdaptersCommand(BaseOliveCLICommand):
    @staticmethod
    def register_subcommand(parser: ArgumentParser):
        sub_parser = parser.add_parser(
            "export-adapters",
            help=(
                "Export lora adapter weights to a .npz file that will be consumed by ONNX models generated by Olive"
                " ExtractedAdapters pass."
            ),
        )
        sub_parser.add_argument(
            "--adapter_path",
            type=str,
            help="Path to the adapter to export.",
        )
        sub_parser.add_argument(
            "--output_path",
            type=str,
            help="Path to save the exported weights. Will be saved as a .npz file.",
        )
        sub_parser.add_argument(
            "--dtype",
            type=str,
            default="float32",
            choices=["float32", "float16"],
            help="Data type to save the weights as.",
        )
        sub_parser.add_argument(
            "--pack_weights",
            action="store_true",
            help=(
                "Whether to pack the weights. If True, the weights for each module type will be packed into a single"
                " array."
            ),
        )
        sub_parser.set_defaults(func=ExportAdaptersCommand)

    def run(self):
        import numpy as np
        import torch
        from peft import LoraConfig, load_peft_weights

        adapter_weights = load_peft_weights(self.args.adapter_path, device="cpu")

        transformed_weights = {}
        for name, value in adapter_weights.items():
            new_name = name.replace("base_model.model.model", "model")
            # cast to dtype first since some dtypes like bfloat16 are not supported by numpy
            # need to copy since the numpy array is read-only
            transformed_weights[new_name] = value.to(getattr(torch, self.args.dtype)).numpy().transpose().copy()

        if self.args.pack_weights:
            lora_config = LoraConfig.from_pretrained(self.args.adapter_path)
            packed_weights = {}
            for module_type in lora_config.target_modules:
                packed_weights.update(self.pack_weights(transformed_weights, module_type))
            transformed_weights = packed_weights

        output_path = Path(self.args.output_path).with_suffix(".npz")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        np.savez(output_path, **transformed_weights)

        return output_path

    @classmethod
    def pack_weights(cls, weights: Dict[str, "NDArray"], module_type: str) -> Dict[str, "NDArray"]:
        """Pack the weights for a given module type into an array each for lora_A and lora_B."""
        import numpy as np

        packed_weights = {}
        for lora_i in ["lora_A", "lora_B"]:
            matching_modules = sorted(
                [name for name in weights if module_type in name and lora_i in name], key=cls.get_sort_key
            )
            packed_weights[f"{module_type}.{lora_i}.weight.packed"] = np.concatenate(
                [weights[name] for name in matching_modules]
            )

        return packed_weights

    @staticmethod
    def get_sort_key(module_name: str):
        """Get the key to sort the module names by."""
        parts = module_name.split(".")
        for i, part in enumerate(parts):
            try:
                # want the layers to be sorted by the number
                parts[i] = int(part)
            except ValueError:
                pass
        return parts
