{
    "model_decoder_layers": "model.layers",
    "name": "awq",
    "scaling_layers": [
        {
            "inp": "self_attn.q_proj",
            "layers": [
                "self_attn.q_proj",
                "self_attn.k_proj",
                "self_attn.v_proj"
            ],
            "module2inspect": "self_attn",
            "prev_op": "input_layernorm"
        },
        {
            "inp": "self_attn.o_proj",
            "layers": [
                "self_attn.o_proj"
            ],
            "prev_op": "self_attn.v_proj"
        },
        {
            "inp": "mlp.gate_proj",
            "layers": [
                "mlp.gate_proj",
                "mlp.up_proj"
            ],
            "module2inspect": "mlp",
            "prev_op": "pre_feedforward_layernorm"
        },
        {
            "inp": "mlp.down_proj",
            "layers": [
                "mlp.down_proj"
            ],
            "prev_op": "mlp.up_proj"
        }
    ],
    "num_attention_heads": 8,
    "num_key_value_heads": 4
}
