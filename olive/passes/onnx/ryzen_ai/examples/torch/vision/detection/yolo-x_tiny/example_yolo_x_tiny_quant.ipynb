{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO-X Tiny Quant example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the ENV & Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)s] %(message)s',\n",
    "    stream=sys.stdout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: C++ kernel compilation check start.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: C++ kernel build directory /home/haoliang/.cache/torch_extensions/py39_cu124/kernel_ext\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: C++ kernel loading. First-time compilation may take a few minutes...\u001b[0m\n",
      "/group/ossdphi_algo_scratch_06/haoliang/software/anaconda/envs/quark_nvidia/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: C++ kernel compilation is already complete. Ending the C++ kernel compilation check. Total time: 1.1662 seconds\u001b[0m\n",
      "/group/ossdphi_algo_scratch_06/haoliang/software/anaconda/envs/quark_nvidia/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "from trainer import Trainer\n",
    "from yolo_x_tiny_exp import Exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--ckpt\", default=\"./yolox_tiny.pth\", type=str, help=\"pre train checkpoint\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "parser.add_argument('--random_size_range', type=int, default=3, help='random_size')\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"0\", help=\"exp name\")\n",
    "parser.add_argument('--data_dir', default='./coco_data', help='Data set directory.')\n",
    "\n",
    "parser.add_argument(\"--min_lr_ratio\", type=float, default=0.01, help=\"batch size\")\n",
    "parser.add_argument(\"--ema_decay\", type=float, default=0.9995, help=\"ema decay reate.\")\n",
    "\n",
    "parser.add_argument('--output_dir', default='./YOLOX_outputs', help='Experiments results save path.')\n",
    "parser.add_argument('--workers', default=4, type=int, help='Number of data loading workers to be used.')\n",
    "parser.add_argument('--multiscale_range', default=5, type=int, help='multiscale_range.')\n",
    "parser.add_argument(\"--start_epoch\", type=int, default=280, help=\"batch size\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the experiments & trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/trainer.py:81: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "exp = Exp(args)\n",
    "trainer = Trainer(exp, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare FP32 model & test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] args: Namespace(ckpt='./yolox_tiny.pth', batch_size=64, random_size_range=3, experiment_name='0', data_dir='./coco_data', min_lr_ratio=0.01, ema_decay=0.9995, output_dir='./YOLOX_outputs', workers=4, multiscale_range=5, start_epoch=280)\n",
      "[INFO] exp value:\n",
      "╒═══════════════════╤════════════════════════════╕\n",
      "│ keys              │ values                     │\n",
      "╞═══════════════════╪════════════════════════════╡\n",
      "│ seed              │ None                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ output_dir        │ './YOLOX_outputs'          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ print_interval    │ 10                         │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ eval_interval     │ 1                          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ dataset           │ None                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ num_classes       │ 80                         │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ depth             │ 0.33                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ width             │ 0.375                      │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ act               │ 'silu'                     │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ data_num_workers  │ 4                          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ input_size        │ (416, 416)                 │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ multiscale_range  │ 5                          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ random_size       │ (10, 16)                   │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ data_dir          │ './coco_data'              │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ train_ann         │ 'instances_train2017.json' │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ val_ann           │ 'instances_val2017.json'   │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ mosaic_prob       │ 1.0                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ mixup_prob        │ 1.0                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ hsv_prob          │ 1.0                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ flip_prob         │ 0.5                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ degrees           │ 10.0                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ translate         │ 0.1                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ mosaic_scale      │ (0.5, 1.5)                 │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ enable_mixup      │ False                      │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ mixup_scale       │ (0.5, 1.5)                 │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ shear             │ 2.0                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ warmup_epochs     │ 5                          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ max_epoch         │ 300                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ warmup_lr         │ 0                          │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ min_lr_ratio      │ 0.01                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ basic_lr_per_img  │ 0.00015625                 │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ scheduler         │ 'yoloxwarmcos'             │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ no_aug_epochs     │ 15                         │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ ema               │ True                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ weight_decay      │ 0.0005                     │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ momentum          │ 0.9                        │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ save_history_ckpt │ True                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ exp_name          │ 'yolo_x_tiny_0'            │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ test_size         │ (416, 416)                 │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ test_conf         │ 0.01                       │\n",
      "├───────────────────┼────────────────────────────┤\n",
      "│ nmsthre           │ 0.65                       │\n",
      "╘═══════════════════╧════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"args: {}\".format(trainer.args))\n",
    "logging.info(\"exp value:\\n{}\".format(trainer.exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/trainer.py:270: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_file, map_location=self.device)[\"model\"]\n"
     ]
    }
   ],
   "source": [
    "model = trainer.exp.get_model()\n",
    "model.to(trainer.device)\n",
    "model = trainer.load_pretrain_weight(model)\n",
    "trainer.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluator = trainer.exp.get_evaluator(batch_size=int(trainer.args.batch_size / 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the FP32 model on the COCO val dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:17<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Use standard COCOeval.\n",
      "[INFO] Evaluate in main process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/evaluators/coco_evaluator.py:170: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  statistics = torch.cuda.FloatTensor([inference_time, nms_time, n_samples])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=1.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=25.46s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=5.32s).\n"
     ]
    }
   ],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward time: 1.05 ms, Average NMS time: 0.73 ms, Average inference time: 1.78 ms\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.500\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.358\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.281\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.666\n",
      "per class AP:\n",
      "| class         | AP     | class        | AP     | class          | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 45.462 | bicycle      | 22.587 | car            | 29.208 |\n",
      "| motorcycle    | 37.703 | airplane     | 59.774 | bus            | 54.783 |\n",
      "| train         | 62.246 | truck        | 26.632 | boat           | 16.633 |\n",
      "| traffic light | 17.320 | fire hydrant | 54.876 | stop sign      | 57.192 |\n",
      "| parking meter | 37.498 | bench        | 17.594 | bird           | 24.194 |\n",
      "| cat           | 55.498 | dog          | 50.711 | horse          | 49.657 |\n",
      "| sheep         | 39.687 | cow          | 42.653 | elephant       | 58.525 |\n",
      "| bear          | 61.826 | zebra        | 58.177 | giraffe        | 61.879 |\n",
      "| backpack      | 7.966  | umbrella     | 33.595 | handbag        | 7.701  |\n",
      "| tie           | 21.594 | suitcase     | 25.480 | frisbee        | 48.662 |\n",
      "| skis          | 16.839 | snowboard    | 19.329 | sports ball    | 30.541 |\n",
      "| kite          | 33.551 | baseball bat | 17.453 | baseball glove | 28.579 |\n",
      "| skateboard    | 38.373 | surfboard    | 28.050 | tennis racket  | 33.226 |\n",
      "| bottle        | 23.842 | wine glass   | 20.117 | cup            | 26.853 |\n",
      "| fork          | 20.894 | knife        | 8.357  | spoon          | 6.395  |\n",
      "| bowl          | 34.565 | banana       | 23.788 | apple          | 12.621 |\n",
      "| sandwich      | 27.753 | orange       | 23.892 | broccoli       | 18.542 |\n",
      "| carrot        | 16.192 | hot dog      | 29.910 | pizza          | 44.009 |\n",
      "| donut         | 35.706 | cake         | 29.548 | chair          | 21.936 |\n",
      "| couch         | 41.129 | potted plant | 19.646 | bed            | 43.987 |\n",
      "| dining table  | 28.812 | toilet       | 60.130 | tv             | 49.625 |\n",
      "| laptop        | 48.868 | mouse        | 47.888 | remote         | 14.362 |\n",
      "| keyboard      | 42.230 | cell phone   | 23.520 | microwave      | 51.570 |\n",
      "| oven          | 32.114 | toaster      | 23.942 | sink           | 32.347 |\n",
      "| refrigerator  | 50.017 | book         | 9.476  | clock          | 41.509 |\n",
      "| vase          | 25.430 | scissors     | 17.137 | teddy bear     | 39.025 |\n",
      "| hair drier    | 0.000  | toothbrush   | 9.764  |                |        |\n",
      "per class AR:\n",
      "| class         | AR     | class        | AR     | class          | AR     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 53.501 | bicycle      | 34.204 | car            | 40.495 |\n",
      "| motorcycle    | 48.338 | airplane     | 69.441 | bus            | 64.134 |\n",
      "| train         | 70.421 | truck        | 49.734 | boat           | 32.311 |\n",
      "| traffic light | 27.871 | fire hydrant | 59.703 | stop sign      | 61.867 |\n",
      "| parking meter | 40.667 | bench        | 32.530 | bird           | 31.452 |\n",
      "| cat           | 67.673 | dog          | 64.587 | horse          | 56.801 |\n",
      "| sheep         | 53.418 | cow          | 52.769 | elephant       | 68.929 |\n",
      "| bear          | 69.718 | zebra        | 65.338 | giraffe        | 70.216 |\n",
      "| backpack      | 23.315 | umbrella     | 46.437 | handbag        | 23.574 |\n",
      "| tie           | 30.794 | suitcase     | 45.953 | frisbee        | 57.391 |\n",
      "| skis          | 28.008 | snowboard    | 35.652 | sports ball    | 35.154 |\n",
      "| kite          | 45.810 | baseball bat | 27.034 | baseball glove | 37.703 |\n",
      "| skateboard    | 47.709 | surfboard    | 39.101 | tennis racket  | 44.133 |\n",
      "| bottle        | 39.497 | wine glass   | 28.475 | cup            | 42.682 |\n",
      "| fork          | 34.140 | knife        | 20.092 | spoon          | 21.779 |\n",
      "| bowl          | 53.708 | banana       | 40.919 | apple          | 37.076 |\n",
      "| sandwich      | 52.599 | orange       | 48.035 | broccoli       | 44.968 |\n",
      "| carrot        | 37.781 | hot dog      | 38.880 | pizza          | 57.113 |\n",
      "| donut         | 53.171 | cake         | 49.581 | chair          | 40.762 |\n",
      "| couch         | 63.831 | potted plant | 38.947 | bed            | 57.975 |\n",
      "| dining table  | 51.813 | toilet       | 73.073 | tv             | 62.500 |\n",
      "| laptop        | 59.784 | mouse        | 57.358 | remote         | 31.731 |\n",
      "| keyboard      | 57.516 | cell phone   | 37.557 | microwave      | 65.091 |\n",
      "| oven          | 51.748 | toaster      | 51.111 | sink           | 49.156 |\n",
      "| refrigerator  | 63.889 | book         | 24.482 | clock          | 52.884 |\n",
      "| vase          | 44.343 | scissors     | 32.222 | teddy bear     | 50.842 |\n",
      "| hair drier    | 0.000  | toothbrush   | 27.368 |                |        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PTQ & evaluate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Quantization config & Quantizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch import ModelQuantizer, ModelExporter\n",
    "from quark.torch.quantization.config.config import QuantizationSpec, QuantizationConfig, Config\n",
    "from quark.torch.quantization.config.type import Dtype, QSchemeType, ScaleType, RoundType, QuantizationMode\n",
    "from quark.torch.quantization.observer.observer import PerTensorPowOf2MinMSEObserver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Configuration checking start.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Configuration checking end. The configuration is effective. This is weight quantization and activation static quantization.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "INT8_PER_WEIGHT_TENSOR_SPEC = QuantizationSpec(dtype=Dtype.int8,\n",
    "                                                       qscheme=QSchemeType.per_tensor,\n",
    "                                                       observer_cls=PerTensorPowOf2MinMSEObserver,\n",
    "                                                       symmetric=True,\n",
    "                                                       scale_type=ScaleType.float,\n",
    "                                                       round_method=RoundType.half_even,\n",
    "                                                       is_dynamic=False)\n",
    "quant_config = QuantizationConfig(weight=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "                                    input_tensors=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "                                    output_tensors=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "                                    bias=INT8_PER_WEIGHT_TENSOR_SPEC)\n",
    "quant_config = Config(global_quant_config=quant_config, quant_mode=QuantizationMode.fx_graph_mode)\n",
    "trainer.quantizer = ModelQuantizer(quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare calibration Dataset & Fx graph model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_data = [x[0].to(trainer.device) for x in list(itertools.islice(trainer.evaluator.dataloader, 1))]\n",
    "dummy_input = torch.randn(1, 3, *trainer.exp.input_size).to(trainer.device)\n",
    "trainer.model = trainer.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Based on the original YOLO_X Tiny repo code, loss calculation and bounding-boxes decode code are integrated in YOLO_X Tiny `forward`, we modify the code and let the `trainer.model.base_model` only contain the backbone network. We only need to quantize this part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model = torch.export.export_for_training(trainer.model.base_model, (dummy_input, )).module()\n",
    "graph_model = torch.fx.GraphModule(graph_model, graph_model.graph)\n",
    "trainer.model.base_model = graph_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PTQ & evaluate the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Quantizing with the quantization configuration:\n",
      "Config(\n",
      "    global_quant_config=QuantizationConfig(\n",
      "        input_tensors=QuantizationSpec(\n",
      "            dtype=Dtype.int8,\n",
      "            observer_cls=<class 'quark.torch.quantization.observer.observer.PerTensorPowOf2MinMSEObserver'>,\n",
      "            is_dynamic=False,\n",
      "            qscheme=QSchemeType.per_tensor,\n",
      "            ch_axis=None,\n",
      "            group_size=None,\n",
      "            is_mx_scale_constraint=None,\n",
      "            symmetric=True,\n",
      "            round_method=RoundType.half_even,\n",
      "            scale_type=ScaleType.float,\n",
      "            scale_format=None,\n",
      "            scale_calculation_mode=None,\n",
      "            qat_spec=None,\n",
      "            mx_element_dtype=None,\n",
      "            zero_point_type=ZeroPointType.int32,\n",
      "            is_scale_quant=False,\n",
      "        ),\n",
      "        output_tensors=QuantizationSpec(\n",
      "            dtype=Dtype.int8,\n",
      "            observer_cls=<class 'quark.torch.quantization.observer.observer.PerTensorPowOf2MinMSEObserver'>,\n",
      "            is_dynamic=False,\n",
      "            qscheme=QSchemeType.per_tensor,\n",
      "            ch_axis=None,\n",
      "            group_size=None,\n",
      "            is_mx_scale_constraint=None,\n",
      "            symmetric=True,\n",
      "            round_method=RoundType.half_even,\n",
      "            scale_type=ScaleType.float,\n",
      "            scale_format=None,\n",
      "            scale_calculation_mode=None,\n",
      "            qat_spec=None,\n",
      "            mx_element_dtype=None,\n",
      "            zero_point_type=ZeroPointType.int32,\n",
      "            is_scale_quant=False,\n",
      "        ),\n",
      "        weight=QuantizationSpec(\n",
      "            dtype=Dtype.int8,\n",
      "            observer_cls=<class 'quark.torch.quantization.observer.observer.PerTensorPowOf2MinMSEObserver'>,\n",
      "            is_dynamic=False,\n",
      "            qscheme=QSchemeType.per_tensor,\n",
      "            ch_axis=None,\n",
      "            group_size=None,\n",
      "            is_mx_scale_constraint=None,\n",
      "            symmetric=True,\n",
      "            round_method=RoundType.half_even,\n",
      "            scale_type=ScaleType.float,\n",
      "            scale_format=None,\n",
      "            scale_calculation_mode=None,\n",
      "            qat_spec=None,\n",
      "            mx_element_dtype=None,\n",
      "            zero_point_type=ZeroPointType.int32,\n",
      "            is_scale_quant=False,\n",
      "        ),\n",
      "        bias=QuantizationSpec(\n",
      "            dtype=Dtype.int8,\n",
      "            observer_cls=<class 'quark.torch.quantization.observer.observer.PerTensorPowOf2MinMSEObserver'>,\n",
      "            is_dynamic=False,\n",
      "            qscheme=QSchemeType.per_tensor,\n",
      "            ch_axis=None,\n",
      "            group_size=None,\n",
      "            is_mx_scale_constraint=None,\n",
      "            symmetric=True,\n",
      "            round_method=RoundType.half_even,\n",
      "            scale_type=ScaleType.float,\n",
      "            scale_format=None,\n",
      "            scale_calculation_mode=None,\n",
      "            qat_spec=None,\n",
      "            mx_element_dtype=None,\n",
      "            zero_point_type=ZeroPointType.int32,\n",
      "            is_scale_quant=False,\n",
      "        ),\n",
      "        target_device=None,\n",
      "    ),\n",
      "    layer_type_quant_config={},\n",
      "    layer_quant_config={},\n",
      "    kv_cache_quant_config={},\n",
      "    softmax_quant_spec=None,\n",
      "    exclude=[\n",
      "    ],\n",
      "    algo_config=None,\n",
      "    pre_quant_opt_config=[\n",
      "    ],\n",
      "    quant_mode=QuantizationMode.fx_graph_mode,\n",
      "    log_severity_level=1,\n",
      ")\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Lack of specific information of pre-optimization configuration. However, PyTorch version 2.5.0+cu124 detected. Only torch versions between 2.2 and 2.4 support auto generating algorithms configuration.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Pre-quantization optimization start.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Pre-quantization optimization end.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Totally replace op.conv2d->op.bn to QuantizedConvBatchNorm2d count:\t74, found but skip: 0\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Totally replace op.conv2d to QuantConv2d count:\t9, found but skip: 0\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Replace sliu: 74 to x * sigmoid(x)\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 1_th pass SplitQuantModuleCalledOverOnce\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 2_th pass ConvertBn2D2ConvQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 3_th pass ConvertReduceMean2GapQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 4_th pass SplitLargeKernelPoolQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 5_th pass ConvertAdaptiveavgpool2d2Quantadaptiveavgpool2DQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 6_th pass ConverAvgpool2d2QuantAvgPool2dQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 7_th pass ConvertSplit2SliceQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 8_th pass ConvertDeleteRedundantSliceQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 9_th pass ConvertSigmoid2HardSigmoidQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]:  Total replace aten.sigmoid nodes to aten.hardsigmoid: 80.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 10_th pass ConvertSilu2HardswishQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 11_th pass ConvertLeakyReLu2QuantLeakyReLuQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze bn_stats.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Disable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Whether find Droutout: False, change mode to: False\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Calibration start.\u001b[0m\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/group/ossdphi_algo_scratch_06/haoliang/Quark/quark/torch/quantization/observer/observer.py:311: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  zero_point = torch.tensor(quant_min - min_val_neg / scale).to(dtype=torch.int32)\n",
      "100%|██████████| 1/1 [01:20<00:00, 80.43s/it]\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Calibration end.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Model quantization has been completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "quantized_model = trainer.quantizer.quantize_model(graph_model, calib_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model = quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze bn_stats.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Disable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Total find: 447 PerTensorPowOf2MinMSEObserver, clear 447 of thems observed tensors\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Whether find Droutout: False, change mode to: False\u001b[0m\n",
      "100%|██████████| 157/157 [00:33<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Use standard COCOeval.\n",
      "[INFO] Evaluate in main process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/evaluators/coco_evaluator.py:170: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  statistics = torch.cuda.FloatTensor([inference_time, nms_time, n_samples])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=1.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=21.15s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.34s).\n"
     ]
    }
   ],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward time: 4.67 ms, Average NMS time: 0.67 ms, Average inference time: 5.35 ms\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.251\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.428\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.233\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.359\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.154\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.554\n",
      "per class AP:\n",
      "| class         | AP     | class        | AP     | class          | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 37.358 | bicycle      | 17.472 | car            | 23.051 |\n",
      "| motorcycle    | 31.910 | airplane     | 48.350 | bus            | 45.151 |\n",
      "| train         | 54.177 | truck        | 22.153 | boat           | 12.650 |\n",
      "| traffic light | 13.470 | fire hydrant | 48.835 | stop sign      | 49.839 |\n",
      "| parking meter | 31.490 | bench        | 13.023 | bird           | 18.322 |\n",
      "| cat           | 42.997 | dog          | 38.980 | horse          | 41.341 |\n",
      "| sheep         | 22.875 | cow          | 28.682 | elephant       | 45.459 |\n",
      "| bear          | 45.971 | zebra        | 50.502 | giraffe        | 52.927 |\n",
      "| backpack      | 6.311  | umbrella     | 26.609 | handbag        | 5.412  |\n",
      "| tie           | 16.673 | suitcase     | 18.404 | frisbee        | 37.594 |\n",
      "| skis          | 11.085 | snowboard    | 14.257 | sports ball    | 22.341 |\n",
      "| kite          | 24.991 | baseball bat | 13.450 | baseball glove | 24.487 |\n",
      "| skateboard    | 31.679 | surfboard    | 21.272 | tennis racket  | 26.336 |\n",
      "| bottle        | 19.749 | wine glass   | 17.318 | cup            | 21.623 |\n",
      "| fork          | 14.635 | knife        | 6.180  | spoon          | 3.879  |\n",
      "| bowl          | 27.167 | banana       | 16.394 | apple          | 7.197  |\n",
      "| sandwich      | 19.712 | orange       | 18.686 | broccoli       | 11.298 |\n",
      "| carrot        | 11.615 | hot dog      | 22.019 | pizza          | 34.478 |\n",
      "| donut         | 26.062 | cake         | 21.085 | chair          | 16.881 |\n",
      "| couch         | 27.904 | potted plant | 15.381 | bed            | 27.205 |\n",
      "| dining table  | 21.652 | toilet       | 48.632 | tv             | 39.311 |\n",
      "| laptop        | 37.720 | mouse        | 39.992 | remote         | 9.300  |\n",
      "| keyboard      | 32.454 | cell phone   | 18.992 | microwave      | 42.461 |\n",
      "| oven          | 23.714 | toaster      | 5.221  | sink           | 21.187 |\n",
      "| refrigerator  | 33.849 | book         | 6.761  | clock          | 34.373 |\n",
      "| vase          | 19.965 | scissors     | 13.140 | teddy bear     | 31.636 |\n",
      "| hair drier    | 0.000  | toothbrush   | 5.810  |                |        |\n",
      "per class AR:\n",
      "| class         | AR     | class        | AR     | class          | AR     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 45.956 | bicycle      | 29.204 | car            | 34.364 |\n",
      "| motorcycle    | 42.316 | airplane     | 58.182 | bus            | 56.360 |\n",
      "| train         | 62.895 | truck        | 40.870 | boat           | 26.934 |\n",
      "| traffic light | 23.833 | fire hydrant | 54.653 | stop sign      | 55.600 |\n",
      "| parking meter | 37.500 | bench        | 24.136 | bird           | 26.253 |\n",
      "| cat           | 55.396 | dog          | 54.174 | horse          | 49.449 |\n",
      "| sheep         | 46.299 | cow          | 39.812 | elephant       | 58.333 |\n",
      "| bear          | 57.887 | zebra        | 58.083 | giraffe        | 61.422 |\n",
      "| backpack      | 18.329 | umbrella     | 39.042 | handbag        | 17.444 |\n",
      "| tie           | 24.524 | suitcase     | 33.110 | frisbee        | 46.957 |\n",
      "| skis          | 22.116 | snowboard    | 26.087 | sports ball    | 29.846 |\n",
      "| kite          | 35.015 | baseball bat | 23.724 | baseball glove | 31.554 |\n",
      "| skateboard    | 40.782 | surfboard    | 32.210 | tennis racket  | 35.156 |\n",
      "| bottle        | 33.959 | wine glass   | 27.625 | cup            | 34.950 |\n",
      "| fork          | 23.814 | knife        | 14.954 | spoon          | 14.625 |\n",
      "| bowl          | 44.880 | banana       | 33.730 | apple          | 27.669 |\n",
      "| sandwich      | 41.356 | orange       | 36.947 | broccoli       | 32.532 |\n",
      "| carrot        | 27.863 | hot dog      | 31.680 | pizza          | 43.169 |\n",
      "| donut         | 43.598 | cake         | 39.032 | chair          | 33.591 |\n",
      "| couch         | 53.295 | potted plant | 30.117 | bed            | 46.626 |\n",
      "| dining table  | 40.719 | toilet       | 62.626 | tv             | 51.215 |\n",
      "| laptop        | 47.835 | mouse        | 50.943 | remote         | 22.473 |\n",
      "| keyboard      | 47.647 | cell phone   | 31.336 | microwave      | 55.091 |\n",
      "| oven          | 40.699 | toaster      | 21.111 | sink           | 37.289 |\n",
      "| refrigerator  | 50.556 | book         | 17.795 | clock          | 42.772 |\n",
      "| vase          | 35.620 | scissors     | 25.000 | teddy bear     | 43.421 |\n",
      "| hair drier    | 0.000  | toothbrush   | 15.088 |                |        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform QAT based on PTQ results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Based on the PTQ results, we perform the PTQ, through training, and adjust the weight/bias.\n",
    "This can get higher results.\n",
    "2. We adopt the training code from the original YOLO-X Tiny repo, and we train the model from 280 epoch. Based on the development time and our work focused mainly on the Quark Fx QAT tool, we only tried several parameters to perform training. Differently, we using one single GPU to perform training to largely reduce the training complexity. The user can try other hyperparameters to get higher results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataloader & Optimizer etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import ModelEMA\n",
    "from data import DataPrefetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=15.37s)\n",
      "creating index...\n",
      "index created!\n",
      "[INFO] init prefetcher, this might take one minute or less...\n"
     ]
    }
   ],
   "source": [
    "trainer.no_aug = trainer.start_epoch >= trainer.max_epoch - trainer.exp.no_aug_epochs\n",
    "trainer.train_loader = trainer.exp.get_data_loader(batch_size=trainer.args.batch_size,\n",
    "                                                no_aug=trainer.no_aug,\n",
    "                                                cache_img=None)\n",
    "logging.info(\"init prefetcher, this might take one minute or less...\")\n",
    "trainer.prefetcher = DataPrefetcher(trainer.train_loader)\n",
    "\n",
    "trainer.max_iter = len(trainer.train_loader)\n",
    "trainer.lr_scheduler = trainer.exp.get_lr_scheduler(trainer.exp.basic_lr_per_img * trainer.args.batch_size, trainer.max_iter)\n",
    "trainer.optimizer = trainer.exp.get_optimizer(trainer.args.batch_size)\n",
    "#  ------ using ema for better coverage ---\n",
    "if trainer.use_model_ema:\n",
    "    trainer.ema_model = ModelEMA(trainer.model, trainer.args.ema_decay)  # 0.9995\n",
    "    trainer.ema_model.updates = trainer.max_iter * trainer.start_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training to further improve accuracy\n",
    "NOTE: We only training one epoch for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training start...\n",
      "[INFO] ---> start train epoch281\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Training start...\")\n",
    "# logging.info(\"\\n{}\".format(trainer.model))\n",
    "trainer.epoch = 280\n",
    "logging.info(\"---> start train epoch{}\".format(trainer.epoch + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: in function, `train_in_iter`, \n",
    "  1. We close the observer, meaning, during training the scale will not change;\n",
    "  2. Based on experience, we found that during training, we close the `bn` update that can get higher results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable update bn_stats.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Whether find Droutout: False, change mode to: True\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Disable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze bn_stats.\u001b[0m\n",
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/trainer.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.amp_training):\n",
      "/group/ossdphi_algo_scratch_06/haoliang/Quark/examples/torch/vision/detection/yolo-x_tiny/models/yolox.py:336: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/group/ossdphi_algo_scratch_06/haoliang/software/anaconda/envs/quark_nvidia/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: quark_quant::DeQuantStub: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch: 281/300, iter: 10/1849, gpu mem: 8149Mb, mem: 106.9Gb, iter_time: 1.577s, data_time: 0.002s, total_loss: 6.3, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.7, cls_loss: 1.1, lr: 1.078e-04, size: 416, ETA: 16:11:53\n",
      "[INFO] epoch: 281/300, iter: 20/1849, gpu mem: 8149Mb, mem: 106.9Gb, iter_time: 1.400s, data_time: 0.005s, total_loss: 6.2, iou_loss: 2.6, l1_loss: 0.0, conf_loss: 2.5, cls_loss: 1.2, lr: 1.078e-04, size: 384, ETA: 15:16:56\n",
      "[INFO] epoch: 281/300, iter: 30/1849, gpu mem: 8149Mb, mem: 107.1Gb, iter_time: 1.392s, data_time: 0.003s, total_loss: 6.0, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.2, lr: 1.077e-04, size: 352, ETA: 14:56:51\n",
      "[INFO] epoch: 281/300, iter: 40/1849, gpu mem: 8151Mb, mem: 107.2Gb, iter_time: 1.514s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.0, lr: 1.077e-04, size: 416, ETA: 15:05:29\n",
      "[INFO] epoch: 281/300, iter: 50/1849, gpu mem: 12124Mb, mem: 107.2Gb, iter_time: 1.758s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.077e-04, size: 512, ETA: 15:40:38\n",
      "[INFO] epoch: 281/300, iter: 60/1849, gpu mem: 12124Mb, mem: 107.3Gb, iter_time: 1.469s, data_time: 0.002s, total_loss: 6.2, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.3, lr: 1.077e-04, size: 416, ETA: 15:34:17\n",
      "[INFO] epoch: 281/300, iter: 70/1849, gpu mem: 12124Mb, mem: 107.3Gb, iter_time: 1.735s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.0, lr: 1.077e-04, size: 480, ETA: 15:53:04\n",
      "[INFO] epoch: 281/300, iter: 80/1849, gpu mem: 12124Mb, mem: 107.4Gb, iter_time: 1.349s, data_time: 0.002s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.077e-04, size: 384, ETA: 15:37:24\n",
      "[INFO] epoch: 281/300, iter: 90/1849, gpu mem: 12124Mb, mem: 107.4Gb, iter_time: 1.628s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.076e-04, size: 480, ETA: 15:44:16\n",
      "[INFO] epoch: 281/300, iter: 100/1849, gpu mem: 12124Mb, mem: 107.4Gb, iter_time: 1.406s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.076e-04, size: 384, ETA: 15:36:01\n",
      "[INFO] epoch: 281/300, iter: 110/1849, gpu mem: 12124Mb, mem: 107.5Gb, iter_time: 1.557s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.076e-04, size: 448, ETA: 15:37:41\n",
      "[INFO] epoch: 281/300, iter: 120/1849, gpu mem: 12124Mb, mem: 107.4Gb, iter_time: 1.444s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.076e-04, size: 352, ETA: 15:33:15\n",
      "[INFO] epoch: 281/300, iter: 130/1849, gpu mem: 12124Mb, mem: 107.5Gb, iter_time: 1.616s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.076e-04, size: 480, ETA: 15:37:35\n",
      "[INFO] epoch: 281/300, iter: 140/1849, gpu mem: 12124Mb, mem: 107.4Gb, iter_time: 1.750s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.076e-04, size: 512, ETA: 15:47:06\n",
      "[INFO] epoch: 281/300, iter: 150/1849, gpu mem: 12124Mb, mem: 107.6Gb, iter_time: 1.348s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.075e-04, size: 384, ETA: 15:38:54\n",
      "[INFO] epoch: 281/300, iter: 160/1849, gpu mem: 12124Mb, mem: 107.6Gb, iter_time: 1.612s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.2, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.075e-04, size: 480, ETA: 15:41:47\n",
      "[INFO] epoch: 281/300, iter: 170/1849, gpu mem: 12124Mb, mem: 107.7Gb, iter_time: 1.117s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.075e-04, size: 384, ETA: 15:26:27\n",
      "[INFO] epoch: 281/300, iter: 180/1849, gpu mem: 12124Mb, mem: 107.8Gb, iter_time: 0.686s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.075e-04, size: 448, ETA: 14:58:08\n",
      "[INFO] epoch: 281/300, iter: 190/1849, gpu mem: 12124Mb, mem: 107.8Gb, iter_time: 0.759s, data_time: 0.004s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.075e-04, size: 384, ETA: 14:35:06\n",
      "[INFO] epoch: 281/300, iter: 200/1849, gpu mem: 12140Mb, mem: 108.0Gb, iter_time: 0.972s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 0.9, lr: 1.075e-04, size: 512, ETA: 14:20:55\n",
      "[INFO] epoch: 281/300, iter: 210/1849, gpu mem: 12140Mb, mem: 107.9Gb, iter_time: 0.741s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.074e-04, size: 384, ETA: 14:01:20\n",
      "[INFO] epoch: 281/300, iter: 220/1849, gpu mem: 12140Mb, mem: 107.9Gb, iter_time: 0.768s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.074e-04, size: 416, ETA: 13:44:15\n",
      "[INFO] epoch: 281/300, iter: 230/1849, gpu mem: 12140Mb, mem: 108.1Gb, iter_time: 0.883s, data_time: 0.005s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.074e-04, size: 480, ETA: 13:31:42\n",
      "[INFO] epoch: 281/300, iter: 240/1849, gpu mem: 12140Mb, mem: 108.4Gb, iter_time: 0.717s, data_time: 0.004s, total_loss: 5.9, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.074e-04, size: 352, ETA: 13:15:58\n",
      "[INFO] epoch: 281/300, iter: 250/1849, gpu mem: 12140Mb, mem: 108.0Gb, iter_time: 0.718s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.074e-04, size: 352, ETA: 13:01:30\n",
      "[INFO] epoch: 281/300, iter: 260/1849, gpu mem: 12140Mb, mem: 108.4Gb, iter_time: 0.975s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.074e-04, size: 512, ETA: 12:54:11\n",
      "[INFO] epoch: 281/300, iter: 270/1849, gpu mem: 12140Mb, mem: 108.0Gb, iter_time: 0.835s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.073e-04, size: 480, ETA: 12:44:14\n",
      "[INFO] epoch: 281/300, iter: 280/1849, gpu mem: 12140Mb, mem: 108.1Gb, iter_time: 0.792s, data_time: 0.003s, total_loss: 6.0, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.3, lr: 1.073e-04, size: 448, ETA: 12:34:02\n",
      "[INFO] epoch: 281/300, iter: 290/1849, gpu mem: 12140Mb, mem: 108.3Gb, iter_time: 0.721s, data_time: 0.005s, total_loss: 6.0, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.2, lr: 1.073e-04, size: 320, ETA: 12:23:02\n",
      "[INFO] epoch: 281/300, iter: 300/1849, gpu mem: 12140Mb, mem: 108.2Gb, iter_time: 0.813s, data_time: 0.008s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.073e-04, size: 448, ETA: 12:14:38\n",
      "[INFO] epoch: 281/300, iter: 310/1849, gpu mem: 12140Mb, mem: 108.1Gb, iter_time: 0.731s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.2, lr: 1.073e-04, size: 352, ETA: 12:05:09\n",
      "[INFO] epoch: 281/300, iter: 320/1849, gpu mem: 12140Mb, mem: 108.0Gb, iter_time: 0.718s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.073e-04, size: 384, ETA: 11:56:00\n",
      "[INFO] epoch: 281/300, iter: 330/1849, gpu mem: 12140Mb, mem: 100.7Gb, iter_time: 0.652s, data_time: 0.003s, total_loss: 6.0, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.072e-04, size: 416, ETA: 11:46:12\n",
      "[INFO] epoch: 281/300, iter: 340/1849, gpu mem: 12140Mb, mem: 100.6Gb, iter_time: 0.621s, data_time: 0.002s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.072e-04, size: 416, ETA: 11:36:24\n",
      "[INFO] epoch: 281/300, iter: 350/1849, gpu mem: 12140Mb, mem: 100.6Gb, iter_time: 0.577s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.072e-04, size: 384, ETA: 11:26:23\n",
      "[INFO] epoch: 281/300, iter: 360/1849, gpu mem: 12140Mb, mem: 101.3Gb, iter_time: 0.626s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.072e-04, size: 320, ETA: 11:17:44\n",
      "[INFO] epoch: 281/300, iter: 370/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.667s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.072e-04, size: 352, ETA: 11:10:14\n",
      "[INFO] epoch: 281/300, iter: 380/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.669s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.072e-04, size: 448, ETA: 11:03:09\n",
      "[INFO] epoch: 281/300, iter: 390/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.562s, data_time: 0.002s, total_loss: 5.6, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.071e-04, size: 352, ETA: 10:54:46\n",
      "[INFO] epoch: 281/300, iter: 400/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.667s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 0.9, lr: 1.071e-04, size: 416, ETA: 10:48:23\n",
      "[INFO] epoch: 281/300, iter: 410/1849, gpu mem: 12140Mb, mem: 100.7Gb, iter_time: 0.611s, data_time: 0.002s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.071e-04, size: 416, ETA: 10:41:29\n",
      "[INFO] epoch: 281/300, iter: 420/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.678s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.071e-04, size: 448, ETA: 10:35:53\n",
      "[INFO] epoch: 281/300, iter: 430/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.692s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.2, lr: 1.071e-04, size: 352, ETA: 10:30:43\n",
      "[INFO] epoch: 281/300, iter: 440/1849, gpu mem: 12140Mb, mem: 101.5Gb, iter_time: 0.648s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.071e-04, size: 352, ETA: 10:25:11\n",
      "[INFO] epoch: 281/300, iter: 450/1849, gpu mem: 12140Mb, mem: 100.7Gb, iter_time: 0.621s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.070e-04, size: 416, ETA: 10:19:32\n",
      "[INFO] epoch: 281/300, iter: 460/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.645s, data_time: 0.002s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.070e-04, size: 416, ETA: 10:14:26\n",
      "[INFO] epoch: 281/300, iter: 470/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.625s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.070e-04, size: 416, ETA: 10:09:17\n",
      "[INFO] epoch: 281/300, iter: 480/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.591s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.070e-04, size: 416, ETA: 10:03:55\n",
      "[INFO] epoch: 281/300, iter: 490/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.573s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.070e-04, size: 416, ETA: 9:58:33\n",
      "[INFO] epoch: 281/300, iter: 500/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.661s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.070e-04, size: 480, ETA: 9:54:27\n",
      "[INFO] epoch: 281/300, iter: 510/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.507s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.070e-04, size: 352, ETA: 9:48:40\n",
      "[INFO] epoch: 281/300, iter: 520/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.478s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.069e-04, size: 320, ETA: 9:42:47\n",
      "[INFO] epoch: 281/300, iter: 530/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.664s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.069e-04, size: 480, ETA: 9:39:14\n",
      "[INFO] epoch: 281/300, iter: 540/1849, gpu mem: 12140Mb, mem: 100.8Gb, iter_time: 0.514s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.069e-04, size: 352, ETA: 9:34:08\n",
      "[INFO] epoch: 281/300, iter: 550/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.649s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.069e-04, size: 480, ETA: 9:30:43\n",
      "[INFO] epoch: 281/300, iter: 560/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.659s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.069e-04, size: 480, ETA: 9:27:31\n",
      "[INFO] epoch: 281/300, iter: 570/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.695s, data_time: 0.004s, total_loss: 5.8, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.069e-04, size: 512, ETA: 9:24:48\n",
      "[INFO] epoch: 281/300, iter: 580/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.703s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.068e-04, size: 512, ETA: 9:22:15\n",
      "[INFO] epoch: 281/300, iter: 590/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.502s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.068e-04, size: 320, ETA: 9:17:44\n",
      "[INFO] epoch: 281/300, iter: 600/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.582s, data_time: 0.002s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.068e-04, size: 416, ETA: 9:14:10\n",
      "[INFO] epoch: 281/300, iter: 610/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.523s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.068e-04, size: 352, ETA: 9:10:08\n",
      "[INFO] epoch: 281/300, iter: 620/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.557s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.068e-04, size: 384, ETA: 9:06:33\n",
      "[INFO] epoch: 281/300, iter: 630/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.559s, data_time: 0.003s, total_loss: 5.3, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.068e-04, size: 384, ETA: 9:03:06\n",
      "[INFO] epoch: 281/300, iter: 640/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.590s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.067e-04, size: 416, ETA: 9:00:03\n",
      "[INFO] epoch: 281/300, iter: 650/1849, gpu mem: 12140Mb, mem: 97.4Gb, iter_time: 0.669s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.067e-04, size: 480, ETA: 8:57:50\n",
      "[INFO] epoch: 281/300, iter: 660/1849, gpu mem: 12140Mb, mem: 97.5Gb, iter_time: 0.664s, data_time: 0.004s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.067e-04, size: 480, ETA: 8:55:38\n",
      "[INFO] epoch: 281/300, iter: 670/1849, gpu mem: 12140Mb, mem: 97.5Gb, iter_time: 0.510s, data_time: 0.003s, total_loss: 6.1, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.3, lr: 1.067e-04, size: 320, ETA: 8:52:06\n",
      "[INFO] epoch: 281/300, iter: 680/1849, gpu mem: 12140Mb, mem: 97.5Gb, iter_time: 0.547s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.067e-04, size: 384, ETA: 8:48:59\n",
      "[INFO] epoch: 281/300, iter: 690/1849, gpu mem: 12140Mb, mem: 97.5Gb, iter_time: 0.549s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.067e-04, size: 384, ETA: 8:45:59\n",
      "[INFO] epoch: 281/300, iter: 700/1849, gpu mem: 12140Mb, mem: 97.5Gb, iter_time: 0.737s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.2, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.067e-04, size: 512, ETA: 8:44:42\n",
      "[INFO] epoch: 281/300, iter: 710/1849, gpu mem: 12140Mb, mem: 97.6Gb, iter_time: 0.743s, data_time: 0.004s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.066e-04, size: 512, ETA: 8:43:29\n",
      "[INFO] epoch: 281/300, iter: 720/1849, gpu mem: 12140Mb, mem: 97.6Gb, iter_time: 0.742s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.066e-04, size: 512, ETA: 8:42:18\n",
      "[INFO] epoch: 281/300, iter: 730/1849, gpu mem: 12140Mb, mem: 97.6Gb, iter_time: 0.522s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.066e-04, size: 352, ETA: 8:39:20\n",
      "[INFO] epoch: 281/300, iter: 740/1849, gpu mem: 12140Mb, mem: 97.6Gb, iter_time: 0.565s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.066e-04, size: 384, ETA: 8:36:47\n",
      "[INFO] epoch: 281/300, iter: 750/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.667s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.066e-04, size: 480, ETA: 8:35:07\n",
      "[INFO] epoch: 281/300, iter: 760/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.655s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.066e-04, size: 480, ETA: 8:33:24\n",
      "[INFO] epoch: 281/300, iter: 770/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.536s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.065e-04, size: 352, ETA: 8:30:48\n",
      "[INFO] epoch: 281/300, iter: 780/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.680s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.065e-04, size: 480, ETA: 8:29:22\n",
      "[INFO] epoch: 281/300, iter: 790/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.545s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.2, lr: 1.065e-04, size: 352, ETA: 8:26:56\n",
      "[INFO] epoch: 281/300, iter: 800/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.749s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.065e-04, size: 512, ETA: 8:26:06\n",
      "[INFO] epoch: 281/300, iter: 810/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.556s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.065e-04, size: 384, ETA: 8:23:51\n",
      "[INFO] epoch: 281/300, iter: 820/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.587s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.065e-04, size: 416, ETA: 8:21:53\n",
      "[INFO] epoch: 281/300, iter: 830/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.653s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.065e-04, size: 480, ETA: 8:20:27\n",
      "[INFO] epoch: 281/300, iter: 840/1849, gpu mem: 12140Mb, mem: 97.7Gb, iter_time: 0.713s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.064e-04, size: 512, ETA: 8:19:28\n",
      "[INFO] epoch: 281/300, iter: 850/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.593s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.064e-04, size: 416, ETA: 8:17:39\n",
      "[INFO] epoch: 281/300, iter: 860/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.648s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.064e-04, size: 480, ETA: 8:16:16\n",
      "[INFO] epoch: 281/300, iter: 870/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.543s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.064e-04, size: 384, ETA: 8:14:10\n",
      "[INFO] epoch: 281/300, iter: 880/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.666s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.064e-04, size: 480, ETA: 8:12:59\n",
      "[INFO] epoch: 281/300, iter: 890/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.520s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.064e-04, size: 352, ETA: 8:10:49\n",
      "[INFO] epoch: 281/300, iter: 900/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.726s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.063e-04, size: 512, ETA: 8:10:05\n",
      "[INFO] epoch: 281/300, iter: 910/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.523s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.063e-04, size: 352, ETA: 8:08:01\n",
      "[INFO] epoch: 281/300, iter: 920/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.707s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.063e-04, size: 512, ETA: 8:07:11\n",
      "[INFO] epoch: 281/300, iter: 930/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.539s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.063e-04, size: 384, ETA: 8:05:18\n",
      "[INFO] epoch: 281/300, iter: 940/1849, gpu mem: 12140Mb, mem: 97.8Gb, iter_time: 0.644s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.063e-04, size: 480, ETA: 8:04:07\n",
      "[INFO] epoch: 281/300, iter: 950/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.546s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.063e-04, size: 384, ETA: 8:02:20\n",
      "[INFO] epoch: 281/300, iter: 960/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.488s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.063e-04, size: 320, ETA: 8:00:14\n",
      "[INFO] epoch: 281/300, iter: 970/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.580s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.062e-04, size: 416, ETA: 7:58:44\n",
      "[INFO] epoch: 281/300, iter: 980/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.617s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.062e-04, size: 448, ETA: 7:57:30\n",
      "[INFO] epoch: 281/300, iter: 990/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.700s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.062e-04, size: 512, ETA: 7:56:47\n",
      "[INFO] epoch: 281/300, iter: 1000/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.513s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.062e-04, size: 352, ETA: 7:54:58\n",
      "[INFO] epoch: 281/300, iter: 1010/1849, gpu mem: 12140Mb, mem: 97.9Gb, iter_time: 0.634s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.062e-04, size: 448, ETA: 7:53:54\n",
      "[INFO] epoch: 281/300, iter: 1020/1849, gpu mem: 12140Mb, mem: 98.0Gb, iter_time: 0.523s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.062e-04, size: 352, ETA: 7:52:12\n",
      "[INFO] epoch: 281/300, iter: 1030/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.934s, data_time: 0.002s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.061e-04, size: 416, ETA: 7:52:55\n",
      "[INFO] epoch: 281/300, iter: 1040/1849, gpu mem: 12140Mb, mem: 98.4Gb, iter_time: 0.617s, data_time: 0.002s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.061e-04, size: 416, ETA: 7:51:47\n",
      "[INFO] epoch: 281/300, iter: 1050/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.726s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.061e-04, size: 512, ETA: 7:51:19\n",
      "[INFO] epoch: 281/300, iter: 1060/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.616s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.061e-04, size: 448, ETA: 7:50:13\n",
      "[INFO] epoch: 281/300, iter: 1070/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.612s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.061e-04, size: 448, ETA: 7:49:07\n",
      "[INFO] epoch: 281/300, iter: 1080/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.536s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.061e-04, size: 384, ETA: 7:47:36\n",
      "[INFO] epoch: 281/300, iter: 1090/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.636s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.061e-04, size: 448, ETA: 7:46:41\n",
      "[INFO] epoch: 281/300, iter: 1100/1849, gpu mem: 12140Mb, mem: 98.4Gb, iter_time: 0.513s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.060e-04, size: 352, ETA: 7:45:06\n",
      "[INFO] epoch: 281/300, iter: 1110/1849, gpu mem: 12140Mb, mem: 98.4Gb, iter_time: 0.613s, data_time: 0.004s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.060e-04, size: 448, ETA: 7:44:05\n",
      "[INFO] epoch: 281/300, iter: 1120/1849, gpu mem: 12140Mb, mem: 98.4Gb, iter_time: 0.574s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.060e-04, size: 416, ETA: 7:42:52\n",
      "[INFO] epoch: 281/300, iter: 1130/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.575s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.060e-04, size: 416, ETA: 7:41:41\n",
      "[INFO] epoch: 281/300, iter: 1140/1849, gpu mem: 12140Mb, mem: 98.3Gb, iter_time: 0.601s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.060e-04, size: 416, ETA: 7:40:39\n",
      "[INFO] epoch: 281/300, iter: 1150/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.550s, data_time: 0.004s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.060e-04, size: 384, ETA: 7:39:23\n",
      "[INFO] epoch: 281/300, iter: 1160/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.594s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.060e-04, size: 416, ETA: 7:38:21\n",
      "[INFO] epoch: 281/300, iter: 1170/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.654s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.059e-04, size: 480, ETA: 7:37:39\n",
      "[INFO] epoch: 281/300, iter: 1180/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.579s, data_time: 0.003s, total_loss: 5.2, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 1.9, cls_loss: 1.0, lr: 1.059e-04, size: 416, ETA: 7:36:34\n",
      "[INFO] epoch: 281/300, iter: 1190/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.650s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.059e-04, size: 480, ETA: 7:35:51\n",
      "[INFO] epoch: 281/300, iter: 1200/1849, gpu mem: 12140Mb, mem: 98.2Gb, iter_time: 0.520s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.6, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.059e-04, size: 352, ETA: 7:34:31\n",
      "[INFO] epoch: 281/300, iter: 1210/1849, gpu mem: 12140Mb, mem: 98.4Gb, iter_time: 0.737s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.059e-04, size: 512, ETA: 7:34:16\n",
      "[INFO] epoch: 281/300, iter: 1220/1849, gpu mem: 12140Mb, mem: 98.6Gb, iter_time: 0.581s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.059e-04, size: 416, ETA: 7:33:15\n",
      "[INFO] epoch: 281/300, iter: 1230/1849, gpu mem: 12140Mb, mem: 99.0Gb, iter_time: 0.485s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.059e-04, size: 320, ETA: 7:31:47\n",
      "[INFO] epoch: 281/300, iter: 1240/1849, gpu mem: 12140Mb, mem: 99.0Gb, iter_time: 0.545s, data_time: 0.003s, total_loss: 5.3, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.058e-04, size: 384, ETA: 7:30:39\n",
      "[INFO] epoch: 281/300, iter: 1250/1849, gpu mem: 12140Mb, mem: 99.1Gb, iter_time: 0.733s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.058e-04, size: 480, ETA: 7:30:24\n",
      "[INFO] epoch: 281/300, iter: 1260/1849, gpu mem: 12140Mb, mem: 99.1Gb, iter_time: 0.728s, data_time: 0.004s, total_loss: 6.0, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.2, lr: 1.058e-04, size: 320, ETA: 7:30:08\n",
      "[INFO] epoch: 281/300, iter: 1270/1849, gpu mem: 12140Mb, mem: 99.1Gb, iter_time: 0.980s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.058e-04, size: 512, ETA: 7:31:04\n",
      "[INFO] epoch: 281/300, iter: 1280/1849, gpu mem: 12140Mb, mem: 99.5Gb, iter_time: 0.807s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.058e-04, size: 416, ETA: 7:31:10\n",
      "[INFO] epoch: 281/300, iter: 1290/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.614s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.058e-04, size: 384, ETA: 7:30:22\n",
      "[INFO] epoch: 281/300, iter: 1300/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.829s, data_time: 0.005s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.058e-04, size: 448, ETA: 7:30:35\n",
      "[INFO] epoch: 281/300, iter: 1310/1849, gpu mem: 12140Mb, mem: 100.9Gb, iter_time: 0.791s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.057e-04, size: 448, ETA: 7:30:36\n",
      "[INFO] epoch: 281/300, iter: 1320/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.783s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.057e-04, size: 416, ETA: 7:30:35\n",
      "[INFO] epoch: 281/300, iter: 1330/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.655s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.057e-04, size: 320, ETA: 7:30:00\n",
      "[INFO] epoch: 281/300, iter: 1340/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.832s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.057e-04, size: 448, ETA: 7:30:12\n",
      "[INFO] epoch: 281/300, iter: 1350/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.795s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.057e-04, size: 416, ETA: 7:30:14\n",
      "[INFO] epoch: 281/300, iter: 1360/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.774s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.057e-04, size: 416, ETA: 7:30:11\n",
      "[INFO] epoch: 281/300, iter: 1370/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.655s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.057e-04, size: 320, ETA: 7:29:36\n",
      "[INFO] epoch: 281/300, iter: 1380/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.945s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.056e-04, size: 512, ETA: 7:30:17\n",
      "[INFO] epoch: 281/300, iter: 1390/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.766s, data_time: 0.003s, total_loss: 5.3, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.056e-04, size: 416, ETA: 7:30:11\n",
      "[INFO] epoch: 281/300, iter: 1400/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.871s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.056e-04, size: 480, ETA: 7:30:32\n",
      "[INFO] epoch: 281/300, iter: 1410/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.869s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.056e-04, size: 480, ETA: 7:30:52\n",
      "[INFO] epoch: 281/300, iter: 1420/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.959s, data_time: 0.004s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.056e-04, size: 512, ETA: 7:31:34\n",
      "[INFO] epoch: 281/300, iter: 1430/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.711s, data_time: 0.004s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.056e-04, size: 384, ETA: 7:31:14\n",
      "[INFO] epoch: 281/300, iter: 1440/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.813s, data_time: 0.004s, total_loss: 5.4, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.056e-04, size: 448, ETA: 7:31:19\n",
      "[INFO] epoch: 281/300, iter: 1450/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.801s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.055e-04, size: 448, ETA: 7:31:21\n",
      "[INFO] epoch: 281/300, iter: 1460/1849, gpu mem: 12140Mb, mem: 101.0Gb, iter_time: 0.714s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.2, lr: 1.055e-04, size: 352, ETA: 7:31:02\n",
      "[INFO] epoch: 281/300, iter: 1470/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.708s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.055e-04, size: 384, ETA: 7:30:41\n",
      "[INFO] epoch: 281/300, iter: 1480/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.722s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.055e-04, size: 384, ETA: 7:30:24\n",
      "[INFO] epoch: 281/300, iter: 1490/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.805s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.055e-04, size: 416, ETA: 7:30:27\n",
      "[INFO] epoch: 281/300, iter: 1500/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.814s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.055e-04, size: 448, ETA: 7:30:31\n",
      "[INFO] epoch: 281/300, iter: 1510/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.678s, data_time: 0.004s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.055e-04, size: 352, ETA: 7:30:04\n",
      "[INFO] epoch: 281/300, iter: 1520/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.972s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.054e-04, size: 512, ETA: 7:30:46\n",
      "[INFO] epoch: 281/300, iter: 1530/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.881s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.054e-04, size: 480, ETA: 7:31:06\n",
      "[INFO] epoch: 281/300, iter: 1540/1849, gpu mem: 12140Mb, mem: 101.1Gb, iter_time: 0.886s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.054e-04, size: 480, ETA: 7:31:26\n",
      "[INFO] epoch: 281/300, iter: 1550/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.649s, data_time: 0.004s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.054e-04, size: 320, ETA: 7:30:52\n",
      "[INFO] epoch: 281/300, iter: 1560/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.782s, data_time: 0.003s, total_loss: 5.9, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.054e-04, size: 416, ETA: 7:30:49\n",
      "[INFO] epoch: 281/300, iter: 1570/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.696s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.054e-04, size: 352, ETA: 7:30:26\n",
      "[INFO] epoch: 281/300, iter: 1580/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.668s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.054e-04, size: 352, ETA: 7:29:57\n",
      "[INFO] epoch: 281/300, iter: 1590/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.761s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.053e-04, size: 416, ETA: 7:29:49\n",
      "[INFO] epoch: 281/300, iter: 1600/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.797s, data_time: 0.004s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.0, lr: 1.053e-04, size: 448, ETA: 7:29:49\n",
      "[INFO] epoch: 281/300, iter: 1610/1849, gpu mem: 12140Mb, mem: 101.2Gb, iter_time: 0.646s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.1, lr: 1.053e-04, size: 320, ETA: 7:29:15\n",
      "[INFO] epoch: 281/300, iter: 1620/1849, gpu mem: 12148Mb, mem: 101.1Gb, iter_time: 0.949s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 0.9, lr: 1.053e-04, size: 512, ETA: 7:29:49\n",
      "[INFO] epoch: 281/300, iter: 1630/1849, gpu mem: 12148Mb, mem: 101.2Gb, iter_time: 0.766s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.0, lr: 1.053e-04, size: 416, ETA: 7:29:42\n",
      "[INFO] epoch: 281/300, iter: 1640/1849, gpu mem: 12148Mb, mem: 101.2Gb, iter_time: 0.631s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.053e-04, size: 320, ETA: 7:29:06\n",
      "[INFO] epoch: 281/300, iter: 1650/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.807s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.053e-04, size: 448, ETA: 7:29:08\n",
      "[INFO] epoch: 281/300, iter: 1660/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.684s, data_time: 0.004s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.052e-04, size: 352, ETA: 7:28:43\n",
      "[INFO] epoch: 281/300, iter: 1670/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.718s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.1, lr: 1.052e-04, size: 384, ETA: 7:28:26\n",
      "[INFO] epoch: 281/300, iter: 1680/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.654s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.052e-04, size: 352, ETA: 7:27:56\n",
      "[INFO] epoch: 281/300, iter: 1690/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.795s, data_time: 0.003s, total_loss: 5.8, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.4, cls_loss: 1.1, lr: 1.052e-04, size: 448, ETA: 7:27:56\n",
      "[INFO] epoch: 281/300, iter: 1700/1849, gpu mem: 12148Mb, mem: 101.2Gb, iter_time: 0.891s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.3, cls_loss: 1.1, lr: 1.052e-04, size: 480, ETA: 7:28:15\n",
      "[INFO] epoch: 281/300, iter: 1710/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.818s, data_time: 0.003s, total_loss: 5.3, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.052e-04, size: 448, ETA: 7:28:19\n",
      "[INFO] epoch: 281/300, iter: 1720/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.932s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.052e-04, size: 512, ETA: 7:28:46\n",
      "[INFO] epoch: 281/300, iter: 1730/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.619s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.051e-04, size: 352, ETA: 7:28:09\n",
      "[INFO] epoch: 281/300, iter: 1740/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.680s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 1.9, cls_loss: 1.0, lr: 1.051e-04, size: 352, ETA: 7:27:44\n",
      "[INFO] epoch: 281/300, iter: 1750/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.659s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.5, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.051e-04, size: 352, ETA: 7:27:16\n",
      "[INFO] epoch: 281/300, iter: 1760/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.806s, data_time: 0.004s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.051e-04, size: 448, ETA: 7:27:17\n",
      "[INFO] epoch: 281/300, iter: 1770/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.800s, data_time: 0.003s, total_loss: 5.6, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.051e-04, size: 448, ETA: 7:27:17\n",
      "[INFO] epoch: 281/300, iter: 1780/1849, gpu mem: 12148Mb, mem: 101.3Gb, iter_time: 0.718s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.1, lr: 1.051e-04, size: 384, ETA: 7:27:01\n",
      "[INFO] epoch: 281/300, iter: 1790/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.865s, data_time: 0.003s, total_loss: 5.3, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.051e-04, size: 480, ETA: 7:27:14\n",
      "[INFO] epoch: 281/300, iter: 1800/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.728s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.0, lr: 1.051e-04, size: 416, ETA: 7:26:59\n",
      "[INFO] epoch: 281/300, iter: 1810/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.820s, data_time: 0.003s, total_loss: 5.4, iou_loss: 2.3, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.050e-04, size: 448, ETA: 7:27:03\n",
      "[INFO] epoch: 281/300, iter: 1820/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.619s, data_time: 0.003s, total_loss: 5.7, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.2, cls_loss: 1.1, lr: 1.050e-04, size: 320, ETA: 7:26:27\n",
      "[INFO] epoch: 281/300, iter: 1830/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.792s, data_time: 0.003s, total_loss: 5.5, iou_loss: 2.4, l1_loss: 0.0, conf_loss: 2.0, cls_loss: 1.1, lr: 1.050e-04, size: 416, ETA: 7:26:25\n",
      "[INFO] epoch: 281/300, iter: 1840/1849, gpu mem: 12148Mb, mem: 101.4Gb, iter_time: 0.997s, data_time: 0.027s, total_loss: 5.4, iou_loss: 2.2, l1_loss: 0.0, conf_loss: 2.1, cls_loss: 1.0, lr: 1.050e-04, size: 512, ETA: 7:27:03\n"
     ]
    }
   ],
   "source": [
    "trainer.train_in_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "To simplify, we directly load the fintuned weight to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619103/1136909203.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trainer.model.load_state_dict(torch.load(\"./YOLOX_outputs/yolo_x_tiny_exp_3031/best_ckpt.pth\")['model'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.load_state_dict(torch.load(\"./YOLOX_outputs/yolo_x_tiny_exp_3031/best_ckpt.pth\")['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze bn_stats.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Disable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Whether find Droutout: False, change mode to: False\u001b[0m\n",
      "100%|██████████| 157/157 [00:31<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Use standard COCOeval.\n",
      "[INFO] Evaluate in main process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=1.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=20.63s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.00s).\n"
     ]
    }
   ],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average forward time: 4.61 ms, Average NMS time: 0.55 ms, Average inference time: 5.16 ms\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.482\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.325\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.469\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.181\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.467\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.618\n",
      "per class AP:\n",
      "| class         | AP     | class        | AP     | class          | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 43.136 | bicycle      | 22.020 | car            | 27.367 |\n",
      "| motorcycle    | 34.753 | airplane     | 56.467 | bus            | 52.879 |\n",
      "| train         | 56.340 | truck        | 25.159 | boat           | 15.253 |\n",
      "| traffic light | 16.424 | fire hydrant | 53.223 | stop sign      | 53.343 |\n",
      "| parking meter | 32.080 | bench        | 16.014 | bird           | 22.307 |\n",
      "| cat           | 52.831 | dog          | 46.520 | horse          | 45.721 |\n",
      "| sheep         | 36.491 | cow          | 38.855 | elephant       | 52.929 |\n",
      "| bear          | 59.907 | zebra        | 55.788 | giraffe        | 60.935 |\n",
      "| backpack      | 7.642  | umbrella     | 31.516 | handbag        | 6.831  |\n",
      "| tie           | 20.119 | suitcase     | 21.526 | frisbee        | 46.387 |\n",
      "| skis          | 17.211 | snowboard    | 18.129 | sports ball    | 26.874 |\n",
      "| kite          | 31.079 | baseball bat | 17.338 | baseball glove | 27.624 |\n",
      "| skateboard    | 35.687 | surfboard    | 26.048 | tennis racket  | 29.963 |\n",
      "| bottle        | 21.483 | wine glass   | 18.687 | cup            | 25.468 |\n",
      "| fork          | 19.301 | knife        | 7.272  | spoon          | 5.139  |\n",
      "| bowl          | 31.969 | banana       | 21.570 | apple          | 11.236 |\n",
      "| sandwich      | 24.803 | orange       | 23.368 | broccoli       | 16.342 |\n",
      "| carrot        | 16.346 | hot dog      | 26.853 | pizza          | 42.475 |\n",
      "| donut         | 35.211 | cake         | 27.985 | chair          | 20.473 |\n",
      "| couch         | 37.991 | potted plant | 17.656 | bed            | 39.300 |\n",
      "| dining table  | 27.084 | toilet       | 55.538 | tv             | 48.052 |\n",
      "| laptop        | 46.768 | mouse        | 45.186 | remote         | 11.901 |\n",
      "| keyboard      | 39.890 | cell phone   | 20.609 | microwave      | 43.148 |\n",
      "| oven          | 29.038 | toaster      | 17.533 | sink           | 29.318 |\n",
      "| refrigerator  | 46.459 | book         | 8.118  | clock          | 38.219 |\n",
      "| vase          | 21.929 | scissors     | 15.203 | teddy bear     | 36.759 |\n",
      "| hair drier    | 0.000  | toothbrush   | 8.132  |                |        |\n",
      "per class AR:\n",
      "| class         | AR     | class        | AR     | class          | AR     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 50.789 | bicycle      | 31.497 | car            | 38.217 |\n",
      "| motorcycle    | 44.850 | airplane     | 63.916 | bus            | 61.095 |\n",
      "| train         | 63.947 | truck        | 45.169 | boat           | 29.976 |\n",
      "| traffic light | 25.363 | fire hydrant | 58.020 | stop sign      | 57.067 |\n",
      "| parking meter | 35.667 | bench        | 28.418 | bird           | 30.679 |\n",
      "| cat           | 64.356 | dog          | 57.615 | horse          | 52.978 |\n",
      "| sheep         | 48.729 | cow          | 48.306 | elephant       | 63.651 |\n",
      "| bear          | 68.310 | zebra        | 63.083 | giraffe        | 68.147 |\n",
      "| backpack      | 19.811 | umbrella     | 42.924 | handbag        | 20.111 |\n",
      "| tie           | 28.175 | suitcase     | 38.662 | frisbee        | 53.130 |\n",
      "| skis          | 27.344 | snowboard    | 31.304 | sports ball    | 32.231 |\n",
      "| kite          | 41.927 | baseball bat | 26.483 | baseball glove | 35.270 |\n",
      "| skateboard    | 43.184 | surfboard    | 36.292 | tennis racket  | 39.378 |\n",
      "| bottle        | 35.291 | wine glass   | 27.009 | cup            | 40.447 |\n",
      "| fork          | 29.302 | knife        | 18.308 | spoon          | 18.498 |\n",
      "| bowl          | 47.849 | banana       | 36.297 | apple          | 28.347 |\n",
      "| sandwich      | 44.689 | orange       | 42.982 | broccoli       | 37.660 |\n",
      "| carrot        | 35.562 | hot dog      | 35.040 | pizza          | 53.908 |\n",
      "| donut         | 47.073 | cake         | 44.226 | chair          | 36.962 |\n",
      "| couch         | 57.778 | potted plant | 34.591 | bed            | 53.374 |\n",
      "| dining table  | 44.072 | toilet       | 67.039 | tv             | 58.576 |\n",
      "| laptop        | 57.056 | mouse        | 54.623 | remote         | 25.724 |\n",
      "| keyboard      | 53.268 | cell phone   | 32.748 | microwave      | 55.455 |\n",
      "| oven          | 43.706 | toaster      | 47.778 | sink           | 44.044 |\n",
      "| refrigerator  | 58.333 | book         | 21.311 | clock          | 48.352 |\n",
      "| vase          | 39.708 | scissors     | 26.667 | teddy bear     | 47.105 |\n",
      "| hair drier    | 0.000  | toothbrush   | 14.561 |                |        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze model & export to onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze model\n",
    "For better deployment in the AMD NPU device, we apply several hardware optimizations (e.g. adjust the scale, insert multiply nodes to perform adjustment for hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze bn_stats.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Disable observer.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Enable fake quant.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Whether find Droutout: False, change mode to: False\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze model start.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze quantized torch.fx.GraphModule \u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Only after calibration/training and before convert to Onnx model, can use _post_quant_optimize()\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 1_th pass ConvertClip2ReLUQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 2_th pass ApplyConstrain2ConcatQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_8 input quantizer: fake_quantizer_66 scale change from 0.03125 to 0.0625\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_14 input quantizer: fake_quantizer_233 scale change from 0.03125 to 0.25\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_15 input quantizer: fake_quantizer_235 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_15 input quantizer: fake_quantizer_236 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_16 input quantizer: fake_quantizer_254 scale change from 0.03125 to 0.25\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_17 input quantizer: fake_quantizer_256 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_17 input quantizer: fake_quantizer_257 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_18 input quantizer: fake_quantizer_275 scale change from 0.03125 to 0.25\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_19 input quantizer: fake_quantizer_277 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: cat_19 input quantizer: fake_quantizer_278 scale change from 0.0078125 to 0.03125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: permute not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: node: cat_20 user num != 1 or not followed by quantizer node,                         Please check whether meet the quant demand.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 3_th pass AlignSingleInOutOpScaleQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 4_th pass AlignSingleInOutModuleScaleQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 5_th pass AlignSingleInOutOpScaleQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 6_th pass AdjustShiftReadQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 7_th pass AdjustShiftWriteQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 8_th pass AdjustShiftCutQOPass\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_1_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_3_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_2_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_4_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_5_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_6_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_7_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_9_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_8_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_10_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_11_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_12_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_13_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_14_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_15_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_16_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_17_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_19_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_18_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_20_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_21_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_22_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_23_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_24_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_25_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_26_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_27_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_28_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_29_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_31_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_30_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_32_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_33_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_34_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_35_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_37_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_36_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_38_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_39_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_40_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_41_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_43_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_42_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_44_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_45_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_46_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_59_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_47_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_49_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_48_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_50_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_51_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_52_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_67_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_53_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_55_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_54_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_56_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_57_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_58_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_75_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_63_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_60_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_61_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_62_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_64_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_66_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_65_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_71_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_68_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_69_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_70_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_72_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_74_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_73_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_79_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_76_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_77_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_78_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_80_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_82_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_81_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 9_th pass AdjustShiftBiasQOPass\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_1_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_3_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_2_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_4_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_5_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_6_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_7_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_9_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_8_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_10_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_11_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_12_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_13_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_14_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_15_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_16_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_17_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_19_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_18_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_20_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_21_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_22_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_23_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_24_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_25_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_26_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_27_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_28_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_29_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_31_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_30_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_32_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_33_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_34_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_35_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_37_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_36_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_38_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_39_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_40_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_41_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_43_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_42_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_44_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_45_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_46_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_59_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_47_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_49_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_48_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_50_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_51_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_52_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_67_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_53_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_55_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_54_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_56_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_57_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_58_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_75_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_63_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_60_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_61_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_62_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_64_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_66_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_65_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_71_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_68_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_69_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_70_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_72_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_74_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_73_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_79_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_76_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_77_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_78_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_80_bn_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_82_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: conv2d_81_quantized_module not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 10_th pass AdjustHardSigmoidQOPass\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_1_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_3_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_2_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_4_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_5_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_6_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_7_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_9_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_8_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_10_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_11_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_12_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_13_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_14_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_15_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_16_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_17_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_19_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_18_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_20_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_21_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_22_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_23_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_24_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_25_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_26_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_27_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_28_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_29_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_31_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_30_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_32_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_33_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_34_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_35_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_37_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_36_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_38_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_39_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_40_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_41_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_43_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_42_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_44_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_45_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_46_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_59_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_47_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_49_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_48_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_50_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_51_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_52_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_64_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_53_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_55_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_54_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_56_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_57_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_58_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_69_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_62_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_60_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_61_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_63_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_1_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_1_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_67_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_65_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_66_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_default_66_replaced outputbias quantizer scale change from 0.015625 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_68_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_2_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_2_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_3_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_3_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_72_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_70_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_71_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_default_73_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_4_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_4_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: Node: hardsigmoid_5_replaced not contain quantizer, please check\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Node: hardsigmoid_5_replaced outputbias quantizer scale change from 0.03125 to 0.0078125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 11_th pass AdjustShiftSwishQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Running 12_th pass ConvertHardSigmoidDpuVersionQOPass\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_1_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_3_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_2_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_4_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_5_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_6_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_7_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_9_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_8_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_10_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_11_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_12_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_13_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_14_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_15_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_16_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_17_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_19_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_18_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_20_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_21_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_22_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_23_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_24_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_25_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_26_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_27_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_28_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_29_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_31_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_30_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_32_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_33_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_34_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_35_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_37_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_36_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_38_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_39_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_40_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_41_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_43_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_42_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_44_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_45_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_46_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_59_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_47_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_49_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_48_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_50_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_51_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_52_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_64_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_53_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_55_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_54_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_56_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_57_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_58_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_69_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_62_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_60_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_61_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_63_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_1_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_67_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_65_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_66_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_68_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_2_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_3_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_72_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_70_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_71_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_default_73_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_4_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Found HardSigmoid node: hardsigmoid_5_replacedConvert to DPU version. Mul with scale: 1.0001220703125\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Freeze model end.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freezeded_model = trainer.quantizer.freeze(trainer.model.base_model.eval())\n",
    "trainer.model.base_model = freezeded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch.export.config.config import ExporterConfig, JsonExporterConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Start exporting quantized onnx model ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Quantized onnx model exported to export_onnx/quark_model.onnx successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = ExporterConfig(json_export_config=JsonExporterConfig())\n",
    "exporter = ModelExporter(config=config, export_dir=\"./export_onnx/\")\n",
    "# NOTE for NPU compile, it is better using batch-size = 1 for better compliance\n",
    "example_inputs = (torch.rand(1, 3, 416, 416).to(trainer.device), )\n",
    "exporter.export_onnx_model(trainer.model, example_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplity the Onnx model and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxsim import simplify\n",
    "import onnx\n",
    "quant_model = onnx.load(\"./export_onnx/quark_model.onnx\")\n",
    "model_simp, check = simplify(quant_model)\n",
    "onnx.save_model(model_simp, \"./export_onnx/sample_quark_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `netron` to visualize the model (Optional)\n",
    "```shell\n",
    "$netron  ./export_onnx/sample_quark_model.onnx\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quark_nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
