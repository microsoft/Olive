# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------
import logging
from copy import deepcopy
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import onnx_ir as ir

from olive.common.utils import WeightsFileFormat, save_weights
from olive.hardware.accelerator import AcceleratorSpec
from olive.model import ONNXModelHandler
from olive.model.utils import resolve_onnx_path
from olive.passes import Pass
from olive.passes.onnx.common import (
    DORA_NAME_PATTERNS,
    LOHA_NAME_PATTERNS,
    LORA_NAME_PATTERNS,
    AdapterType,
    get_adapter_name,
    get_external_data_config,
    model_proto_to_olive_model,
)
from olive.passes.pass_config import BasePassConfig, PassConfigParam

if TYPE_CHECKING:
    from numpy.typing import NDArray

logger = logging.getLogger(__name__)


class ExtractAdapters(Pass):
    """Extract adapter weights from ONNX model and save them as external weights file.

    If make_inputs is False, model proto is invalid after this pass as the adapter weights point to non-existent
    external files. Inference session must be created by first loading the adapter weights using
    SessionOptions.add_external_initializers.

    If make_inputs is True, the adapter weights are inputs to the model and must be provided during inference.
    """

    @classmethod
    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> dict[str, PassConfigParam]:
        config = {
            "adapter_type": PassConfigParam(
                type_=AdapterType,
                default_value=AdapterType.LORA,
                description=f"Type of adapter to extract. Valid values are {AdapterType.__members__.values()}.",
            ),
            "make_inputs": PassConfigParam(
                type_=bool,
                default_value=True,
                description=(
                    "Convert adapter weights to inputs. If false, the adapter weights will be set as initializers with"
                    " external data."
                ),
            ),
            "dynamic_lora_r": PassConfigParam(
                type_=bool,
                default_value=True,
                description=(
                    "Whether the model uses dynamic shape for lora_r. Only used if make_inputs is True. Valid only for"
                    " float modules."
                ),
            ),
            "optional_inputs": PassConfigParam(
                type_=bool,
                default_value=True,
                description=(
                    "Create default initializers (empty tensor with lora_r dimension set to 0) for the adapter weights,"
                    " if inputs not provided during inference. Only used if make_inputs is True. Valid only for float"
                    " modules."
                ),
            ),
            "save_format": PassConfigParam(
                type_=WeightsFileFormat,
                default_value=WeightsFileFormat.ONNX_ADAPTER,
                description="Format to save the weights in.",
            ),
        }
        config.update(get_external_data_config())
        return config

    def _run_for_config(
        self, model: ONNXModelHandler, config: type[BasePassConfig], output_model_path: str
    ) -> ONNXModelHandler:
        print("=== ExtractAdapters Pass å¼€å§‹æ‰§è¡Œ ===")
        print(f"è¾“å…¥æ¨¡å‹è·¯å¾„: {model.model_path}")
        print(f"è¾“å‡ºæ¨¡å‹è·¯å¾„: {output_model_path}")
        print(f"é€‚é…å™¨ç±»å‹: {config.adapter_type}")
        print(f"make_inputs: {config.make_inputs}")
        print(f"save_format: {config.save_format}")
        logger.warning("=== ExtractAdapters Pass å¼€å§‹æ‰§è¡Œ ===")
        logger.warning(f"è¾“å…¥æ¨¡å‹è·¯å¾„: {model.model_path}")
        logger.warning(f"è¾“å‡ºæ¨¡å‹è·¯å¾„: {output_model_path}")
        logger.warning(f"é€‚é…å™¨ç±»å‹: {config.adapter_type}")
        logger.warning(f"make_inputs: {config.make_inputs}")
        logger.warning(f"save_format: {config.save_format}")
        
        # éªŒè¯è¾“å…¥æ¨¡å‹
        if model is None:
            print("âŒ é”™è¯¯ï¼šè¾“å…¥æ¨¡å‹ä¸º Noneï¼")
            logger.error("è¾“å…¥æ¨¡å‹ä¸º Noneï¼")
            return None
        
        if not hasattr(model, 'model_path') or not model.model_path:
            print("âŒ é”™è¯¯ï¼šè¾“å…¥æ¨¡å‹æ²¡æœ‰æœ‰æ•ˆçš„ model_pathï¼")
            logger.error("è¾“å…¥æ¨¡å‹æ²¡æœ‰æœ‰æ•ˆçš„ model_pathï¼")
            return None
        
        logger.warning(f"è¾“å…¥æ¨¡å‹ç±»å‹: {type(model)}")
        logger.warning(f"è¾“å…¥æ¨¡å‹å±æ€§: {getattr(model, 'model_attributes', 'None')}")
        
        try:
            output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
            logger.warning(f"è§£æåçš„è¾“å‡ºæ¨¡å‹è·¯å¾„: {output_model_path}")
        except Exception as e:
            logger.error(f"è§£æè¾“å‡ºæ¨¡å‹è·¯å¾„æ—¶å‡ºé”™: {e}")
            return None

        try:
            logger.warning("æ­£åœ¨åŠ è½½ IR æ¨¡å‹...")
            ir_model = model.load_ir_model()
            logger.warning(f"IR æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç±»å‹: {type(ir_model)}")
            
            logger.warning("æ­£åœ¨åŠ è½½å¤–éƒ¨æ•°æ®åˆ°æ¨¡å‹...")
            ir.external_data.load_to_model(ir_model)
            logger.warning("å¤–éƒ¨æ•°æ®åŠ è½½å®Œæˆ")
            
            # è®°å½•æ¨¡å‹åŸºæœ¬ä¿¡æ¯
            if hasattr(ir_model, 'graph') and ir_model.graph:
                logger.warning(f"æ¨¡å‹å›¾ä¸­çš„åˆå§‹åŒ–å™¨æ•°é‡: {len(ir_model.graph.initializers)}")
                logger.warning(f"æ¨¡å‹å›¾ä¸­çš„è¾“å…¥æ•°é‡: {len(ir_model.graph.inputs)}")
                logger.warning(f"æ¨¡å‹å›¾ä¸­çš„è¾“å‡ºæ•°é‡: {len(ir_model.graph.outputs)}")
                
                # è®°å½•å‰å‡ ä¸ªåˆå§‹åŒ–å™¨çš„åç§°
                init_names = list(ir_model.graph.initializers.keys())[:10]
                logger.warning(f"å‰10ä¸ªåˆå§‹åŒ–å™¨åç§°: {init_names}")
            else:
                logger.error("IR æ¨¡å‹æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç»“æ„ï¼")
                return None
                
        except Exception as e:
            logger.error(f"åŠ è½½ IR æ¨¡å‹æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None

        # dictionary to store adapter weights
        weights = {}

        try:
            print(f"ğŸ” å¼€å§‹æå– {config.adapter_type} é€‚é…å™¨æƒé‡...")
            logger.warning(f"å¼€å§‹æå– {config.adapter_type} é€‚é…å™¨æƒé‡...")
            if config.adapter_type in [AdapterType.LORA, AdapterType.DORA, AdapterType.LOHA]:
                weights = self._extract_adapter(ir_model, adapter_type=config.adapter_type)
                print(f"ğŸ“Š æå–åˆ°çš„æƒé‡æ•°é‡: {len(weights)}")
                logger.warning(f"æå–åˆ°çš„æƒé‡æ•°é‡: {len(weights)}")
                if weights:
                    print(f"ğŸ“ æƒé‡åç§°: {list(weights.keys())}")
                    logger.warning(f"æƒé‡åç§°: {list(weights.keys())}")
                    # è®°å½•æƒé‡çš„å½¢çŠ¶ä¿¡æ¯
                    for name, weight in weights.items():
                        print(f"  æƒé‡ {name}: shape={weight.shape}, dtype={weight.dtype}")
                        logger.warning(f"æƒé‡ {name}: shape={weight.shape}, dtype={weight.dtype}")
                else:
                    print("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•æƒé‡ï¼")
                    logger.warning("æ²¡æœ‰æå–åˆ°ä»»ä½•æƒé‡ï¼")
            else:
                print(f"âŒ é”™è¯¯ï¼šä¸æ”¯æŒçš„é€‚é…å™¨ç±»å‹: {config.adapter_type}")
                logger.error(f"ä¸æ”¯æŒçš„é€‚é…å™¨ç±»å‹: {config.adapter_type}")
                raise ValueError(f"Unsupported adapter type: {config.adapter_type}")
        except Exception as e:
            print(f"âŒ æå–é€‚é…å™¨æƒé‡æ—¶å‡ºé”™: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e)}")
            logger.error(f"æå–é€‚é…å™¨æƒé‡æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None

        if not weights:
            print(f"âš ï¸ æ¨¡å‹ä¸­æ²¡æœ‰æ‰¾åˆ° {config.adapter_type} æ¨¡å—ï¼Œè¿”å›åŸå§‹æ¨¡å‹")
            print("=== ExtractAdapters Pass ç»“æŸï¼ˆè¿”å›åŸå§‹æ¨¡å‹ï¼‰===")
            logger.warning("No %s modules found in the model. Returning the original model.", config.adapter_type)
            logger.warning("=== ExtractAdapters Pass ç»“æŸï¼ˆè¿”å›åŸå§‹æ¨¡å‹ï¼‰===")
            return model

        try:
            if config.make_inputs:
                logger.info("å¼€å§‹å°†æƒé‡è½¬æ¢ä¸ºè¾“å…¥...")
                # create inputs for the weights
                for weight_name in weights:
                    logger.info(f"æ­£åœ¨å¤„ç†æƒé‡: {weight_name}")
                    self._convert_initializer_to_input(ir_model, weight_name)
                    self._make_dynamic_optional(ir_model, weights, weight_name, config)
                logger.info("æƒé‡è½¬æ¢ä¸ºè¾“å…¥å®Œæˆ")
        except Exception as e:
            logger.error(f"è½¬æ¢æƒé‡ä¸ºè¾“å…¥æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None

        try:
            print("ğŸ’¾ å¼€å§‹ä¿å­˜æƒé‡æ–‡ä»¶...")
            print(f"æƒé‡æ•°é‡: {len(weights)}")
            print(f"è¾“å‡ºæ¨¡å‹è·¯å¾„: {output_model_path}")
            print(f"è¾“å‡ºæ¨¡å‹çˆ¶ç›®å½•: {Path(output_model_path).parent}")
            print(f"ä¿å­˜æ ¼å¼: {config.save_format}")
            logger.info("å¼€å§‹ä¿å­˜æƒé‡æ–‡ä»¶...")
            weights_path = save_weights(weights, Path(output_model_path).parent / "adapter_weights", config.save_format)
            print(f"âœ… æƒé‡æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            print(f"  å®Œæ•´è·¯å¾„: {weights_path}")
            print(f"  æ–‡ä»¶å: {weights_path.name if hasattr(weights_path, 'name') else Path(weights_path).name}")
            print(f"  æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {Path(weights_path).exists()}")
            logger.info(f"æƒé‡æ–‡ä»¶ä¿å­˜æˆåŠŸ: {weights_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æƒé‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            logger.error(f"ä¿å­˜æƒé‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None

        try:
            print("ğŸ—ï¸ å¼€å§‹ä¿å­˜æ¨¡å‹...")
            weights_file_name = weights_path.name if hasattr(weights_path, 'name') else Path(weights_path).name
            print(f"config.make_inputs = {config.make_inputs}")
            print(f"weights_file_name = {weights_file_name}")
            external_init_file = weights_file_name if not config.make_inputs else None
            constant_inputs_file = weights_file_name if config.make_inputs else None
            print(f"external_initializers_file_name = {external_init_file}")
            print(f"constant_inputs_file_name = {constant_inputs_file}")
            logger.info("å¼€å§‹ä¿å­˜æ¨¡å‹...")
            # save the model
            output_model = model_proto_to_olive_model(
                ir.to_proto(ir_model),
                output_model_path,
                config,
                external_initializers_file_name=external_init_file,
                constant_inputs_file_name=constant_inputs_file,
            )
            
            if output_model is None:
                print("âŒ è‡´å‘½é”™è¯¯ï¼šmodel_proto_to_olive_model è¿”å›äº† Noneï¼")
                logger.error("model_proto_to_olive_model è¿”å›äº† Noneï¼")
                return None
            
            print(f"âœ… è¾“å‡ºæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œç±»å‹: {type(output_model)}")
            print(f"ğŸ“ è¾“å‡ºæ¨¡å‹è·¯å¾„: {getattr(output_model, 'model_path', 'None')}")
            print(f"ğŸ“ constant_inputs_file_name: {getattr(output_model, 'constant_inputs_file_name', 'None')}")
            print(f"ğŸ“ constant_inputs_path: {getattr(output_model, 'constant_inputs_path', 'None')}")
            logger.info(f"è¾“å‡ºæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œç±»å‹: {type(output_model)}")
            logger.info(f"è¾“å‡ºæ¨¡å‹è·¯å¾„: {getattr(output_model, 'model_path', 'None')}")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¾“å‡ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"åˆ›å»ºè¾“å‡ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            traceback_str = traceback.format_exc()
            print(f"å®Œæ•´é”™è¯¯å †æ ˆ:\n{traceback_str}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback_str}")
            return None

        try:
            logger.info("å¼€å§‹è®¾ç½®æ¨¡å‹å±æ€§...")
            output_model.model_attributes = deepcopy(model.model_attributes) or {}
            logger.info(f"å¤åˆ¶çš„æ¨¡å‹å±æ€§: {output_model.model_attributes}")
            
            # add adapter weights to the model attributes
            output_model.model_attributes["additional_files"] = additional_files = output_model.model_attributes.get(
                "additional_files", []
            )
            additional_files.append(str(weights_path))
            logger.info(f"æ·»åŠ çš„é™„åŠ æ–‡ä»¶: {additional_files}")
            
            # save information about the weights in the model attributes
            weights_info = {name: [list(value.shape), str(value.dtype)] for name, value in weights.items()}
            logger.info(f"æƒé‡ä¿¡æ¯: {weights_info}")
            
            if not config.make_inputs:
                output_model.model_attributes["external_initializers"] = weights_info
                logger.info("æƒé‡ä¿¡æ¯ä¿å­˜ä¸º external_initializers")
            else:
                output_model.model_attributes["constant_inputs"] = weights_info
                logger.info("æƒé‡ä¿¡æ¯ä¿å­˜ä¸º constant_inputs")
                
            logger.info(f"æœ€ç»ˆæ¨¡å‹å±æ€§: {output_model.model_attributes}")
            
        except Exception as e:
            logger.error(f"è®¾ç½®æ¨¡å‹å±æ€§æ—¶å‡ºé”™: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
        
        print("ğŸ‰ === ExtractAdapters Pass æˆåŠŸå®Œæˆ ===")
        print(f"ğŸ“¦ è¿”å›çš„æ¨¡å‹ç±»å‹: {type(output_model)}")
        print(f"ğŸ“ è¿”å›çš„æ¨¡å‹è·¯å¾„: {getattr(output_model, 'model_path', 'None')}")
        print(f"ğŸ“ constant_inputs_path: {getattr(output_model, 'constant_inputs_path', 'None')}")
        print(f"ğŸ“ external_initializers_path: {getattr(output_model, 'external_initializers_path', 'None')}")
        logger.warning("=== ExtractAdapters Pass æˆåŠŸå®Œæˆ ===")
        logger.warning(f"è¿”å›çš„æ¨¡å‹ç±»å‹: {type(output_model)}")
        logger.warning(f"è¿”å›çš„æ¨¡å‹è·¯å¾„: {getattr(output_model, 'model_path', 'None')}")
        return output_model

    def _convert_initializer_to_input(self, model: ir.Model, initializer_name: str):
        """Convert a specific initializer to an input."""
        logger.info(f"å°†åˆå§‹åŒ–å™¨è½¬æ¢ä¸ºè¾“å…¥: {initializer_name}")
        
        graph = model.graph

        # Check if the initializer exists
        if initializer_name not in graph.initializers:
            logger.error(f"åˆå§‹åŒ–å™¨ '{initializer_name}' åœ¨å›¾ä¸­ä¸å­˜åœ¨ï¼")
            logger.info(f"å¯ç”¨çš„åˆå§‹åŒ–å™¨: {list(graph.initializers.keys())[:10]}...")
            raise ValueError(f"Initializer '{initializer_name}' not found in graph")

        # Get the initializer
        initializer = graph.initializers[initializer_name]
        logger.info(f"æ‰¾åˆ°åˆå§‹åŒ–å™¨: {initializer_name}, ç±»å‹: {type(initializer)}")

        # Check if it's already an input
        if initializer in graph.inputs:
            logger.info(f"åˆå§‹åŒ–å™¨ {initializer_name} å·²ç»æ˜¯è¾“å…¥ï¼Œè·³è¿‡")
            return  # Already an input

        # Add to inputs
        graph.inputs.append(initializer)
        logger.info(f"æˆåŠŸå°† {initializer_name} æ·»åŠ åˆ°è¾“å…¥åˆ—è¡¨ï¼Œå½“å‰è¾“å…¥æ•°é‡: {len(graph.inputs)}")

    def _decompose_gemm(self, ir_model: ir.Model):
        """Decompose Gemm nodes into MatMul and Add nodes."""
        from onnxscript import rewriter
        from onnxscript.rewriter.rules.common import gemm_to_matmul_add_rule

        return rewriter.rewrite(ir_model, pattern_rewrite_rules=[gemm_to_matmul_add_rule])

    def _extract_adapter(self, ir_model: ir.Model, adapter_type: AdapterType = AdapterType.LORA):
        """Extract adapter weights for LoRA, DoRA, or LoHa from an ONNX model.

        LoRA:
        lora_A -> MatMul -> ...
        lora_B -> MatMul -> ...

        DoRA:
        Besides LoRA A and LoRA B, DoRA also has a learnable magnitude vector M (dora_M):
                         W' = mV + dV = mV + mAB
        AB + dora_M -> Div -> ...

        LoHa:
        hada_w1_a + hada_w1_b -> MatMul -> ...
        hada_w2_a + hada_w2_b -> MatMul -> ...
        """
        logger.warning(f"=== å¼€å§‹æå– {adapter_type} é€‚é…å™¨æƒé‡ ===")
        
        if adapter_type == AdapterType.DORA:
            logger.warning("DoRA ç±»å‹ï¼Œéœ€è¦å…ˆåˆ†è§£ Gemm èŠ‚ç‚¹...")
            try:
                ir_model = self._decompose_gemm(ir_model)
                logger.warning("Gemm èŠ‚ç‚¹åˆ†è§£å®Œæˆ")
            except Exception as e:
                logger.error(f"åˆ†è§£ Gemm èŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
                raise

        # dictionary to store adapter weights
        weights = {}

        # Get the appropriate patterns for the adapter type
        patterns = None
        if adapter_type == AdapterType.LORA:
            patterns = LORA_NAME_PATTERNS
            logger.warning(f"ä½¿ç”¨ LoRA æ¨¡å¼: {patterns}")
        elif adapter_type == AdapterType.DORA:
            patterns = DORA_NAME_PATTERNS
            logger.warning(f"ä½¿ç”¨ DoRA æ¨¡å¼: {patterns}")
        elif adapter_type == AdapterType.LOHA:
            patterns = LOHA_NAME_PATTERNS
            logger.warning(f"ä½¿ç”¨ LoHa æ¨¡å¼: {patterns}")
        else:
            logger.error(f"ä¸æ”¯æŒçš„é€‚é…å™¨ç±»å‹: {adapter_type}")
            raise ValueError(f"Unsupported adapter type: {adapter_type}")

        logger.warning(f"å¼€å§‹æ‰«ææ¨¡å‹ä¸­çš„åˆå§‹åŒ–å™¨ï¼Œæ€»æ•°: {len(ir_model.graph.initializers)}")
        
        to_rename = []
        matched_count = 0
        for i, initializer in enumerate(ir_model.graph.initializers.values()):
            if i < 10:  # åªè®°å½•å‰10ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                logger.warning(f"æ£€æŸ¥åˆå§‹åŒ–å™¨ [{i}]: {initializer.name}")
            
            adapter_weight = get_adapter_name(initializer, patterns)
            if adapter_weight is None:
                if i < 10:
                    logger.warning(f"  -> ä¸åŒ¹é…ä»»ä½•é€‚é…å™¨æ¨¡å¼")
                continue

            logger.warning(f"æ‰¾åˆ°åŒ¹é…çš„é€‚é…å™¨æƒé‡: {initializer.name} -> {adapter_weight}")
            to_rename.append((initializer, adapter_weight))
            matched_count += 1

        logger.warning(f"æ€»å…±æ‰¾åˆ° {matched_count} ä¸ªåŒ¹é…çš„é€‚é…å™¨æƒé‡")
        
        if not to_rename:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„é€‚é…å™¨æƒé‡ï¼")
            print("å¯èƒ½çš„åŸå› :")
            print("1. æ¨¡å‹ä¸­æ²¡æœ‰é€‚é…å™¨æƒé‡")
            print("2. é€‚é…å™¨æƒé‡çš„å‘½åæ¨¡å¼ä¸é¢„æœŸä¸ç¬¦")
            print("3. é€‚é…å™¨ç±»å‹é€‰æ‹©é”™è¯¯")
            
            # è®°å½•ä¸€äº›åˆå§‹åŒ–å™¨åç§°ä¾›è°ƒè¯•
            init_names = list(ir_model.graph.initializers.keys())[:20]
            print(f"ğŸ” æ¨¡å‹ä¸­çš„å‰20ä¸ªåˆå§‹åŒ–å™¨åç§°: {init_names}")
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„é€‚é…å™¨æƒé‡ï¼")
            logger.warning("å¯èƒ½çš„åŸå› :")
            logger.warning("1. æ¨¡å‹ä¸­æ²¡æœ‰é€‚é…å™¨æƒé‡")
            logger.warning("2. é€‚é…å™¨æƒé‡çš„å‘½åæ¨¡å¼ä¸é¢„æœŸä¸ç¬¦")
            logger.warning("3. é€‚é…å™¨ç±»å‹é€‰æ‹©é”™è¯¯")
            logger.warning(f"æ¨¡å‹ä¸­çš„å‰20ä¸ªåˆå§‹åŒ–å™¨åç§°: {init_names}")
            return weights

        logger.warning("å¼€å§‹å¤„ç†åŒ¹é…çš„é€‚é…å™¨æƒé‡...")
        for i, (initializer, adapter_weight) in enumerate(to_rename):
            logger.info(f"å¤„ç†æƒé‡ [{i+1}/{len(to_rename)}]: {initializer.name} -> {adapter_weight}")
            
            try:
                old_name = initializer.name

                # Store the weight data
                if hasattr(initializer, 'const_value') and initializer.const_value is not None:
                    weight_data = initializer.const_value.numpy()
                    weights[adapter_weight] = weight_data
                    logger.info(f"  æƒé‡æ•°æ®æå–æˆåŠŸ: shape={weight_data.shape}, dtype={weight_data.dtype}")
                else:
                    logger.error(f"  åˆå§‹åŒ–å™¨ {old_name} æ²¡æœ‰æœ‰æ•ˆçš„ const_value")
                    continue

                # Rename the initializer
                initializer.name = adapter_weight

                # Update the initializers dictionary
                if old_name in ir_model.graph.initializers:
                    ir_model.graph.initializers.pop(old_name)
                    ir_model.graph.initializers[adapter_weight] = initializer
                    logger.info(f"  åˆå§‹åŒ–å™¨å­—å…¸æ›´æ–°æˆåŠŸ")
                else:
                    logger.warning(f"  åˆå§‹åŒ–å™¨ {old_name} ä¸åœ¨å­—å…¸ä¸­")

                # Create external tensor
                external_tensor = ir.ExternalTensor(
                    location="dummy-location.bin",
                    offset=None,
                    length=None,
                    dtype=initializer.const_value.dtype,
                    shape=initializer.const_value.shape,
                    name=adapter_weight,
                    base_dir="",
                )

                initializer.const_value = external_tensor
                logger.info(f"  å¤–éƒ¨å¼ é‡åˆ›å»ºæˆåŠŸ")
                
            except Exception as e:
                logger.error(f"å¤„ç†æƒé‡ {initializer.name} æ—¶å‡ºé”™: {e}")
                logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
                import traceback
                logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                continue

        logger.warning(f"=== é€‚é…å™¨æƒé‡æå–å®Œæˆï¼ŒæˆåŠŸæå– {len(weights)} ä¸ªæƒé‡ ===")
        return weights

    def _make_dynamic_optional(
        self, model: ir.Model, weights: dict[str, "NDArray"], name: str, config: type[BasePassConfig]
    ):
        """Make the input dynamic and optional."""
        logger.info(f"å¤„ç†åŠ¨æ€å¯é€‰è¾“å…¥: {name}")
        
        if "lora_magnitude_vector" in name:
            # magnitude vector's shape is independent of lora_r, so we do nothing
            logger.info(f"è·³è¿‡ magnitude vector: {name}")
            return

        # Determine which dimension should be made dynamic based on pattern in name
        dim_idx = 1
        if "lora_A" in name:
            dim_idx = 1
            logger.info(f"LoRA A æƒé‡ï¼Œä½¿ç”¨ç»´åº¦ç´¢å¼•: {dim_idx}")
        elif "lora_B" in name:
            dim_idx = 0
            logger.info(f"LoRA B æƒé‡ï¼Œä½¿ç”¨ç»´åº¦ç´¢å¼•: {dim_idx}")
        elif "hada_w1_a" in name or "hada_w2_a" in name:
            dim_idx = 0  # For the first matrix in Hadamard products
            logger.info(f"LoHa w1_a/w2_a æƒé‡ï¼Œä½¿ç”¨ç»´åº¦ç´¢å¼•: {dim_idx}")
        elif "hada_w1_b" in name or "hada_w2_b" in name:
            dim_idx = 1  # For the second matrix in Hadamard products
            logger.info(f"LoHa w1_b/w2_b æƒé‡ï¼Œä½¿ç”¨ç»´åº¦ç´¢å¼•: {dim_idx}")

        # make the input dynamic
        if config.dynamic_lora_r:
            logger.info(f"è®¾ç½®åŠ¨æ€ lora_r ç»´åº¦: {name}, dim_idx={dim_idx}")
            try:
                self._make_input_dim_dynamic(model, name, dim_idx, "lora_r")
                logger.info(f"æˆåŠŸè®¾ç½®åŠ¨æ€ç»´åº¦: {name}")
            except Exception as e:
                logger.error(f"è®¾ç½®åŠ¨æ€ç»´åº¦æ—¶å‡ºé”™: {e}")
                raise
        else:
            logger.info(f"è·³è¿‡åŠ¨æ€ç»´åº¦è®¾ç½® (dynamic_lora_r=False): {name}")

        # create default initializer with the lora_r dimension set to 0
        if config.optional_inputs:
            logger.info(f"åˆ›å»ºå¯é€‰è¾“å…¥çš„é»˜è®¤åˆå§‹åŒ–å™¨: {name}")
            try:
                shape = list(weights[name].shape)
                original_shape = shape.copy()
                shape[dim_idx] = 0
                logger.info(f"åŸå§‹å½¢çŠ¶: {original_shape}, æ–°å½¢çŠ¶: {shape}")
                
                zero_array = np.zeros(shape, dtype=weights[name].dtype)
                logger.info(f"åˆ›å»ºé›¶æ•°ç»„: shape={zero_array.shape}, dtype={zero_array.dtype}")
                
                initializer_value = model.graph.initializers[name]
                initializer_value.const_value = ir.Tensor(zero_array)
                model.graph.inputs.append(initializer_value)
                logger.info(f"æˆåŠŸåˆ›å»ºå¯é€‰è¾“å…¥: {name}")
            except Exception as e:
                logger.error(f"åˆ›å»ºå¯é€‰è¾“å…¥æ—¶å‡ºé”™: {e}")
                raise
        else:
            logger.info(f"è·³è¿‡å¯é€‰è¾“å…¥åˆ›å»º (optional_inputs=False): {name}")

    def _make_input_dim_dynamic(self, model: ir.Model, input_name: str, dim_idx: int, dim_param: str):
        """Make a dimension of an input dynamic."""
        logger.info(f"è®¾ç½®è¾“å…¥ç»´åº¦ä¸ºåŠ¨æ€: {input_name}, dim_idx={dim_idx}, dim_param={dim_param}")
        
        # Find the input value
        input_value = None
        for inp in model.graph.inputs:
            if inp.name == input_name:
                input_value = inp
                break

        if input_value is None:
            logger.error(f"{input_name} ä¸æ˜¯ä¸€ä¸ªè¾“å…¥ï¼")
            logger.info(f"å½“å‰è¾“å…¥åˆ—è¡¨: {[inp.name for inp in model.graph.inputs]}")
            raise ValueError(f"{input_name} is not an input.")

        logger.info(f"æ‰¾åˆ°è¾“å…¥: {input_name}, ç±»å‹: {type(input_value)}")

        if input_value.shape is None:
            logger.error(f"è¾“å…¥ {input_name} æ²¡æœ‰å½¢çŠ¶ä¿¡æ¯ï¼")
            raise ValueError(f"Input {input_name} does not have shape information.")

        logger.info(f"è¾“å…¥å½¢çŠ¶: {input_value.shape}, é•¿åº¦: {len(input_value.shape)}")

        if dim_idx >= len(input_value.shape):
            logger.error(f"è¾“å…¥ {input_name} çš„ç»´åº¦æ•°ä¸º {len(input_value.shape)}ï¼Œä½†å°è¯•è®¿é—®ç»´åº¦ {dim_idx}")
            raise ValueError(
                f"Input {input_name} has rank {len(input_value.shape.dims)} but trying to access dim {dim_idx}."
            )

        # Create new shape with symbolic dimension
        new_dims = list(input_value.shape)
        logger.info(f"åŸå§‹ç»´åº¦: {new_dims}")
        
        if isinstance(new_dims[dim_idx], ir.SymbolicDim) and new_dims[dim_idx].value is not None:
            logger.error(f"æ— æ³•æ›¿æ¢ç°æœ‰çš„åŠ¨æ€ç»´åº¦ {new_dims[dim_idx].value} ä¸º {dim_param}")
            raise ValueError(f"Can't replace existing dynamic dim {new_dims[dim_idx].value} with {dim_param}")

        new_dims[dim_idx] = ir.SymbolicDim(dim_param)
        input_value.shape = ir.Shape(new_dims)
        logger.info(f"æ–°ç»´åº¦: {new_dims}")
        logger.info(f"æˆåŠŸè®¾ç½®åŠ¨æ€ç»´åº¦: {input_name}[{dim_idx}] = {dim_param}")
