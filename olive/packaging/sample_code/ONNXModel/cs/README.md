# Olive output instruction

## ONNXRuntime installation
Please follow the instruction to install ONNX Runtime: https://onnxruntime.ai/docs/install/#cccwinml-installs

## Follow the code sample to use olive output model
Please check code_sample.cpp for the sample how to use output model and inference_config.json for your inference. Find more details about ONNX Runtime C# API in https://onnxruntime.ai/docs/get-started/with-csharp.html.

The sample code works with ONNX Runtime 1.14.x and prior versions.
