import argparse
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import torch
from peft import LoraConfig, load_peft_weights

if TYPE_CHECKING:
    from numpy.typing import NDArray


def parse_args(raw_args):
    parser = argparse.ArgumentParser(
        "Export lora adapter weights to a .npz file that will be consumed by ONNX models generated by Olive"
        " ExtractedAdapters pass."
    )

    parser.add_argument(
        "--adapter_path",
        type=str,
        help="Path to the adapters weights saved after peft fine-tuning. Can be a local folder or huggingface id.",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        help="Path to save the exported weights. Will be saved as a .npz file.",
    )
    parser.add_argument(
        "--pack_weights",
        action="store_true",
        help=(
            "Whether to pack the weights. If True, the weights for each module type will be packed into a single array."
        ),
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="float32",
        choices=["float32", "float16"],
        help=(
            "Data type to save float weights as. If quantize_int4 is True, this is the data type of the quantization"
            " scales. Default is float32."
        ),
    )
    # quantization options
    parser.add_argument(
        "--quantize_int4",
        action="store_true",
        help="Quantize the weights to int4 using blockwise quantization.",
    )
    int4_group = parser.add_argument_group("int4 quantization options")
    int4_group.add_argument(
        "--int4_block_size",
        type=int,
        default=32,
        choices=[16, 32, 64, 128, 256],
        help="Block size for int4 quantization. Default is 32.",
    )
    int4_group.add_argument(
        "--int4_quantization_mode",
        type=str,
        default="symmetric",
        choices=["symmetric", "asymmetric"],
        help="Quantization mode for int4 quantization. Default is symmetric.",
    )

    return parser.parse_args(raw_args)


def get_sort_key(module_name: str):
    """Get the key to sort the module names by."""
    parts = module_name.split(".")
    for i, part in enumerate(parts):
        try:
            # want the layers to be sorted by the number
            parts[i] = int(part)
        except ValueError:
            pass
    return parts


def int4_block_quant(float_weight: "NDArray", block_size: int, is_symmetric: bool):
    """Quantize a weight tensor to int4."""
    # Only need to quantize the weight tensors directly
    # Not the same as OnnxMatMul4Quantizer pass which quantizes an entire model
    # TODO(jambayk): When ORT 1.18.0 is released, use DefaultWeightOnlyQuantizer.int4_block_quant
    from onnxruntime.quantization.matmul_4bits_quantizer import quantize_matmul_4bits

    rows, cols = float_weight.shape

    blob_size = block_size // 2
    k_blocks = (rows + block_size - 1) // block_size
    padded_rows = k_blocks * block_size
    pad_len = padded_rows - rows
    if pad_len > 0:
        float_weight = np.pad(float_weight, ((0, pad_len), (0, 0)), "constant")

    # block wise quantization, each block comes from a single column
    packed = np.zeros((cols, k_blocks, blob_size), dtype="uint8")
    scales = np.zeros((cols * k_blocks), dtype=float_weight.dtype)
    zero_point = np.zeros(cols * ((k_blocks + 1) // 2), dtype="uint8")
    quantize_matmul_4bits(packed, float_weight, scales, zero_point, block_size, cols, rows, is_symmetric)

    return packed, scales, zero_point


def main(raw_args=None):
    args = parse_args(raw_args)

    adapter_weights = load_peft_weights(args.adapter_path, device="cpu")

    transformed_weights = {}
    float_modules = set()
    quant_modules = set()
    for name, value in adapter_weights.items():
        new_name = name.replace("base_model.model.model", "model")
        # cast to dtype first since some dtypes like bfloat16 are not supported by numpy
        # need to copy since the numpy array is read-only
        float_weight = value.to(getattr(torch, args.dtype)).numpy().transpose().copy()
        if not args.quantize_int4:
            transformed_weights[new_name] = float_weight
            float_modules.add(new_name.replace(".weight", ""))
        else:
            weight, scale, zero_point = int4_block_quant(
                float_weight, args.int4_block_size, args.int4_quantization_mode == "symmetric"
            )
            transformed_weights[new_name.replace(".weight", ".quant.weight")] = weight
            transformed_weights[new_name.replace(".weight", ".quant.scale")] = scale
            if args.int4_quantization_mode == "asymmetric":
                # otherwise it's always 0 and not part of the node inputs
                transformed_weights[new_name.replace(".weight", ".quant.zero_point")] = zero_point
            quant_modules.add(new_name.replace(".weight", ".quant"))

    if args.pack_weights:
        from olive.passes.onnx.extract_adapters import ExtractAdapters

        lora_config = LoraConfig.from_pretrained(args.adapter_path)
        transformed_weights, _ = ExtractAdapters.pack_weights(
            transformed_weights, lora_config.target_modules, float_modules, quant_modules
        )

    output_path = Path(args.output_path).with_suffix(".npz")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(output_path, **transformed_weights)

    return output_path


if __name__ == "__main__":
    main()
