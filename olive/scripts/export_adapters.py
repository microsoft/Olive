import argparse
from pathlib import Path
from typing import TYPE_CHECKING, Dict

import numpy as np
import torch
from peft import LoraConfig, load_peft_weights

if TYPE_CHECKING:
    from numpy.typing import NDArray


def parse_args(raw_args):
    parser = argparse.ArgumentParser(
        "Export lora adapter weights to a .npz file that will be consumed by ONNX models generated by Olive"
        " ExtractedAdapters pass."
    )

    parser.add_argument(
        "--adapter_path",
        type=str,
        help="Path to the adapter to export.",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        help="Path to save the exported weights. Will be saved as a .npz file.",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="float32",
        choices=["float32", "float16"],
        help="Data type to save the weights as.",
    )
    parser.add_argument(
        "--pack_weights",
        action="store_true",
        help=(
            "Whether to pack the weights. If True, the weights for each module type will be packed into a single array."
        ),
    )

    return parser.parse_args(raw_args)


def get_sort_key(module_name: str):
    """Get the key to sort the module names by."""
    parts = module_name.split(".")
    for i, part in enumerate(parts):
        try:
            # want the layers to be sorted by the number
            parts[i] = int(part)
        except ValueError:
            pass
    return parts


def pack_weights(weights: Dict[str, "NDArray"], module_type: str) -> Dict[str, "NDArray"]:
    """Pack the weights for a given module type into an array each for lora_A and lora_B."""
    packed_weights = {}
    for lora_i in ["lora_A", "lora_B"]:
        matching_modules = sorted(
            [name for name in weights if module_type in name and lora_i in name], key=get_sort_key
        )
        packed_weights[f"{module_type}.{lora_i}.weight.packed"] = np.concatenate(
            [weights[name] for name in matching_modules]
        )

    return packed_weights


def main(raw_args=None):
    args = parse_args(raw_args)

    adapter_weights = load_peft_weights(args.adapter_path, device="cpu")

    transformed_weights = {}
    for name, value in adapter_weights.items():
        new_name = name.replace("base_model.model.model", "model")
        # cast to dtype first since some dtypes like bfloat16 are not supported by numpy
        # need to copy since the numpy array is read-only
        transformed_weights[new_name] = value.to(getattr(torch, args.dtype)).numpy().transpose().copy()

    if args.pack_weights:
        lora_config = LoraConfig.from_pretrained(args.adapter_path)
        packed_weights = {}
        for module_type in lora_config.target_modules:
            packed_weights.update(pack_weights(transformed_weights, module_type))
        transformed_weights = packed_weights

    output_path = Path(args.output_path).with_suffix(".npz")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(output_path, **transformed_weights)

    return output_path


if __name__ == "__main__":
    main()
