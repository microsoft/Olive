{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository shows how to deploy and use Onnx pipeline with dockers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull dockers. It should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with existing credentials...\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnx-converter\n",
      "Digest: sha256:080d344ba335d2131ae15769dd8daf149acaa5a1f122df998c2c7223231a62f4\n",
      "Status: Image is up to date for ziylregistry.azurecr.io/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from create-input\n",
      "Digest: sha256:cb94537e14345c9f0a03014566006658b030c5fed66f35bb511471838c733df0\n",
      "Status: Image is up to date for ziylregistry.azurecr.io/create-input:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from perf_test\n",
      "Digest: sha256:0b93e6a1d4e4cd5e0057cf503fce53b8702d2445252b7837e844e50752d2a369\n",
      "Status: Image is up to date for ziylregistry.azurecr.io/perf_test:latest\n"
     ]
    }
   ],
   "source": [
    "!sh build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 23:18:13.396371: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tf2onnx/convert.py\", line 135, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tf2onnx/convert.py\", line 99, in main\n",
      "    graph_def, inputs, outputs = loader.from_saved_model(args.saved_model, args.inputs, args.outputs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tf2onnx/loader.py\", line 87, in from_saved_model\n",
      "    meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 196, in load\n",
      "    loader = SavedModelLoader(export_dir)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 212, in __init__\n",
      "    self._saved_model = _parse_saved_model(export_dir)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 82, in _parse_saved_model\n",
      "    constants.SAVED_MODEL_FILENAME_PB))\n",
      "OSError: SavedModel file does not exist at: model_240000/{saved_model.pbtxt|saved_model.pb}\n",
      "Traceback (most recent call last):\n",
      "  File \"src/onnx-converter.py\", line 282, in <module>\n",
      "    main()\n",
      "  File \"src/onnx-converter.py\", line 276, in main\n",
      "    converter(args)\n",
      "  File \"src/onnx-converter.py\", line 213, in tf2onnx\n",
      "    \"--output\", args.output_onnx_path])\n",
      "  File \"/usr/local/lib/python3.6/subprocess.py\", line 311, in check_call\n",
      "    raise CalledProcessError(retcode, cmd)\n",
      "subprocess.CalledProcessError: Command '['python', '-m', 'tf2onnx.convert', '--saved-model', 'model_240000', '--output', '/mnt/output/elmo.onnx']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!docker run -v %CD%\\model_240000:/mnt/model_240000 ziylregistry.azurecr.io/onnx-converter --model model_240000 --output_onnx_path /mnt/output/elmo.onnx --model_type tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) model: string\n",
    "\n",
    "Required. The path of the model that needs to be converted.\n",
    "\n",
    "(2) output_onnx_path: string\n",
    "\n",
    "Required. The path to store the converted onnx model. Should end with \".onnx\". e.g. output.onnx\n",
    "\n",
    "(3) model_type: string\n",
    "\n",
    "Required. The name of original model framework.\n",
    "\n",
    "Available types are caffe, cntk, coreml, keras, libsvm, mxnet, scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "(4) model_inputs: string\n",
    "\n",
    "Optional. The model's input names. Required for tensorflow frozen models and checkpoints.\n",
    "\n",
    "(5) model_outputs: string\n",
    "\n",
    "Optional. The model's output names. Required for tensorflow frozen models checkpoints.\n",
    "\n",
    "(6) model_params: string\n",
    "\n",
    "Optional. The params of the model if needed.\n",
    "\n",
    "(7) model_input_shapes: list of tuple\n",
    "\n",
    "Optional. List of tuples. The input shape(s) of the model. Each dimension separated by ','.\n",
    "\n",
    "(8) target_opset: int\n",
    "\n",
    "Optional. Specifies the opset for ONNX, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3. Defaults to 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate-Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/deepthink.onnx inputs: \n",
      "input name: long_MRC_2/passage_mask:0, shape: [32, None, 40], type: tensor(float)\n",
      "input name: long_MRC_2/passage:0, shape: [32, None, 40], type: tensor(int32)\n",
      "input name: long_MRC_2/target_mask:0, shape: [32], type: tensor(int32)\n",
      "input name: long_MRC_2/start_only_label:0, shape: [32, None], type: tensor(float)\n",
      "input name: long_MRC_2/sent_passage_mask:0, shape: [32, None], type: tensor(float)\n",
      "input name: long_MRC_2/ques_mask:0, shape: [None, 40], type: tensor(float)\n",
      "input name: long_MRC_2/ques_input:0, shape: [None, 40], type: tensor(int32)\n",
      "Randomized input .pb file generated at  /mnt/test_data_set_0\n"
     ]
    }
   ],
   "source": [
    "!docker run -v %CD%\\deepthink.onnx:/mnt/deepthink.onnx ziylregistry.azurecr.io/create-input --model /mnt/deepthink.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters\n",
    "\n",
    "(1) model: string\n",
    "    \n",
    "Required. The path of the model that needs to be converted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance test tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  1\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32638 ; hostname=5645bc30e758 ; expr=cudaSetDevice(device_id_); \n",
      "Stacktrace:\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32694 ; hostname=5645bc30e758 ; expr=cudaSetDevice(device_id_); \n",
      "Stacktrace:\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cuda None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "mkldnn_openmp OMP_NUM_THREADS=1, passive None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "mkldnn_openmp OMP_NUM_THREADS=1, active None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "mkldnn_openmp passive None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "mkldnn_openmp active None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "mkldnn None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cpu_openmp OMP_NUM_THREADS=1, passive None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cpu_openmp OMP_NUM_THREADS=1, active None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cpu_openmp passive None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cpu_openmp active None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ngraph None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession Load model from /mnt/model failed:Protobuf parsing failed.\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "cpu None\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32729 ; hostname=5645bc30e758 ; expr=cudaSetDevice(device_id_); \n",
      "Stacktrace:\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "\n",
      "/home/artr/repo/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:127 OrtCreateSession /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:97 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/artr/repo/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:91 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32585 ; hostname=5645bc30e758 ; expr=cudaSetDevice(device_id_); \n",
      "Stacktrace:\n",
      "\n",
      "Stacktrace:\n",
      "\n",
      "\n",
      "\n",
      "Setting thread pool size to 0\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "Results:\n",
      "cuda error\n",
      "mkldnn_openmp OMP_NUM_THREADS=1, passive error\n",
      "mkldnn_openmp OMP_NUM_THREADS=1, active error\n",
      "mkldnn_openmp passive error\n",
      "mkldnn_openmp active error\n",
      "mkldnn error\n",
      "cpu_openmp OMP_NUM_THREADS=1, passive error\n",
      "cpu_openmp OMP_NUM_THREADS=1, active error\n",
      "cpu_openmp passive error\n",
      "cpu_openmp active error\n",
      "ngraph error\n",
      "cpu error\n",
      "tensorrt error\n"
     ]
    }
   ],
   "source": [
    "!docker run -v model:/mnt/model ziylregistry.azurecr.io/perf_test /mnt/model /mnt/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
