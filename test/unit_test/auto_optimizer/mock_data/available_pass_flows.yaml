# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------

_variables:
  - &fp32 ["OnnxConversion", "OrtTransformersOptimization", "OrtPerfTuning"]
  - &fp16 ["OnnxConversion", "OrtTransformersOptimization", "OrtPerfTuning"]
  - &mixed_fp16 [
      "OnnxConversion",
      "OrtTransformersOptimization",
      "OrtMixedPrecision",
      "OrtPerfTuning",
    ]
  - &int8 [
      "OnnxConversion",
      "OrtTransformersOptimization",
      "OnnxQuantization",
      "OrtPerfTuning",
    ]
  - &int4 [
      "OnnxConversion",
      "OrtTransformersOptimization",
      "OnnxMatMul4Quantizer",
      "OrtPerfTuning",
    ]
  - &inc_int8 [
      "OnnxConversion",
      "OrtTransformersOptimization",
      "IncQuantization",
      "OrtPerfTuning",
    ]
  - &vitisai_int8 [
      "OnnxConversion",
      "OrtTransformersOptimization",
      "VitisAIQuantization",
      "OrtPerfTuning",
    ]

mapping:
  # gpu, CudaExecutionProvider, precision: fp32, fp16, int4
  gpu_cuda_fp32:
    # skip OptimumConversion for now as it wil be merged into OnnxConversion
    - *fp32
  gpu_cuda_fp16:
    - *fp16
    - *mixed_fp16
  gpu_cuda_int4:
    # TODO(anyone): unify int4 quantization
    - *int4

  # gpu, TensorrtExecutionProvider, precision: fp32, fp16
  gpu_tensorrt_fp32:
    - *fp32
  gpu_tensorrt_fp16:
    - *fp16

  # gpu, DmlExecutionProvider, precision: fp32, fp16
  gpu_dml_fp32:
    - *fp32
  gpu_dml_fp16:
    - *fp16
    - *mixed_fp16

  # cpu, CPUExecutionProvider, precision: fp32, fp16, int8, int4
  cpu_cpu_fp32:
    - *fp32
  cpu_cpu_int8:
    - *int8
    - *inc_int8
  cpu_cpu_int4:
    - *int4

  # npu, VitisAIExecutionProvider, precision: fp32, int8
  npu_vitisai_fp32:
    - *fp32
  npu_vitisai_int8:
    - *vitisai_int8
