<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ONNX related – General &mdash; Olive  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/autodoc_pydantic.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/width.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/custom_version.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="PyTorch related – General" href="pytorch.html" />
    <link rel="prev" title="Configuring HW-dependent optimizations" href="../configure_hardware_optimizations.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Olive
          </a>
              <div class="version">
                0.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">OVERVIEW</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview/olive.html">Olive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/design.html">Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/quicktour.html">Quick Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/options.html">Olive Options</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getstarted/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getstarted/quickstart_examples.html">Quickstart Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">TUTORIALS</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../configure_systems.html">Configuring OliveSystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configure_metrics.html">Configuring Metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configure_pass.html">Configuring Pass</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../configure_hardware_optimizations.html">Configuring HW-dependent optimizations</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">ONNX related – General</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-conversion">Model Conversion</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-configuration">Example Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-optimizer">Model Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Example Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ort-transformers-optimization">ORT Transformers Optimization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">Example Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#post-training-quantization-ptq">Post Training Quantization (PTQ)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Example Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ort-performance-tuning">ORT Performance Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">Example Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pytorch.html">PyTorch related – General</a></li>
<li class="toctree-l2"><a class="reference internal" href="openvino.html">OpenVINO related – Intel HW</a></li>
<li class="toctree-l2"><a class="reference internal" href="snpe.html">SNPE related – Qualcomm HW</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_users.html">Advanced User Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../how_to_add_pass.html">How to add new Pass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../how_to_write_userscript.html">How to write <code class="docutils literal notranslate"><span class="pre">user_script</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">EXAMPLES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Inception model optimization on Qualcomm NPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html#cifar10-optimization-with-openvino-for-intel-hw">Cifar10 optimization with OpenVINO for Intel HW</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html#bert-optimization-with-qat-customized-training-loop-on-cpu">BERT optimization with QAT Customized Training Loop on CPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html#resnet-optimization-with-qat-default-training-loop-on-cpu">ResNet optimization with QAT Default Training Loop on CPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html#resnet-optimization-with-qat-pytorch-lightning-module-on-cpu">ResNet optimization with QAT PyTorch Lightning Module on CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/models.html">OliveModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/systems.html">OliveSystems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/evaluator.html">OliveEvaluator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/metric.html">Metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/search-algorithms.html">SearchAlgorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/engine.html">Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/passes.html">Passes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Olive</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../configure_hardware_optimizations.html">Configuring HW-dependent optimizations</a></li>
      <li class="breadcrumb-item active">ONNX related – General</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/passes/onnx.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="onnx-related-general">
<h1>ONNX related – General<a class="headerlink" href="#onnx-related-general" title="Permalink to this heading">¶</a></h1>
<p>Olive provides multiple Passes that execute optimization tools related to ONNX. <a class="reference external" href="https://onnx.ai/">ONNX</a> is
an open format built to represent machine learning models. <a class="reference external" href="https://onnxruntime.ai/docs/">ONNX Runtime</a> is a cross-platform machine-learning
model accelerator, with a flexible interface to integrate hardware-specific libraries.</p>
<p>Olive provides easy access to the model optimization tools available in ONNX Runtime.</p>
<section id="model-conversion">
<h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this heading">¶</a></h2>
<p>The user might not have a model ready in the ONNX format. <code class="docutils literal notranslate"><span class="pre">OnnxConversion</span></code> converts PyTorch models to ONNX using
<a class="reference external" href="https://pytorch.org/docs/stable/onnx.html">torch.onnx</a>.</p>
<p>Please refer to <a class="reference internal" href="../../api/passes.html#onnx-conversion"><span class="std std-ref">OnnxConversion</span></a> for more details about the pass and its config parameters.</p>
<section id="example-configuration">
<h3>Example Configuration<a class="headerlink" href="#example-configuration" title="Permalink to this heading">¶</a></h3>
<p>a. Provide input shapes</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxConversion&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;input_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;input_shapes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">]],</span>
<span class="w">        </span><span class="nt">&quot;input_types&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;int64&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;int64&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;int64&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;output_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;target_opset&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">13</span>
<span class="w">    </span><span class="p">}</span>
<span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>b. Provide custom input tensor function</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxConversion&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;input_tensor_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;create_input_tensors&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;input_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;output_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;target_opset&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">13</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Check out <a class="reference external" href="https://github.com/microsoft/Olive/blob/main/examples/bert_ptq_cpu/user_script.py">this file</a>
for an example implementation of <code class="docutils literal notranslate"><span class="pre">&quot;user_script.py&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;create_input_tensors&quot;</span></code>.</p>
</section>
</section>
<section id="model-optimizer">
<h2>Model Optimizer<a class="headerlink" href="#model-optimizer" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">OnnxModelOptimizer</span></code> optimizes an ONNX model by fusing nodes. Fusing nodes involves merging multiple nodes in a model into a single node to
reduce the computational cost and improve the performance of the model.
The optimization process involves analyzing the structure of the ONNX model and identifying nodes that can be fused.</p>
<p>Please refer to <a class="reference internal" href="../../api/passes.html#onnx-model-optimizer"><span class="std std-ref">OnnxModelOptimizer</span></a> for more details about the pass and its config parameters.</p>
<section id="id1">
<h3>Example Configuration<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxModelOptimizer&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="ort-transformers-optimization">
<h2>ORT Transformers Optimization<a class="headerlink" href="#ort-transformers-optimization" title="Permalink to this heading">¶</a></h2>
<p>While ONNX Runtime automatically applies most optimizations while loading transformer models, some of the latest optimizations that have not
yet been integrated into ONNX Runtime.
<code class="docutils literal notranslate"><span class="pre">OrtTransformersOptimization</span></code> provides an offline capability to optimize <a class="reference external" href="https://huggingface.co/docs/transformers/index">transformers</a> models
in scenarios where ONNX Runtime does not apply the optimization at load time.
These optimizations are provided by onnxruntime through
<a class="reference external" href="https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers">onnxruntime.transformers</a>. Please
refer to the <a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/README.md">corresponding documentation</a>
for more details on the optimizations done by this tool.</p>
<p>Please refer to <a class="reference internal" href="../../api/passes.html#ort-transformers-optimization"><span class="std std-ref">OrtTransformersOptimization</span></a> for more details about the pass and its config parameters.</p>
<section id="id2">
<h3>Example Configuration<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OrtTransformersOptimization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;bert&quot;</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="post-training-quantization-ptq">
<h2>Post Training Quantization (PTQ)<a class="headerlink" href="#post-training-quantization-ptq" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://onnxruntime.ai/docs/performance/quantization.html" title="ONNX Runtime Quantization">Quantization</a> is a technique to compress deep learning models by reducing the precision of the model weights from 32 bits to 8 bits. This
technique is used to reduce the memory footprint and improve the inference performance of the model. Quantization can be applied to the
weights of the model, the activations of the model, or both.</p>
<p>There are two ways to quantize a model in onnxruntime:</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://onnxruntime.ai/docs/performance/quantization.html#dynamic-quantization" title="Dynamic Quantization">Dynamic Quantization</a>:
Dynamic quantization calculates the quantization parameters (scale and zero point) for activations dynamically, which means there is no
any requirement for the calibration dataset.</p>
<p>These calculations increase the cost of inference, while usually achieve higher accuracy comparing to static ones.</p>
</li>
<li><p><a class="reference external" href="https://onnxruntime.ai/docs/performance/quantization.html#static-quantization" title="Static Quantization">Static Quantization</a>:
Static quantization method runs the model using a set of inputs called calibration data. In this way, user must provide a calibration
dataset to calculate the quantization parameters (scale and zero point) for activations before quantizing the model.</p></li>
</ol>
<p>Olive consolidates the dynamic and static quantization into a single pass called <code class="docutils literal notranslate"><span class="pre">OnnxQuantization</span></code>, and provide the user with the ability to
tune both quantization methods and hyperparameter at the same time.
If the user desires to only tune either of dynamic or static quantization, Olive also supports them through <code class="docutils literal notranslate"><span class="pre">OnnxDynamicQuantization</span></code> and
<code class="docutils literal notranslate"><span class="pre">OnnxStaticQuantization</span></code> respectively.</p>
<p>Please refer to <a class="reference internal" href="../../api/passes.html#onnx-quantization"><span class="std std-ref">OnnxQuantization</span></a>, <a class="reference internal" href="../../api/passes.html#onnx-dynamic-quantization"><span class="std std-ref">OnnxDynamicQuantization</span></a> and
<a class="reference internal" href="../../api/passes.html#onnx-static-quantization"><span class="std std-ref">OnnxStaticQuantization</span></a> for more details about the passes and their config parameters.</p>
<section id="id3">
<h3>Example Configuration<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>a. Tune the parameters of the OlivePass with pre-defined search space</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxQuantization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dataloader_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;glue_calibration_reader&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;default_to_search&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="p">}</span>
</pre></div>
</div>
<p>b. Select parameters to tune</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxQuantization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// select per_channel to tune with &quot;DEFAULT_SEARCH&quot;.</span>
<span class="w">        </span><span class="c1">// other parameters will use the default value, not to be tuned.</span>
<span class="w">        </span><span class="nt">&quot;per_channel&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;DEFAULT_SEARCH&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dataloader_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;glue_calibration_reader&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>c. Use default values of the OlivePass (no tuning in this way)</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxQuantization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// set per_channel to &quot;DEFAULT&quot; value.</span>
<span class="w">        </span><span class="nt">&quot;per_channel&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dataloader_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;glue_calibration_reader&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>d. Specify parameters with user defined values</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;onnx_quantization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OnnxQuantization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// set per_channel to True.</span>
<span class="w">        </span><span class="nt">&quot;per_channel&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dataloader_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;glue_calibration_reader&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Check out <a class="reference external" href="https://github.com/microsoft/Olive/blob/main/examples/bert_ptq_cpu/user_script.py">this file</a>
for an example implementation of <code class="docutils literal notranslate"><span class="pre">&quot;user_script.py&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;glue_calibration_reader&quot;</span></code>.</p>
</section>
</section>
<section id="ort-performance-tuning">
<h2>ORT Performance Tuning<a class="headerlink" href="#ort-performance-tuning" title="Permalink to this heading">¶</a></h2>
<p>ONNX Runtime provides high performance across a range of hardware options through its Execution Providers interface for different execution
environments.
For each model running with each execution provider, there are settings that can be tuned (e.g. thread number, execution mode, etc) to
improve performance.
<code class="docutils literal notranslate"><span class="pre">OrtPerfTuning</span></code> covers basic knobs that can be leveraged to find the best performance for your model and hardware.</p>
<section id="id4">
<h3>Example Configuration<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;OrtPerfTuning&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;user_script&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user_script.py&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dataloader_func&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;create_dataloader&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Check out <a class="reference external" href="https://github.com/microsoft/Olive/blob/main/examples/bert_ptq_cpu/user_script.py">this file</a>
for an example implementation of <code class="docutils literal notranslate"><span class="pre">&quot;user_script.py&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;create_dataloader&quot;</span></code>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../configure_hardware_optimizations.html" class="btn btn-neutral float-left" title="Configuring HW-dependent optimizations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pytorch.html" class="btn btn-neutral float-right" title="PyTorch related – General" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, olivedevteam@microsoft.com.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>