---
hide:
- navigation
- toc
---

# O(nnx)Live: The AI Model Optimization Toolkit for ONNX Runtime

Olive is a core component of the [ONNX ecosystem](https://onnxruntime.ai), which allows AI Engineers to compose workflows of cutting edge AI Model optimization tasks - such as quantization, compression, graph optimizations and finetuning - so that your models run with quality and performance on the ONNX Runtime. Olive is capable of **finding** the best optimized model within your defined quality and performance constraints for different hardware providers (Nvidia, AMD, Qualcomm, Intel) across NPU, CPU and GPU.


<div class="grid cards" markdown>

-   :material-clock-fast:{ .lg .middle } __Get Started__

    ---

    Install `olive-ai` with `pip` and get up and running with Olive in minutes.

    [:octicons-arrow-right-24: Getting started](getting-started/getting-started.md)
    

<!-- -   :material-school:{ .lg .middle } __Tutorials__
    
    ---
    
    Find end-to-end tutorials that take you through various model optimization and deployment scenarios.

    [:octicons-arrow-right-24: Tutorials](tutorials/index.md) -->

-   :fontawesome-solid-signs-post:{ .lg .middle } __How To__

    ---

    Find more details on specific Olive capabilities, such as quantization, running workflows on remote compute, model packaging, conversions, and more!

    [:octicons-arrow-right-24: Features](how-to/index.md)

-   :material-api:{ .lg .middle } __API Reference__

    ---

    Get more details on specific Olive capabilities, such as running workflows on remote compute (for example, Azure AI), model packaging, conversions, and more!

    [:octicons-arrow-right-24: APIs](api/index.md)

-   :octicons-diff-added-16:{ .lg .middle } __Extending Olive__

    ---

    Learn about the design of Olive and how to extend Olive with your own optimization methods.

    [:octicons-arrow-right-24: Extend Olive](extending/index.md)