{
    "lora": {
        "type": "LoRA",
        "target_modules": [ "o_proj", "qkv_proj" ],
        "lora_r": 64,
        "lora_alpha": 64,
        "lora_dropout": 0.1,
        "train_data_config": "dataset_default_train",
        "eval_dataset_size": 0.3,
        "training_args": {
            "seed": 0,
            "data_seed": 42,
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": false,
            "learning_rate": 0.0001,
            "num_train_epochs": 3,
            "max_steps": 10,
            "logging_steps": 10,
            "evaluation_strategy": "steps",
            "eval_steps": 187,
            "group_by_length": true,
            "adam_beta2": 0.999,
            "max_grad_norm": 0.3
        }
    },
    "qlora": {
        "type": "QLoRA",
        "compute_dtype": "bfloat16",
        "quant_type": "nf4",
        "double_quant": true,
        "lora_r": 64,
        "lora_alpha": 64,
        "lora_dropout": 0.1,
        "train_data_config": "dataset_default_train",
        "eval_dataset_size": 0.3,
        "training_args": {
            "seed": 0,
            "data_seed": 42,
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": false,
            "learning_rate": 0.0001,
            "num_train_epochs": 3,
            "max_steps": 10,
            "logging_steps": 10,
            "evaluation_strategy": "steps",
            "eval_steps": 187,
            "group_by_length": true,
            "adam_beta2": 0.999,
            "max_grad_norm": 0.3
        }
    }
}
