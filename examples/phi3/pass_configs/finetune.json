{
    "lora": {
        "type": "LoRA",
        "config": {
            "target_modules": [
                "o_proj",
                "qkv_proj"
            ],
            "lora_r": 64,
            "lora_alpha": 64,
            "lora_dropout": 0.1,
            "train_data_config": "dataset_default_train",
            "eval_dataset_size": 0.3,
            "training_args": {
                "seed": 0,
                "data_seed": 42,
                "per_device_train_batch_size": 1,
                "per_device_eval_batch_size": 1,
                "gradient_accumulation_steps": 4,
                "gradient_checkpointing": false,
                "learning_rate": 0.0001,
                "num_train_epochs": 3,
                "max_steps": 10,
                "logging_steps": 10,
                "evaluation_strategy": "steps",
                "eval_steps": 187,
                "group_by_length": true,
                "adam_beta2": 0.999,
                "max_grad_norm": 0.3
            }
        }
    },
    "qlora": {
        "type": "QLoRA",
        "config": {
            "compute_dtype": "bfloat16",
            "quant_type": "nf4",
            "double_quant": true,
            "lora_r": 64,
            "lora_alpha": 64,
            "lora_dropout": 0.1,
            "train_data_config": "dataset_default_train",
            "eval_dataset_size": 0.3,
            "training_args": {
                "seed": 0,
                "data_seed": 42,
                "per_device_train_batch_size": 1,
                "per_device_eval_batch_size": 1,
                "gradient_accumulation_steps": 4,
                "gradient_checkpointing": false,
                "learning_rate": 0.0001,
                "num_train_epochs": 3,
                "max_steps": 10,
                "logging_steps": 10,
                "evaluation_strategy": "steps",
                "eval_steps": 187,
                "group_by_length": true,
                "adam_beta2": 0.999,
                "max_grad_norm": 0.3
            }
        }
    }
}
