{
    "input_model": { "type": "HFModel", "model_path": "meta-llama/Llama-2-7b-hf" },
    "passes": {
        "qq": {
            "type": "QuarkQuantizationPass",
            "quant_scheme": "w_uint4_per_group_asym",
            "quant_algo": "awq",
            "dataset": "pileval_for_awq_benchmark",
            "data_type": "float32",
            "num_calib_data": 128,
            "model_export": [ "hf_format" ],
            "exclude_layers": [  ],
            "awq_config": {
                "name": "awq",
                "scaling_layers": [
                    {
                        "prev_op": "input_layernorm",
                        "layers": [ "self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj" ],
                        "inp": "self_attn.q_proj",
                        "module2inspect": "self_attn"
                    },
                    { "prev_op": "self_attn.v_proj", "layers": [ "self_attn.o_proj" ], "inp": "self_attn.o_proj" },
                    {
                        "prev_op": "post_attention_layernorm",
                        "layers": [ "mlp.gate_proj", "mlp.up_proj" ],
                        "inp": "mlp.gate_proj",
                        "module2inspect": "mlp"
                    },
                    { "prev_op": "mlp.up_proj", "layers": [ "mlp.down_proj" ], "inp": "mlp.down_proj" }
                ],
                "model_decoder_layers": "model.layers"
            }
        },
        "mg": { "type": "VitisGenerateModelLLM", "packed_const": false, "cpu_only": false }
    },
    "log_severity_level": 1,
    "output_dir": "models/llama2-vai",
    "cache_dir": "cache",
    "no_artifacts": true
}
