{
    "input_model":{
        "type": "PyTorchModel",
        "config": {
            "hf_config": {
                "model_name": "meta-llama/Llama-2-7b-hf",
                "model_class": "LlamaForCausalLM",
                "task": "text-generation"
            }
        }
    },
    "passes": {
        "exporter": {
            "type": "GenAIModelExporter",
            "config": {
                "precision": "int4"
            }
        },
        "perf_tuning": {
            "type": "OrtPerfTuning",
            "config": {
                "user_script": "user_script.py",
                "dataloader_func": "dataloader_func_for_merged",
                "dataloader_func_kwargs": {
                    "model_id": "meta-llama/Llama-2-7b-hf",
                    "past_seq_length": 0,
                    "seq_length": 8,
                    "max_seq_length": 2048
                },
                "batch_size": 2,
                "io_bind": true
            }
        }
    },
    "engine": {
        "log_severity_level": 0,
        "evaluate_input_model": false,
        "execution_providers": ["CUDAExecutionProvider"],
        "cache_dir": "cache",
        "output_dir": "models/genai"
    }
}
