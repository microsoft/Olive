{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 using QLoRA and Deploy Model with Multiple Adapters\n",
    "\n",
    "In this tutorial, we will fine-tune a llama2 model using QLoRA, optimize it using ONNX Runtime tools, and extract the fine-tuned adapters from the model. \n",
    "The resulting model can be deployed with multiple adapters for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this tutorial, please ensure you already installed olive-ai. Please refer to the [installation guide](https://github.com/microsoft/Olive?tab=readme-ov-file#installation) for more information.\n",
    "\n",
    "### Install Dependencies\n",
    "We will optimize for `CUDAExecutionProvider` so the corresponding `onnxruntime` should also be installed allong with the other dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements-qlora.txt\n",
    "!pip install ipywidgets tabulate\n",
    "!pip install --pre onnxruntime-genai-cuda --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-genai/pypi/simple/\n",
    "# !pip install --pre onnxruntime-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get access to model and fine-tuning dataset\n",
    "\n",
    "Get access to the following resources on Hugging Face Hub:\n",
    "- [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "- [nampdn-ai/tiny-codes](https://huggingface.co/nampdn-ai/tiny-codes)\n",
    "\n",
    "Login to your Hugging Face account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The olive workflow is defined in the [llama2_qlora.json](../../llama2_qlora.json) file. \n",
    "\n",
    "It fine-tunes [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) model using [QLoRA](https://arxiv.org/abs/2305.14314) on a subsection of [nampdn-ai/tiny-codes](https://huggingface.co/nampdn-ai/tiny-codes) to generate python code given a prompt. The fine-tuned model is then optimized using ONNX Runtime Tools.\n",
    "\n",
    "Performs optimization pipeline:\n",
    "- GPU, FP16: *Pytorch Model -> Fine-tuned Pytorch Model -> Onnx Model -> Transformers Optimized Onnx Model fp16 -> Extract Adapters*\n",
    "\n",
    "**Note:**\n",
    "- The code language is set to `Python` but can be changed to other languages in the config file.\n",
    "Supported languages are Python, TypeScript, JavaScript, Ruby, Julia, Rust, C++, Bash, Java, C#, and Go. Refer to the [dataset card](https://huggingface.co/datasets/nampdn-ai/tiny-codes) for more details on the dataset.\n",
    "- The `ExtractAdapters` pass in this workflow extracts the lora adapters from the model and converts them into model inputs. An alternate option is to extract them as external intializers. This can be done by setting `\"make_inputs\": false` in the `ExtractAdapters` pass configuration.\n",
    "\n",
    "Run the worklow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!olive run --config llama2_qlora.json --setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 olive run --config llama2_qlora.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output model files are can be found at:\n",
    "- Model: `models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/model.onnx`\n",
    "- Adapter weights: `models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/adapter_weights.npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pre-existing Adapters\n",
    "\n",
    "Olive provides a standalone script to export the fine-tuned adapters from a pre-existing repository on huggingface hub or your local directory. The adapters must be fine-tuned on the same base model with the same configuration as the model obtained from the previous step. \n",
    "\n",
    "Lets export the adapters from [Mikael110/llama-2-7b-guanaco-qlora](https://huggingface.co/Mikael110/llama-2-7b-guanaco-qlora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see the available options to export-adapters command\n",
    "!olive export-adapters --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!olive export-adapters --adapter_path Mikael110/llama-2-7b-guanaco-qlora --output_path models/exported/guanaco_qlora.npz --pack_weights --dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model with Multiple Adapters\n",
    "\n",
    "We can now deploy the same model with multiple adapters for different tasks by loading the adapter weights independently of the model and providing the relevant weights as input at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/llama-2-7b-hf\"\n",
    "model_path = \"models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract-metadata/gpu-cuda_model/model.onnx\"\n",
    "adapters = {\n",
    "    \"guanaco\": {\n",
    "        \"weights\": \"models/exported/guanaco_qlora.npz\",\n",
    "        \"template\": \"### Human: {prompt} ### Assistant:\"\n",
    "    },\n",
    "    \"tiny-codes\": {\n",
    "        \"weights\": \"models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract-metadata/gpu-cuda_model/adapter_weights.npz\",\n",
    "        \"template\": \"### Question: {prompt} \\n### Answer:\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Generate Loop\n",
    "\n",
    "\n",
    "We implemented an example class `ORTGenerator` in [generator.py](../utils/generator.py) that loads the model and adapters, and generates code given a prompt. If your execution provider supports IO Binding, it is recommended to use it for better performance since the adapter weights will be preallocated in the device memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add the utils directory to the path\n",
    "sys.path.append(str(Path().resolve().parent / \"utils\"))\n",
    "\n",
    "from generator import ORTGenerator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# load the generator\n",
    "generator = ORTGenerator(model_path, tokenizer, execution_provider=\"CUDAExecutionProvider\", device_id=0, adapters=adapters, adapter_mode=\"inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate using Guanaco Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What time is it? ### Assistant: I'm sorry, but as an AI language model, I do not have access to real-time information.\n",
      "\n",
      "However, I can try to estimate the current time based on the context of your question and my knowledge of the current time zone.\n",
      "\n",
      "In general, the current time can vary depending on your location and the time zone you are in.\n",
      "\n",
      "If you would like to know the current time for a specific location, you can try searching for the time zone for that\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What time is it?\"\n",
    "\n",
    "response = generator.generate(prompt, adapter=\"guanaco\", max_gen_len=100, use_io_binding=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate with Tiny Codes Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Calculate the sum of all even numbers in a list. \n",
      "### Answer: Here's some python code which implements this functionality:\n",
      "\n",
      "```python \n",
      "def sum_even_numbers(lst):\n",
      "    \"\"\"\n",
      "    Returns the sum of all even numbers in a list.\n",
      "\n",
      "    Args:\n",
      "        lst (list): List of numbers to sum.\n",
      "\n",
      "    Returns:\n",
      "        int: The sum of all even numbers in the input list.\n",
      "    \"\"\"\n",
      "    even_nums = []\n",
      "    for num in lst:\n",
      "        if num % 2 == 0:\n",
      "            even_nums.append(num)\n",
      "\n",
      "    return sum(even_nums)\n",
      "``` \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Calculate the sum of all even numbers in a list.\"\n",
    "\n",
    "response = generator.generate(prompt, adapter=\"tiny-codes\", max_gen_len=200, use_io_binding=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime generate() API\n",
    "\n",
    "The [ONNX Runtime generate() API](https://github.com/microsoft/onnxruntime-genai) also supports loading multiple adapters for inference. During generation, the adapter weights can be provided as inputs to the model using `GeneratorParam`'s `set_model_input` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# import torch\n",
    "import onnxruntime_genai as og\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, adapter_weights, prompt, template, max_gen_len=100):\n",
    "    params = og.GeneratorParams(model)\n",
    "    # model doesn't have GQA nodes so we can't use the share buffer option\n",
    "    params.set_search_options(max_length=max_gen_len, past_present_share_buffer=False)\n",
    "    params.input_ids = tokenizer.encode(template.format(prompt=prompt))\n",
    "\n",
    "    for k, v in adapter_weights.items():\n",
    "        params.set_model_input(k, v)\n",
    "\n",
    "    # generate the response\n",
    "    output_tokens = model.generate(params)\n",
    "    return tokenizer.decode(output_tokens)\n",
    "\n",
    "model = og.Model(str(Path(model_path).parent))\n",
    "tokenizer = og.Tokenizer(model)\n",
    "\n",
    "# load the adapter weights\n",
    "adapters_weights = {\n",
    "    key: dict(np.load(value[\"weights\"])) for key, value in adapters.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate with Guanaco Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What time is it? ### Assistant: I'm sorry, but as an AI language model, I do not have access to real-time information.\n",
      "\n",
      "However, I can try to estimate the current time based on the context of your question and my knowledge of the current time zone.\n",
      "\n",
      "In general, the current time can vary depending on your location and the time zone you are in.\n",
      "\n",
      "If you would like to know the current time for a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What time is it?\"\n",
    "\n",
    "response = generate(model, tokenizer, adapters_weights[\"guanaco\"], prompt, adapters[\"guanaco\"][\"template\"], max_gen_len=100)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate with Tiny Codes Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Calculate the sum of all even numbers in a list. \n",
      "### Answer: Here's some python code which implements this functionality:\n",
      "\n",
      "```python \n",
      "def sum_even_numbers(lst):\n",
      "    \"\"\"\n",
      "    Returns the sum of all even numbers in a list.\n",
      "\n",
      "    Args:\n",
      "        lst (list): List of numbers to sum.\n",
      "\n",
      "    Returns:\n",
      "        int: The sum of all even numbers in the input list.\n",
      "    \"\"\"\n",
      "    even_nums = []\n",
      "    for num in lst:\n",
      "        if num % 2 == 0:\n",
      "            even_nums.append(num)\n",
      "\n",
      "    return sum(even_nums)\n",
      "``` \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Calculate the sum of all even numbers in a list.\"\n",
    "\n",
    "response = generate(model, tokenizer, adapters_weights[\"tiny-codes\"], prompt, adapters[\"tiny-codes\"][\"template\"], max_gen_len=200)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
