{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 using QLoRA and Deploy Model with Multiple Adapters\n",
    "\n",
    "In this tutorial, we will fine-tune a llama2 model using QLoRA, optimize it using ONNX Runtime tools, and extract the fine-tuned adapters from the model. \n",
    "The resulting model can be deployed with multiple adapters for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this tutorial, please ensure you already installed olive-ai. Please refer to the [installation guide](https://github.com/microsoft/Olive?tab=readme-ov-file#installation) for more information.\n",
    "\n",
    "### Install Dependencies\n",
    "We will optimize for `CUDAExecutionProvider` so the corresponding `onnxruntime` should also be installed allong with the other dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements-qlora.txt\n",
    "!pip install ipywidgets tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get access to model and fine-tuning dataset\n",
    "\n",
    "Get access to the following resources on Hugging Face Hub:\n",
    "- [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "- [nampdn-ai/tiny-codes](https://huggingface.co/nampdn-ai/tiny-codes)\n",
    "\n",
    "Login to your Hugging Face account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The olive workflow is defined in the [llama2_qlora.json](../../llama2_qlora.json) file. \n",
    "\n",
    "It fine-tunes [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) model using [QLoRA](https://arxiv.org/abs/2305.14314) on a subsection of [nampdn-ai/tiny-codes](https://huggingface.co/nampdn-ai/tiny-codes) to generate python code given a prompt. The fine-tuned model is then optimized using ONNX Runtime Tools.\n",
    "\n",
    "Performs optimization pipeline:\n",
    "- GPU, FP16: *Pytorch Model -> Fine-tuned Pytorch Model -> Onnx Model -> Transformers Optimized Onnx Model fp16 -> Extract Adapters*\n",
    "\n",
    "**Note:**\n",
    "- The code language is set to `Python` but can be changed to other languages by changing the `language` field in the config file.\n",
    "Supported languages are Python, TypeScript, JavaScript, Ruby, Julia, Rust, C++, Bash, Java, C#, and Go. Refer to the [dataset card](https://huggingface.co/datasets/nampdn-ai/tiny-codes) for more details on the dataset.\n",
    "- The `ExtractAdapters` pass in this workflow extracts the lora adapters from the model and converts them into model inputs. An alternate option is to extract them as external intializers. This can be done by setting `\"make_inputs\": false` in the `ExtractAdapters` pass configuration.\n",
    "\n",
    "Run the worklow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-25 12:53:57,682] [INFO] [run.py:279:run] test/lib/python3.8/site-packages/olive/olive_config.json\n",
      "[2024-04-25 12:53:57,683] [INFO] [run.py:285:run] Loading run configuration from: llama2_qlora.json\n",
      "[2024-04-25 12:53:58,125] [INFO] [run.py:91:dependency_setup] The following packages are required in the local environment: ['onnxruntime-gpu']\n",
      "[2024-04-25 12:53:58,164] [INFO] [run.py:321:check_local_ort_installation] onnxruntime-gpu is already installed.\n"
     ]
    }
   ],
   "source": [
    "!olive run --config llama2_qlora.json --setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-25 12:59:55,351] [INFO] [run.py:279:run] Loading Olive module configuration from: test/lib/python3.8/site-packages/olive/olive_config.json\n",
      "[2024-04-25 12:59:55,351] [INFO] [run.py:285:run] Loading run configuration from: llama2_qlora.json\n",
      "[2024-04-25 12:59:56,812] [INFO] [accelerator_creator.py:224:create_accelerators] Running workflow on accelerator specs: gpu-cuda\n",
      "[2024-04-25 12:59:56,812] [INFO] [engine.py:107:initialize] Using cache directory: cache\n",
      "[2024-04-25 12:59:56,813] [INFO] [engine.py:263:run] Running Olive on accelerator: gpu-cuda\n",
      "[2024-04-25 12:59:56,813] [INFO] [engine.py:1075:_create_system] Creating target system ...\n",
      "[2024-04-25 12:59:56,813] [INFO] [engine.py:1078:_create_system] Target system created in 0.000093 seconds\n",
      "[2024-04-25 12:59:56,813] [INFO] [engine.py:1087:_create_system] Creating host system ...\n",
      "[2024-04-25 12:59:56,813] [INFO] [engine.py:1090:_create_system] Host system created in 0.000076 seconds\n",
      "[2024-04-25 12:59:56,862] [INFO] [engine.py:325:run_accelerator] Input model evaluation results: {\n",
      "  \"onnx_merged_latency-avg\": 37.44323\n",
      "}\n",
      "[2024-04-25 12:59:56,863] [INFO] [engine.py:330:run_accelerator] Saved evaluation results of input model to models/tiny-codes-qlora/gpu-cuda_input_model_metrics.json\n",
      "[2024-04-25 12:59:56,863] [INFO] [engine.py:865:_run_pass] Running pass qlora:QLoRA\n",
      "[2024-04-25 12:59:57,523] [INFO] [hf_config.py:107:load_hf_model] Loading Huggingface model from meta-llama/Llama-2-7b-hf\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.60s/it]\n",
      "[2024-04-25 13:00:12,820] [INFO] [lora.py:709:smart_tokenizer_and_embedding_resize] Added 1 new tokens to tokenizer and resized model embedding layer.\n",
      "Generating train split: 100%|█| 1632309/1632309 [00:14<00:00, 112795.00 examples\n",
      "Filter: 100%|███████████████| 1632309/1632309 [00:21<00:00, 76882.92 examples/s]\n",
      "Map: 100%|█████████████████████| 129063/129063 [00:25<00:00, 4994.97 examples/s]\n",
      "[2024-04-25 13:01:55,902] [INFO] [lora.py:664:train_and_save_new_model] Running fine-tuning\n",
      "{'loss': 1.3355, 'grad_norm': 0.1982421875, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      " 67%|████████████████████████████▋              | 10/15 [01:01<00:30,  6.12s/it]\n",
      " 98%|██████████████████████████████████████████▎| 63/64 [01:56<00:01,  1.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.085297703742981, 'eval_runtime': 120.0963, 'eval_samples_per_second': 8.526, 'eval_steps_per_second': 0.533, 'epoch': 0.0}\n",
      " 67%|████████████████████████████▋              | 10/15 [03:02<00:30,  6.12s/it]\n",
      "100%|███████████████████████████████████████████| 64/64 [01:58<00:00,  1.88s/it]\u001b[A\n",
      "{'train_runtime': 216.3513, 'train_samples_per_second': 1.109, 'train_steps_per_second': 0.069, 'train_loss': 1.2367729822794595, 'epoch': 0.0}\n",
      "100%|███████████████████████████████████████████| 15/15 [03:36<00:00, 14.42s/it]\n",
      "[2024-04-25 13:05:48,511] [INFO] [engine.py:952:_run_pass] Pass qlora:QLoRA finished in 351.644019 seconds\n",
      "[2024-04-25 13:05:48,517] [INFO] [engine.py:865:_run_pass] Running pass conversion:OnnxConversion\n",
      "[2024-04-25 13:05:48,589] [WARNING] [conversion.py:399:_load_pytorch_model] Bitsandbytes 4bit quantization is not supported for conversion. The quantization config is removed from the model loading args. Use OnnxBnb4Quantization pass after conversion to quantize the model.\n",
      "[2024-04-25 13:05:48,658] [INFO] [hf_config.py:107:load_hf_model] Loading Huggingface model from meta-llama/Llama-2-7b-hf\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.48s/it]\n",
      "[2024-04-25 13:28:10,754] [INFO] [engine.py:952:_run_pass] Pass conversion:OnnxConversion finished in 1342.234356 seconds\n",
      "[2024-04-25 13:28:10,757] [INFO] [engine.py:865:_run_pass] Running pass transformers_optimization:OrtTransformersOptimization\n",
      "Unable to determine if past_sequence_length + sequence_length <= total_sequence_length, treat as equal\n",
      "[2024-04-25 13:34:26,000] [INFO] [engine.py:952:_run_pass] Pass transformers_optimization:OrtTransformersOptimization finished in 375.239734 seconds\n",
      "[2024-04-25 13:34:26,002] [INFO] [engine.py:865:_run_pass] Running pass extract:ExtractAdapters\n",
      "[2024-04-25 13:35:06,168] [INFO] [engine.py:952:_run_pass] Pass extract:ExtractAdapters finished in 24.839953 seconds\n",
      "[2024-04-25 13:35:06,174] [INFO] [engine.py:843:_run_passes] Run model evaluation for the final model...\n",
      "[2024-04-25 13:35:37,035] [INFO] [engine.py:362:run_accelerator] Save footprint to models/tiny-codes-qlora/gpu-cuda_footprints.json.\n",
      "[2024-04-25 13:35:37,039] [INFO] [engine.py:280:run] Run history for gpu-cuda:\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "| model_id                                                                  | parent_model_id                                                           | from_pass                   |   duration_sec | metrics                               |\n",
      "+===========================================================================+===========================================================================+=============================+================+=======================================+\n",
      "| cc026a1e9eb8ef3842cd61d229cbe524                                          |                                                                           |                             |                | {                                     |\n",
      "|                                                                           |                                                                           |                             |                |   \"onnx_merged_latency-avg\": 37.44323 |\n",
      "|                                                                           |                                                                           |                             |                | }                                     |\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "| 1_QLoRA-cc026a1e9eb8ef3842cd61d229cbe524-fbada217619dd1cb52eaeb9812410205 | cc026a1e9eb8ef3842cd61d229cbe524                                          | QLoRA                       |        351.644 |                                       |\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "| 2_OnnxConversion-1-a05534eac4724f42a8f509b5df07c775                       | 1_QLoRA-cc026a1e9eb8ef3842cd61d229cbe524-fbada217619dd1cb52eaeb9812410205 | OnnxConversion              |       1342.23  |                                       |\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "| 3_OrtTransformersOptimization-2-b146ae945480fd90646289242722efbf-gpu-cuda | 2_OnnxConversion-1-a05534eac4724f42a8f509b5df07c775                       | OrtTransformersOptimization |        375.24  |                                       |\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "| 4_ExtractAdapters-3-1480088e998491a7ff11e7d3e07986c7                      | 3_OrtTransformersOptimization-2-b146ae945480fd90646289242722efbf-gpu-cuda | ExtractAdapters             |         24.84  | {                                     |\n",
      "|                                                                           |                                                                           |                             |                |   \"onnx_merged_latency-avg\": 20.6524  |\n",
      "|                                                                           |                                                                           |                             |                | }                                     |\n",
      "+---------------------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------+----------------+---------------------------------------+\n",
      "[2024-04-25 13:35:37,040] [INFO] [engine.py:295:run] No packaging config provided, skip packaging artifacts\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 olive run --config llama2_qlora.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output model files are can be found at:\n",
    "- Model: `models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/model.onnx`\n",
    "- Adapter weights: `models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/adapter_weights.npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pre-existing Adapters\n",
    "\n",
    "Olive provides a standalone script to export the fine-tuned adapters from a pre-existing repository on huggingface hub or your local directory. The adapters must be fine-tuned on the same base model with the same configuration as the model obtained from the previous step. \n",
    "\n",
    "Lets export the adapters from [Mikael110/llama-2-7b-guanaco-qlora](https://huggingface.co/Mikael110/llama-2-7b-guanaco-qlora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see the available options to export-adapters command\n",
    "!olive export-adapters --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!olive export-adapters --adapter_path Mikael110/llama-2-7b-guanaco-qlora --output_path models/exported/guanaco_qlora.npz --pack_weights --dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model with Multiple Adapters\n",
    "\n",
    "We can now deploy the same base model with multiple adapters for different tasks by loading the adapter weights independently of the model and providing the relevant weights as input at inference time.\n",
    "\n",
    "We implemented an example class `ORTGenerator` in [generator.py](../utils/generator.py) that loads the model and adapters, and generates code given a prompt. If your execution provider supports IO Binding, it is recommended to use it for better performance since the adapter weights will be preallocated in the device memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add the utils directory to the path\n",
    "sys.path.append(str(Path().resolve().parent / \"utils\"))\n",
    "\n",
    "from generator import ORTGenerator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"meta-llama/llama-2-7b-hf\"\n",
    "model_path = \"models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/model.onnx\"\n",
    "adapters = {\n",
    "    \"guanaco\": {\n",
    "        \"weights\": \"models/exported/guanaco_qlora.npz\",\n",
    "        \"template\": \"### Human: {prompt} ### Assistant:\"\n",
    "    },\n",
    "    \"tiny-codes\": {\n",
    "        \"weights\": \"models/tiny-codes-qlora/qlora-conversion-transformers_optimization-extract/gpu-cuda_model/adapter_weights.npz\",\n",
    "        \"template\": \"### Question: {prompt} \\n### Answer:\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# load the generator\n",
    "generator = ORTGenerator(model_path, tokenizer, execution_provider=\"CUDAExecutionProvider\", device_id=0, adapters=adapters, adapter_mode=\"inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate using Guanaco Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What time is it? ### Assistant: I'm sorry, but as an AI language model, I do not have access to real-time information.\n",
      "\n",
      "However, I can try to estimate the current time based on the context of your question and my knowledge of the current time zone.\n",
      "\n",
      "In general, the current time can vary depending on your location and the time zone you are in.\n",
      "\n",
      "If you would like to know the current time for a specific location, you can try searching for the time zone for that\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What time is it?\"\n",
    "\n",
    "response = generator.generate(prompt, adapter=\"guanaco\", max_gen_len=100, use_io_binding=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with Tiny Codes Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Calculate the sum of a list of integers. \n",
      "### Answer: Here is some python code that implements this functionality: \n",
      "\n",
      "def sum_list(lst):\n",
      "    \"\"\"\n",
      "    Calculates the sum of a list of integers.\n",
      "    \n",
      "    Args:\n",
      "        lst (list): List of integers to sum.\n",
      "    \n",
      "    Returns:\n",
      "        int: Sum of the list elements.\n",
      "    \"\"\"  \n",
      "    \n",
      "    return sum(lst)\n",
      "    \n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Calculate the sum of a list of integers.\"\n",
    "\n",
    "response = generator.generate(prompt, adapter=\"tiny-codes\", max_gen_len=100, use_io_binding=True)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
