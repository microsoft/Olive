{
    "input_model":{
        "type": "PyTorchModel",
        "config": {
            "model_script": "user_script.py",
            "io_config": "get_merged_decoder_with_past_io_config",
            "dummy_inputs_func": "get_merged_decoder_with_past_dummy_inputs",
            "hf_config": {
                "model_name": "meta-llama/Llama-2-7b-hf",
                "model_class": "LlamaForCausalLM",
                "task": "text-generation"
            }
        }
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "config": {
                "accelerators": ["gpu"]
            }
        }
    },
    "passes": {
        "pipeline_parallel": {
            "type": "PyTorchPipelineParallel",
            "config": {
                "user_script": "pipeline_parallel_llama2.py",
                "class_name": "LlamaPyTorchPipelineParallel",
                "world_size": 2
            }
        }
    },
    "engine": {
        "log_severity_level": 0,
        "search_strategy": false,
        "evaluate_input_model": false,
        "target": {
            "type": "LocalSystem",
            "config": {
                "accelerators": ["gpu"]
            }
        },
        "execution_providers": ["CUDAExecutionProvider"],
        "cache_dir": "cache",
        "output_dir": "models/pipeline_parallel"
    }
}
