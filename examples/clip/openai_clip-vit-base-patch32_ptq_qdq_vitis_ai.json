{
    "input_model": {
        "type": "HfModel",
        "model_path": "openai/clip-vit-base-patch32",
        "task": "zero-shot-image-classification",
        "load_kwargs": { "attn_implementation": "eager" },
        "io_config": {
            "input_names": [ "input_ids", "pixel_values", "attention_mask" ],
            "input_shapes": [ [ 10, 77 ], [ 1, 3, 224, 224 ], [ 10, 77 ] ],
            "input_types": [ "int64", "float32", "int64" ],
            "output_names": [ "logits_per_image" ],
            "output_shapes": [ [ 1, 2 ] ]
        }
    },
    "data_configs": [
        {
            "name": "quant_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": {
                "type": "clip_dataset",
                "model_name": "openai/clip-vit-base-patch16",
                "dataset_name": "nlphuji/flickr30k",
                "start": 0,
                "end": 10
            },
            "dataloader_config": { "type": "no_auto_batch_dataloader" }
        },
        {
            "name": "metric_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": {
                "type": "clip_dataset",
                "model_name": "openai/clip-vit-base-patch16",
                "dataset_name": "nlphuji/flickr30k",
                "start": 10,
                "end": 20
            },
            "dataloader_config": { "type": "no_auto_batch_dataloader" },
            "post_process_data_config": { "type": "clip_post_process" }
        }
    ],
    "evaluators": {
        "common_evaluator": {
            "metrics": [
                {
                    "name": "accuracy",
                    "type": "accuracy",
                    "backend": "huggingface_metrics",
                    "data_config": "metric_data_config",
                    "sub_types": [
                        { "name": "accuracy", "priority": 1, "goal": { "type": "max-degradation", "value": 0.05 } }
                    ]
                },
                {
                    "name": "latency",
                    "type": "latency",
                    "data_config": "metric_data_config",
                    "sub_types": [
                        { "name": "avg", "goal": { "type": "percent-min-improvement", "value": 0.1 } },
                        { "name": "max" },
                        { "name": "min" }
                    ]
                },
                {
                    "name": "throughput",
                    "type": "throughput",
                    "data_config": "metric_data_config",
                    "sub_types": [ { "name": "avg" }, { "name": "max" }, { "name": "min" } ]
                }
            ]
        }
    },
    "passes": {
        "conversion": { "type": "OnnxConversion", "target_opset": 17 },
        "transformer_optimizer": {
            "type": "orttransformersoptimization",
            "model_type": "clip",
            "opt_level": 1,
            "optimization_options": {
                "enable_gelu": true,
                "enable_bias_gelu": false,
                "enable_layer_norm": true,
                "enable_skip_layer_norm": false,
                "enable_bias_skip_layer_norm": false,
                "enable_attention": false
            }
        },
        "quantization": {
            "type": "OnnxStaticQuantization",
            "quant_preprocess": true,
            "data_config": "quant_data_config",
            "op_types_to_quantize": [
                "MatMul",
                "LayerNormalization",
                "Gemm",
                "Sigmoid",
                "QuickGelu",
                "Mul",
                "Conv",
                "Transpose"
            ],
            "activation_type": "QUInt16",
            "weight_type": "QUInt8",
            "calibrate_method": "MinMax"
        }
    },
    "evaluator": "common_evaluator",
    "cache_dir": "cache/clip-32",
    "clean_cache": false,
    "output_dir": "models/clip-vit-base-patch32"
}
