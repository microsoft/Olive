{
    "input_model": {
        "type": "PytorchModel",
        "model_path": "laion/clip-vit-b-32-laion2b-s34b-b79k",
        "generative": false,
        "io_config": {
            "input_names": [ "pixel_values" ],
            "input_shapes": [ [ 1, 3, 224, 224 ] ],
            "output_names": [ "embeds" ]
        },
        "model_loader": "load_image_encoder",
        "model_script": "clip_script.py",
        "script_dir": "."
    },
    "passes": {
        "conversion": { "type": "OnnxConversion", "target_opset": 20, "dynamic": true, "use_dynamo_exporter": false },
        "to_fixed_shape": {
            "type": "DynamicToFixedShape",
            "dim_param": [ "batch_size", "num_channels", "height", "width" ],
            "dim_value": [ 1, 3, 224, 224 ]
        },
        "surgery": { "type": "GraphSurgeries", "surgeries": [ { "surgeon": "MatMulAddToGemm" } ] },
        "transformer_optimizer": {
            "type": "OrtTransformersOptimization",
            "model_type": "vit",
            "opt_level": 1,
            "optimization_options": {
                "enable_gelu": true,
                "enable_bias_gelu": false,
                "enable_layer_norm": true,
                "enable_skip_layer_norm": false,
                "enable_bias_skip_layer_norm": false,
                "enable_attention": false
            }
        },
        "OnnxQuantization": {
            "type": "OnnxQuantization",
            "data_config": "calib_data",
            "quant_preprocess": true,
            "activation_type": "uint16",
            "precision": "uint8"
        },
        "addmetadata": {
            "type": "VitisAIAddMetaData",
            "config_meta_data_keys": [ "architectures", "model_type" ],
            "activation_type": "uint16",
            "weight_type": "uint8",
            "quant_type": "OnnxStaticQuantization"
        }
    },
    "data_configs": [
        {
            "name": "calib_data",
            "type": "HuggingfaceContainer",
            "load_dataset_config": { "data_name": "nlphuji/flickr30k", "split": "test[:12]" },
            "pre_process_data_config": {
                "type": "pre_process_dataset",
                "model_name": "laion/clip-vit-b-32-laion2b-s34b-b79k",
                "image_col": "image"
            },
            "dataloader_config": { "batch_size": 1 },
            "user_script": "clip_script.py"
        },
        {
            "name": "eval_data",
            "type": "HuggingfaceContainer",
            "load_dataset_config": { "data_name": "nlphuji/flickr_1k_test_image_text_retrieval", "split": "test" },
            "pre_process_data_config": {
                "type": "pre_process_dataset",
                "model_name": "laion/clip-vit-b-32-laion2b-s34b-b79k",
                "generate_ground_truth": true,
                "image_col": "image"
            },
            "post_process_data_config": { "type": "embed_post_process" },
            "dataloader_config": { "batch_size": 1 },
            "user_script": "clip_script.py"
        }
    ],
    "evaluators": {
        "sanity_check": {
            "metrics": [
                {
                    "name": "degrad",
                    "type": "custom",
                    "data_config": "eval_data",
                    "sub_types": [ { "name": "percentage", "priority": 1, "higher_is_better": false } ],
                    "user_config": {
                        "user_script": "clip_script.py",
                        "metric_func": "eval_similarity_degrad",
                        "metric_func_kwargs": { "batch_size": 32 }
                    }
                },
                {
                    "name": "latency",
                    "type": "latency",
                    "sub_types": [
                        { "name": "avg", "priority": 2, "metric_config": { "warmup_num": 20, "repeat_test_num": 100 } },
                        { "name": "p90", "metric_config": { "warmup_num": 20, "repeat_test_num": 100 } }
                    ]
                }
            ]
        }
    },
    "clean_cache": true,
    "clean_evaluation_cache": true,
    "evaluate_input_model": false,
    "output_dir": "models/laion/clip_b32/image"
}
