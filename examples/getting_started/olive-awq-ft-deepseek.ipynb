{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv6vx7wooDfk"
   },
   "source": [
    "# üêã Finetune and Optimize DeepSeek-R1-Distill-Qwen-1.5B with Olive\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. Fine-tune [DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) to classify English phrases into Surprise/Joy/Fear/Sadness.\n",
    "1. Optimize the fine-tuned model for the ONNX Runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêç Install Python dependencies\n",
    "\n",
    "The following cells create a pip requirements file and then install the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "olive-ai[finetune, auto-opt]\n",
    "onnxruntime-genai\n",
    "transformers==4.44.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtY3VYxCoDfm"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxJCT5wioDfp"
   },
   "source": [
    "## üèÉ Train the model\n",
    "\n",
    "Fine-tuning language models helps when we desire very specific outputs. In this example, you'll fine-tune üêã DeepSeek-R1-Distill-Qwen-1.5B from the previous cell to respond to an English phrase with a single word answer that classifies the phrases into one of surprise/fear/joy/sadness categories. Here is a sample of the data used for fine-tuning:\n",
    "\n",
    "```jsonl\n",
    "{\"phrase\": \"The sudden thunderstorm caught me off guard.\", \"tone\": \"surprise\"}\n",
    "{\"phrase\": \"The creaking door at night is quite spooky.\", \"tone\": \"fear\"}\n",
    "{\"phrase\": \"Celebrating my birthday with friends is always fun.\", \"tone\": \"joy\"}\n",
    "{\"phrase\": \"Saying goodbye to my pet was heart-wrenching.\", \"tone\": \"sadness\"}\n",
    "```\n",
    "\n",
    "In the following `olive finetune` command the `--data_name` argument is a Hugging Face dataset [xxyyzzz/phrase_classification](https://huggingface.co/datasets/xxyyzzz/phrase_classification). You can also provide your own data from local disk using the `--data_files` argument.\n",
    "\n",
    "> ‚è≥ **It takes ~15mins to complete the Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8t36pRF2oDfq"
   },
   "outputs": [],
   "source": [
    "!olive finetune \\\n",
    "    --method lora \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "    --trust_remote_code \\\n",
    "    --data_name xxyyzzz/phrase_classification \\\n",
    "    --text_template \"<|begin‚ñÅof‚ñÅsentence|><|User|>{phrase}<|Assistant|>{tone}<|end‚ñÅof‚ñÅsentence|>\" \\\n",
    "    --max_steps 300 \\\n",
    "    --output_path models/deepseek/ft \\\n",
    "    --log_level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7woNXLDF0bhh"
   },
   "source": [
    "## ü™Ñ Automatic model optimization with Olive\n",
    "\n",
    "Next, you'll execute Olive's automatic optimizer using the `auto-opt` CLI command, which will:\n",
    "\n",
    "1. Capture the fine-tuned model into an ONNX graph and convert the weights into the ONNX format.\n",
    "1. Optimize the ONNX graph (e.g. fuse nodes, reshape, etc).\n",
    "1. Extract the fine-tuned LoRA weights and place them into a separate file.\n",
    "\n",
    "> ‚è≥**It takes ~2mins for the automatic optimization to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!olive auto-opt \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "    --adapter_path models/deepseek/ft/adapter \\\n",
    "    --device cpu \\\n",
    "    --provider CPUExecutionProvider \\\n",
    "    --use_ort_genai \\\n",
    "    --precision int4 \\\n",
    "    --output_path models/deepseek/onnx-ao \\\n",
    "    --log_level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Uwm432loDfr"
   },
   "source": [
    "## üß† Inference\n",
    "\n",
    "The code below creates a test app that consumes the model in a simple console chat interface. You will be prompted to enter an English phrase (for example: \"Cricket is a wonderful game\") and the app will output a chat completion.\n",
    "\n",
    "Whilst the inference code uses the Python API for the ONNX Runtime, other language bindings are available in [Java, C#, C++](https://github.com/microsoft/onnxruntime-genai/tree/main/examples).\n",
    "\n",
    "To exit the chat interface, enter `exit` or select `Ctrl+c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puMdoAxjoDfr"
   },
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "model_path = \"models/deepseek/onnx-ao/model\"\n",
    "\n",
    "model = og.Model(f'{model_path}')\n",
    "adapters = og.Adapters(model)\n",
    "adapters.load(f'{model_path}/adapter_weights.onnx_adapter', \"classifier\")\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "# Keep asking for input prompts in a loop\n",
    "while True:\n",
    "    phrase = input(\"Phrase: \")\n",
    "    prompt = f\"<|begin‚ñÅof‚ñÅsentence|><|User|>{phrase}<|Assistant|>\"\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # first run without the adapter\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(past_present_share_buffer=False, temperature=0.5)\n",
    "    generator = og.Generator(model, params)\n",
    "    # set the adapter to active for this response\n",
    "    generator.set_active_adapter(adapters, \"classifier\")\n",
    "\n",
    "    generator.append_tokens(input_tokens)\n",
    "\n",
    "    print(\"Output: \", end='', flush=True)\n",
    "\n",
    "    while not generator.is_done():\n",
    "            generator.generate_next_token()\n",
    "            new_token = generator.get_next_tokens()[0]\n",
    "            print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "    print()\n",
    "    print()\n",
    "    del generator"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "genai-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
