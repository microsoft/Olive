{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEfAinRUdxmG"
   },
   "source": [
    "# ‚ú®  OLIVE: Quickstart\n",
    "\n",
    "This notebook shows you how to get started with OLIVE. You will:\n",
    "\n",
    "1. use the `auto-opt` Olive CLI command to optimize an SLM model for the ONNX Runtime (for CPU devices).\n",
    "1. Use the ONNX Runtime Python binding to execute a simple chat interface that consumes the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmPoU34Vdh6v"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install olive-ai[gpu,finetune]\n",
    "!pip install transformers==4.44.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDS_40wg7C1n"
   },
   "source": [
    "## ü§ó Log-in to Hugging Face\n",
    "\n",
    "The Olive automatic optimization command (`auto-opt`) can pull models from Hugging Face, Local disk, or the Azure AI Model Catalog. In this getting started guide, you'll be optimizing [SmolLM-360M from Hugging Face](https://huggingface.co/HuggingFaceTB/SmolLM-360M).\n",
    "\n",
    "> **üìù NOTE**: Follow the [Hugging Face documentation for setting up User Access Tokens](https://huggingface.co/docs/hub/security-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_shiczT_4Hye"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token {TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7D8rEOx7jnv"
   },
   "source": [
    "## ü™Ñ Automatic model optimization with Olive\n",
    "\n",
    "Next you'll run the `auto-opt` command that will automatically download and optimize Llama-3.2-1B-Instruct. After the model is downloaded, Olive will convert it into ONNX format, quantize (`int4`), and optimizing the graph. It takes around 60secs plus model download time (which will depend on your network bandwidth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrAGXFeSd_O5"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "olive auto-opt \\\n",
    "    --model_name_or_path HuggingFaceTB/SmolLM-360M-Instruct \\\n",
    "    --trust_remote_code \\\n",
    "    --output_path optimized-model \\\n",
    "    --device cpu \\\n",
    "    --provider CPUExecutionProvider \\\n",
    "    --precision int4 \\\n",
    "    --use_model_builder True \\\n",
    "    --log_level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTEL8Zwe7xVn"
   },
   "source": [
    "With the `auto-opt` command, you can change the input model to one that is available on Hugging Face - for example, to [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/tree/main) - or a model that resides on local disk. Olive, will go through the same process of *automatically* converting (to ONNX), optimizing the graph and quantizing the weights. The model can be optimized for different providers and devices - for example, you can choose DirectML (for Windows) as the provider and target either the NPU, GPU, or CPU device.\n",
    "\n",
    "## üß† Inference model using ONNX Runtime\n",
    "\n",
    "The ONNX Runtime (ORT) is a fast and light-weight package (available in many programming languages) that runs cross-platform. ORT enables you to infuse your AI models into your applications so that inference is handled *on-device*. The following code creates a simple console-based chat interface that inferences your optimized model.\n",
    "\n",
    "### How to use\n",
    "You'll be prompted to enter a message to the SLM - for example, you could ask *what is the golden ratio*, or *def print_hello_world():*. To exit type *exit* in the chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amGg0c3swnDF"
   },
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_folder = \"optimized-model/model\"\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model = og.Model(model_folder)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "# Set the max length to something sensible by default,\n",
    "# since otherwise it will be set to the entire context length\n",
    "search_options = {}\n",
    "search_options['max_length'] = 200\n",
    "search_options['past_present_share_buffer'] = False\n",
    "\n",
    "chat_template = \"<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "text = input(\"Input: \")\n",
    "\n",
    "# Keep asking for input phrases\n",
    "while text != \"exit\":\n",
    "    if not text:\n",
    "        print(\"Error, input cannot be empty\")\n",
    "        exit\n",
    "\n",
    "    # generate prompt (prompt template + input)\n",
    "    prompt = f'{chat_template.format(input=text)}'\n",
    "\n",
    "    # encode the prompt using the tokenizer\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_search_options(**search_options)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "\n",
    "    print(\"Output: \", end='', flush=True)\n",
    "    # stream the output\n",
    "    try:\n",
    "        while not generator.is_done():\n",
    "            generator.compute_logits()\n",
    "            generator.generate_next_token()\n",
    "\n",
    "            new_token = generator.get_next_tokens()[0]\n",
    "            print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"  --control+c pressed, aborting generation--\")\n",
    "\n",
    "    print()\n",
    "    text = input(\"Input: \")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
