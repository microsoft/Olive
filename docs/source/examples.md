# Examples

|Scenario| Model|Examples|Hardware Targeted Optimization|
|---|-----------|-----------|-----------|
|NLP|deepseek|[Link](https://github.com/microsoft/Olive/tree/main/examples/deepseek)|`QDQ`: QDQ Model with 4-bit Weights & 16-bit Activations<br>`QNN EP`: PTQ + AOT Compilation for Qualcomm NPUs using QNN EP<br>`Vitis AI EP`: PTQ + AOT Compilation for AMD NPUs using Vitis AI EP<br>`OpenVINO EP`: PTQ + AOT Compilation for OpenVINO EP<br>`Intel® NPU`: PTQ + AWQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||llama2|[Link](https://github.com/microsoft/Olive/tree/main/examples/llama2)|`CPU`: with ONNX Runtime optimizations for optimized FP32 ONNX model<br>`CPU`: with ONNX Runtime optimizations for optimized INT8 ONNX model<br>`CPU`: with ONNX Runtime optimizations for optimized INT4 ONNX model<br>`GPU`: with ONNX Runtime optimizations for optimized FP16 ONNX model<br>`GPU`: with ONNX Runtime optimizations for optimized INT4 ONNX model<br>`GPU`: with QLoRA for model fine tune and ONNX Runtime optimizations for optimized ONNX model
||llama3|[Link](https://github.com/microsoft/Olive/tree/main/examples/llama3)|`QDQ`: QDQ Model with 4-bit Weights & 16-bit Activations<br>`QNN EP`: PTQ + AOT Compilation for Qualcomm NPUs using QNN EP<br>`Vitis AI EP`: PTQ + AOT Compilation for AMD NPUs using Vitis AI EP<br>`OpenVINO EP`: PTQ + AOT Compilation for AMD NPUs using OpenVINO EP<br>`Intel® NPU`: PTQ + AWQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||mistral|[Link](https://github.com/microsoft/Olive/tree/main/examples/mistral)|`CPU`: with Optimum conversion and ONNX Runtime optimizations and Intel® Neural Compressor static quantization for optimized INT8 ONNX model<br>`GPU`: with ONNX Runtime optimizations for optimized FP16 ONNX model
||open llama|[Link](https://github.com/microsoft/Olive/tree/main/examples/open_llama)|`GPU`: with Optimum conversion and merging and ONNX Runtime optimizations for optimized ONNX model <br>`GPU`: with SparseGPT and TorchTRT conversion for an optimized PyTorch model with sparsity<br>`CPU`: with Optimum conversion and merging and ONNX Runtime optimizations and Intel® Neural Compressor 4-bits weight-only quantization for optimized INT4 ONNX model
||phi2|[Link](https://github.com/microsoft/Olive/tree/main/examples/phi2)|`CPU`: with ONNX Runtime optimizations fp32/int4<br>`GPU` with ONNX Runtime optimizations fp16/int4, with PyTorch QLoRA for model fine tune<br>`GPU` with SliceGPT for an optimized PyTorch model with sparsity
||phi3|[Link](https://github.com/microsoft/Olive/tree/main/examples/phi3)|`Intel® GPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||phi3.5|[Link](https://github.com/microsoft/Olive/tree/main/examples/phi3_5)|`QDQ`: QDQ Model with 4-bit Weights & 16-bit Activations<br>`QNN EP`: PTQ + AOT Compilation for Qualcomm NPUs using QNN EP<br>`Vitis AI EP`: PTQ + AOT Compilation for AMD NPUs using Vitis AI EP<br>`OpenVINO EP`: PTQ + AOT Compilation for AMD NPUs using OpenVINO EP<br>`Intel® NPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model<br>`Intel® GPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||phi4|[Link](https://github.com/microsoft/Olive/tree/main/examples/phi4)|`Intel® NPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model<br>`Intel® GPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||qwen2.5|[Link](https://github.com/microsoft/Olive/tree/main/examples/qwen2_5)|`QDQ`: QDQ Model with 4-bit Weights & 16-bit Activations<br>`QNN EP`: PTQ + AOT Compilation for Qualcomm NPUs using QNN EP<br>`Vitis AI EP`: PTQ + AOT Compilation for AMD NPUs using Vitis AI EP<br>`OpenVINO EP`: PTQ + AOT Compilation for AMD NPUs using OpenVINO EP<br>`Intel® NPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model<br>`Intel® GPU`: PTQ with 4-bit weight compression using Intel® Optimum OpenVINO for ONNX OpenVINO IR Encapsulated Model
||falcon|[Link](https://github.com/microsoft/Olive/tree/main/examples/falcon)|`GPU`: with ONNX Runtime optimizations for optimized FP16 ONNX model
||red pajama|[Link](https://github.com/microsoft/Olive/tree/main/examples/red_pajama)| `CPU`: with Optimum conversion and merging and ONNX Runtime optimizations for a single optimized ONNX model
||bert|[Link](https://github.com/microsoft/Olive/tree/main/examples/bert)|`CPU`: with ONNX Runtime optimizations and quantization for optimized INT8 ONNX model<br>`CPU`: with ONNX Runtime optimizations and Intel® Neural Compressor quantization for optimized INT8 ONNX model<br>`GPU`: with ONNX Runtime optimizations for CUDA EP<br>`GPU`: with ONNX Runtime optimizations for TRT EP<br>`QNN EP`: with ONNX Runtime optimizations for QNN EP<br>`Vitis AI EP`: with ONNX Runtime optimizations for Vitis AI EP<br>`OpenVINO EP`: with ONNX Runtime optimizations for OpenVINO EP<br>`QDQ`: with ONNX Runtime optimizations and INT8 quantization encoded in QDQ format<br>`Intel® NPU`: PTQ using Intel® NNCF for ONNX OpenVINO IR encapsulated model
||gptj|[Link](https://github.com/microsoft/Olive/tree/main/examples/gptj)|`CPU`: with Intel® Neural Compressor static/dynamic quantization for INT8 ONNX model
||bge|[Link](https://github.com/microsoft/Olive/tree/main/examples/bge)|`NPU`: with ONNX Runtime optimizations for QNN EP
||audio spectrogram<br>transformer|[Link](https://github.com/microsoft/Olive/tree/main/examples/ast)|`CPU`: with ONNX Runtime optimizations and quantization for optimized INT8 ONNX model
|Vision|stable diffusion|[Link](https://github.com/microsoft/Olive/tree/main/examples/stable_diffusion)|`GPU`: with ONNX Runtime optimization for DirectML EP<br>`GPU`: with ONNX Runtime optimization for CUDA EP<br>`Intel CPU`: with OpenVINO toolkit<br>`QDQ`: with ONNX Runtime static Quantization for ONNX INT8 model with QDQ format
||stable diffusion XL|[Link](https://github.com/microsoft/Olive/tree/main/examples/directml/stable_diffusion_xl)|`GPU`: with ONNX Runtime optimizations with DirectML EP<br>`GPU`: with ONNX Runtime optimization for CUDA EP
||squeezenet|[Link](https://github.com/microsoft/Olive/tree/main/examples/directml/squeezenet)|`GPU`: with ONNX Runtime optimizations with DirectML EP
||mobilenet|[Link](https://github.com/microsoft/Olive/tree/main/examples/mobilenet)|`QNN EP`: with ONNX Runtime static QDQ quantization for ONNX Runtime QNN EP
||resnet|[Link](https://github.com/microsoft/Olive/tree/main/examples/resnet)|`CPU`: with ONNX Runtime static/dynamic Quantization for ONNX INT8 model<br>`QDQ`: with ONNX Runtime static Quantization for ONNX INT8 model with QDQ format<br>`AMD DPU`: with AMD Vitis-AI Quantization<br>`QNN EP`: with ONNX Runtime static QDQ quantization for ONNX Runtime QNN EP<br>`Intel® NPU`: PTQ using Intel® NNCF for ONNX OpenVINO IR encapsulated model<br>`Intel® NPU`: PTQ using Intel® NNCF for ONNX model
||Table Transformer Detection|[Link](https://github.com/microsoft/Olive/tree/main/examples/table_transformer_detection)|`QNN EP`: with ONNX Runtime static QDQ quantization for ONNX Runtime QNN EP
