# Notebooks

- [Learn how to quantize & optimize an SLM for the ONNX Runtime using a single Olive ocmmand](olive_quickstart.ipynb)
- [Choose from a list of over 20 popular SLMs to quantize & optimize for the ONNX runtime](text-gen-optimized-slms.ipynb)
- [Learn how to Quantize (using AWQ method), fine-tune, and optimize an SLM for on-device inference](olive-awq-ft-llama.ipynb)
- [Learn hot wo Finetune and Optimize DeepSeek-R1-Distill-QWen-1.5B for on-device inference](olive-deepseek-finetune.ipynb)
- [Fine-tune Llama2 using QLoRA and Deploy Model with Multiple Adapters](llama2_multilora.ipynb)
- [Extract LoRA from finetuned Stable Diffusion model and Deploy Model with Multiple Adapters](sd_multilora/sd_multilora.ipynb)
