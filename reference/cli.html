
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Command Line Tools &#8212; Olive  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/header.css?v=5dcc4e7b" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'reference/cli';</script>
    <script src="../_static/js/custom_version.js?v=3856a39b"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Engine" href="engine.html" />
    <link rel="prev" title="API reference" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="latest" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Olive  documentation</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../why-olive.html">
    Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting-started/getting-started.html">
    Getting started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../how-to/index.html">
    How-to
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" current active">
  <a class="nav-link dropdown-item nav-internal" href="index.html">
    API reference
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../extending/index.html">
    Extending Olive
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/microsoft/Olive" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/olive-ai" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../why-olive.html">
    Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting-started/getting-started.html">
    Getting started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../how-to/index.html">
    How-to
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    API reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extending/index.html">
    Extending Olive
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/microsoft/Olive" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/olive-ai" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">API reference</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Command Line Tools</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="command-line-tools">
<h1>Command Line Tools<a class="headerlink" href="#command-line-tools" title="Link to this heading">#</a></h1>
<p>Olive provides command line tools that can be invoked using the <code class="docutils literal notranslate"><span class="pre">olive</span></code> command.</p>
</section>
<section id="run">
<h1>Run<a class="headerlink" href="#run" title="Link to this heading">#</a></h1>
<p>Run Olive workflow defined in the input .json configuration file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">run</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">--</span><span class="n">run</span><span class="o">-</span><span class="n">config</span> <span class="n">RUN_CONFIG</span> <span class="p">[</span><span class="o">--</span><span class="n">setup</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">packages</span><span class="p">]</span>
                 <span class="p">[</span><span class="o">--</span><span class="n">tempdir</span> <span class="n">TEMPDIR</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">package</span><span class="o">-</span><span class="n">config</span> <span class="n">PACKAGE_CONFIG</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--run-config, --config</kbd></dt>
<dd><p>Path to json config file</p>
</dd>
<dt><kbd>--setup</kbd></dt>
<dd><p>Setup environment needed to run the workflow</p>
<p>Default: False</p>
</dd>
<dt><kbd>--packages</kbd></dt>
<dd><p>List packages required to run the workflow</p>
<p>Default: False</p>
</dd>
<dt><kbd>--tempdir</kbd></dt>
<dd><p>Root directory for tempfile directories and files</p>
</dd>
<dt><kbd>--package-config</kbd></dt>
<dd><p>For advanced users. Path to optional package (json) config file with location of individual pass module implementation and corresponding dependencies. Configuration might also include user owned/proprietary/private pass implementations.</p>
</dd>
</dl>
</section>
</section>
<section id="finetune">
<h1>Finetune<a class="headerlink" href="#finetune" title="Link to this heading">#</a></h1>
<p>Fine-tune a model on a dataset using HuggingFace peft. Huggingface training arguments can be provided along with the defined options.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">finetune</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TASK</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">trust_remote_code</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">method</span> <span class="p">{</span><span class="n">lora</span><span class="p">,</span><span class="n">qlora</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">lora_r</span> <span class="n">LORA_R</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">lora_alpha</span> <span class="n">LORA_ALPHA</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">target_modules</span> <span class="n">TARGET_MODULES</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">torch_dtype</span> <span class="p">{</span><span class="n">bfloat16</span><span class="p">,</span><span class="n">float16</span><span class="p">,</span><span class="n">float32</span><span class="p">}]</span> <span class="o">-</span><span class="n">d</span> <span class="n">DATA_NAME</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">train_subset</span> <span class="n">TRAIN_SUBSET</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">train_split</span> <span class="n">TRAIN_SPLIT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">eval_subset</span> <span class="n">EVAL_SUBSET</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">eval_split</span> <span class="n">EVAL_SPLIT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">data_files</span> <span class="n">DATA_FILES</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">text_field</span> <span class="n">TEXT_FIELD</span> <span class="o">|</span> <span class="o">--</span><span class="n">text_template</span> <span class="n">TEXT_TEMPLATE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">max_seq_len</span> <span class="n">MAX_SEQ_LEN</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">add_special_tokens</span> <span class="n">ADD_SPECIAL_TOKENS</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">max_samples</span> <span class="n">MAX_SAMPLES</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">batch_size</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>-t, --task</kbd></dt>
<dd><p>Task for which the huggingface model is used.</p>
</dd>
<dt><kbd>--trust_remote_code</kbd></dt>
<dd><p>Trust remote code when loading a huggingface model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: finetuned-adapter</p>
</dd>
<dt><kbd>--torch_dtype</kbd></dt>
<dd><p>Possible choices: bfloat16, float16, float32</p>
<p>The torch dtype to use for training.</p>
<p>Default: “bfloat16”</p>
</dd>
<dt><kbd>-d, --data_name</kbd></dt>
<dd><p>The dataset name.</p>
</dd>
<dt><kbd>--train_subset</kbd></dt>
<dd><p>The subset to use for training.</p>
</dd>
<dt><kbd>--train_split</kbd></dt>
<dd><p>The split to use for training.</p>
<p>Default: “train”</p>
</dd>
<dt><kbd>--eval_subset</kbd></dt>
<dd><p>The subset to use for evaluation.</p>
</dd>
<dt><kbd>--eval_split</kbd></dt>
<dd><p>The dataset split to evaluate on.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--data_files</kbd></dt>
<dd><p>The dataset files. If multiple files, separate by comma.</p>
</dd>
<dt><kbd>--text_field</kbd></dt>
<dd><p>The text field to use for fine-tuning.</p>
</dd>
<dt><kbd>--text_template</kbd></dt>
<dd><p>Template to generate text field from. E.g. ‘### Question: {prompt} n### Answer: {response}’</p>
</dd>
<dt><kbd>--max_seq_len</kbd></dt>
<dd><p>Maximum sequence length for the data.</p>
<p>Default: 1024</p>
</dd>
<dt><kbd>--add_special_tokens</kbd></dt>
<dd><p>Whether to add special tokens during preprocessing.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--max_samples</kbd></dt>
<dd><p>Maximum samples to select from the dataset.</p>
<p>Default: 256</p>
</dd>
<dt><kbd>--batch_size</kbd></dt>
<dd><p>Batch size.</p>
<p>Default: 1</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
</dl>
</section>
<section id="lora-options">
<h2>LoRA options<a class="headerlink" href="#lora-options" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--method</kbd></dt>
<dd><p>Possible choices: lora, qlora</p>
<p>The method to use for fine-tuning</p>
<p>Default: “lora”</p>
</dd>
<dt><kbd>--lora_r</kbd></dt>
<dd><p>LoRA R value.</p>
<p>Default: 64</p>
</dd>
<dt><kbd>--lora_alpha</kbd></dt>
<dd><p>LoRA alpha value.</p>
<p>Default: 16</p>
</dd>
<dt><kbd>--target_modules</kbd></dt>
<dd><p>The target modules for LoRA. If multiple, separate by comma.</p>
</dd>
</dl>
</section>
</section>
<section id="auto-optimization">
<h1>Auto-Optimization<a class="headerlink" href="#auto-optimization" title="Link to this heading">#</a></h1>
<p>Automatically optimize the input model for the given target and precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">auto</span><span class="o">-</span><span class="n">opt</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TASK</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">trust_remote_code</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">a</span> <span class="n">ADAPTER_PATH</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">model_script</span> <span class="n">MODEL_SCRIPT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">script_dir</span> <span class="n">SCRIPT_DIR</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">device</span> <span class="p">{</span><span class="n">gpu</span><span class="p">,</span><span class="n">cpu</span><span class="p">,</span><span class="n">npu</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">provider</span> <span class="p">{</span><span class="n">CPUExecutionProvider</span><span class="p">,</span><span class="n">CUDAExecutionProvider</span><span class="p">,</span><span class="n">DmlExecutionProvider</span><span class="p">,</span><span class="n">JsExecutionProvider</span><span class="p">,</span><span class="n">MIGraphXExecutionProvider</span><span class="p">,</span><span class="n">OpenVINOExecutionProvider</span><span class="p">,</span><span class="n">QNNExecutionProvider</span><span class="p">,</span><span class="n">ROCMExecutionProvider</span><span class="p">,</span><span class="n">TensorrtExecutionProvider</span><span class="p">,</span><span class="n">VitisAIExecutionProvider</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">memory</span> <span class="n">MEMORY</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">d</span> <span class="n">DATA_NAME</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">split</span> <span class="n">SPLIT</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">subset</span> <span class="n">SUBSET</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">input_cols</span> <span class="p">[</span><span class="n">INPUT_COLS</span> <span class="o">...</span><span class="p">]]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">batch_size</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">precision</span> <span class="p">{</span><span class="n">fp4</span><span class="p">,</span><span class="n">fp8</span><span class="p">,</span><span class="n">fp16</span><span class="p">,</span><span class="n">fp32</span><span class="p">,</span><span class="n">int4</span><span class="p">,</span><span class="n">int8</span><span class="p">,</span><span class="n">int16</span><span class="p">,</span><span class="n">int32</span><span class="p">,</span><span class="n">nf4</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">use_dynamo_exporter</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">use_model_builder</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">use_qdq_encoding</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">dynamic</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">fixed</span><span class="o">-</span><span class="n">shape</span><span class="o">-</span><span class="n">dim</span><span class="o">-</span><span class="n">param</span> <span class="p">[</span><span class="n">DYNAMIC_TO_FIXED_SHAPE_DIM_PARAM</span> <span class="o">...</span><span class="p">]]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">dynamic</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">fixed</span><span class="o">-</span><span class="n">shape</span><span class="o">-</span><span class="n">dim</span><span class="o">-</span><span class="n">value</span> <span class="p">[</span><span class="n">DYNAMIC_TO_FIXED_SHAPE_DIM_VALUE</span> <span class="o">...</span><span class="p">]]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">splits</span> <span class="n">NUM_SPLITS</span> <span class="o">|</span> <span class="o">--</span><span class="n">cost</span><span class="o">-</span><span class="n">model</span> <span class="n">COST_MODEL</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">mixed</span><span class="o">-</span><span class="n">precision</span><span class="o">-</span><span class="n">overrides</span><span class="o">-</span><span class="n">config</span> <span class="p">[</span><span class="n">MIXED_PRECISION_OVERRIDES_CONFIG</span> <span class="o">...</span><span class="p">]]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">use_ort_genai</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">enable_search</span> <span class="p">[{</span><span class="n">exhaustive</span><span class="p">,</span><span class="n">tpe</span><span class="p">,</span><span class="n">random</span><span class="p">}]]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">seed</span> <span class="n">SEED</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>-t, --task</kbd></dt>
<dd><p>Task for which the huggingface model is used.</p>
</dd>
<dt><kbd>--trust_remote_code</kbd></dt>
<dd><p>Trust remote code when loading a huggingface model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>-a, --adapter_path</kbd></dt>
<dd><p>Path to the adapters weights saved after peft fine-tuning. Local folder or huggingface id.</p>
</dd>
<dt><kbd>--model_script</kbd></dt>
<dd><p>The script file containing the model definition. Required for the local PyTorch model.</p>
</dd>
<dt><kbd>--script_dir</kbd></dt>
<dd><p>The directory containing the local PyTorch model script file.See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information">https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information</a> for more informsation.</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: auto-opt-output</p>
</dd>
<dt><kbd>--device</kbd></dt>
<dd><p>Possible choices: gpu, cpu, npu</p>
<p>Target device to run the model. Default is cpu.</p>
<p>Default: “cpu”</p>
</dd>
<dt><kbd>--provider</kbd></dt>
<dd><p>Possible choices: CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider, JsExecutionProvider, MIGraphXExecutionProvider, OpenVINOExecutionProvider, QNNExecutionProvider, ROCMExecutionProvider, TensorrtExecutionProvider, VitisAIExecutionProvider</p>
<p>Execution provider to use for ONNX model. Default is CPUExecutionProvider.</p>
<p>Default: “CPUExecutionProvider”</p>
</dd>
<dt><kbd>--memory</kbd></dt>
<dd><p>Memory limit for the accelerator in bytes. Default is None.</p>
</dd>
<dt><kbd>-d, --data_name</kbd></dt>
<dd><p>The dataset name.</p>
</dd>
<dt><kbd>--split</kbd></dt>
<dd><p>The dataset split to use for evaluation.</p>
</dd>
<dt><kbd>--subset</kbd></dt>
<dd><p>The dataset subset to use for evaluation.</p>
</dd>
<dt><kbd>--input_cols</kbd></dt>
<dd><p>The input columns to use for evaluation.</p>
</dd>
<dt><kbd>--batch_size</kbd></dt>
<dd><p>Batch size for evaluation.</p>
<p>Default: 1</p>
</dd>
<dt><kbd>--precision</kbd></dt>
<dd><p>Possible choices: fp4, fp8, fp16, fp32, int4, int8, int16, int32, nf4</p>
<p>The output precision of the optimized model. If not specified, the default precision is fp32 for cpu and fp16 for gpu</p>
<p>Default: “fp32”</p>
</dd>
<dt><kbd>--use_dynamo_exporter</kbd></dt>
<dd><p>Whether to use dynamo_export API to export ONNX model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--use_model_builder</kbd></dt>
<dd><p>Whether to use model builder pass for optimization, enable only when the model is supported by model builder</p>
<p>Default: False</p>
</dd>
<dt><kbd>--use_qdq_encoding</kbd></dt>
<dd><p>Whether to use QDQ encoding for quantized operators instead of ONNXRuntime contrib operators like MatMulNBits</p>
<p>Default: False</p>
</dd>
<dt><kbd>--dynamic-to-fixed-shape-dim-param</kbd></dt>
<dd><p>Symbolic parameter names to use for dynamic to fixed shape pass. Required only when using QNNExecutionProvider.</p>
</dd>
<dt><kbd>--dynamic-to-fixed-shape-dim-value</kbd></dt>
<dd><p>Symbolic parameter values to use for dynamic to fixed shape pass. Required only when using QNNExecutionProvider.</p>
</dd>
<dt><kbd>--num-splits</kbd></dt>
<dd><p>Number of splits to use for model splitting. Input model must be an HfModel.</p>
</dd>
<dt><kbd>--cost-model</kbd></dt>
<dd><p>Path to the cost model csv file to use for model splitting. Mutually exclusive with num-splits. Must be a csv with headers <cite>module,num_params,num_bytes</cite> where each row corresponds to the name or a module (with no children), the number of parameters, and the number of bytes the module uses when in the desired precision.</p>
</dd>
<dt><kbd>--mixed-precision-overrides-config</kbd></dt>
<dd><p>Dictionary of name to precision. Has to be even number of entreis with even entries being the keys and odd entries being the values. Required only when output precision is “fp16” and MixedPrecisionOverrides pass is enabled.</p>
</dd>
<dt><kbd>--use_ort_genai</kbd></dt>
<dd><p>Use OnnxRuntime generate() API to run the model</p>
<p>Default: False</p>
</dd>
<dt><kbd>--enable_search</kbd></dt>
<dd><p>Possible choices: exhaustive, tpe, random</p>
<p>Enable search to produce optimal model for the given criteria. Optionally provide search algorithm from available choices. Use exhastive search algorithm by default.</p>
</dd>
<dt><kbd>--seed</kbd></dt>
<dd><p>Random seed for search algorithm</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
</dl>
</section>
</section>
<section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading">#</a></h1>
<p>Quantize PyTorch or ONNX model using various Quantization algorithms.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">quantize</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TASK</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">trust_remote_code</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">a</span> <span class="n">ADAPTER_PATH</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">model_script</span> <span class="n">MODEL_SCRIPT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">script_dir</span> <span class="n">SCRIPT_DIR</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span> <span class="o">--</span><span class="n">algorithm</span> <span class="p">{</span><span class="n">awq</span><span class="p">,</span><span class="n">dynamic</span><span class="p">,</span><span class="n">gptq</span><span class="p">,</span><span class="n">hqq</span><span class="p">,</span><span class="n">rtn</span><span class="p">}</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">precision</span> <span class="p">{</span><span class="n">int4</span><span class="p">,</span><span class="n">int8</span><span class="p">,</span><span class="n">int16</span><span class="p">,</span><span class="n">uint4</span><span class="p">,</span><span class="n">uint8</span><span class="p">,</span><span class="n">uint16</span><span class="p">,</span><span class="n">fp4</span><span class="p">,</span><span class="n">fp8</span><span class="p">,</span><span class="n">fp16</span><span class="p">,</span><span class="n">nf4</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">implementation</span> <span class="p">{</span><span class="n">awq</span><span class="p">,</span><span class="n">bnb4</span><span class="p">,</span><span class="n">gptq</span><span class="p">,</span><span class="n">inc_dynamic</span><span class="p">,</span><span class="n">matmul4</span><span class="p">,</span><span class="n">mnb_to_qdq</span><span class="p">,</span><span class="n">nvmo</span><span class="p">,</span><span class="n">onnx_dynamic</span><span class="p">,</span><span class="n">quarot</span><span class="p">}]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">qdq</span><span class="o">-</span><span class="n">encoding</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">quarot_rotate</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">d</span> <span class="n">DATA_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">subset</span> <span class="n">SUBSET</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">split</span> <span class="n">SPLIT</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">data_files</span> <span class="n">DATA_FILES</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">text_field</span> <span class="n">TEXT_FIELD</span> <span class="o">|</span> <span class="o">--</span><span class="n">text_template</span> <span class="n">TEXT_TEMPLATE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">max_seq_len</span> <span class="n">MAX_SEQ_LEN</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">add_special_tokens</span> <span class="n">ADD_SPECIAL_TOKENS</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">max_samples</span> <span class="n">MAX_SAMPLES</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">batch_size</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
                      <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>-t, --task</kbd></dt>
<dd><p>Task for which the huggingface model is used.</p>
</dd>
<dt><kbd>--trust_remote_code</kbd></dt>
<dd><p>Trust remote code when loading a huggingface model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>-a, --adapter_path</kbd></dt>
<dd><p>Path to the adapters weights saved after peft fine-tuning. Local folder or huggingface id.</p>
</dd>
<dt><kbd>--model_script</kbd></dt>
<dd><p>The script file containing the model definition. Required for the local PyTorch model.</p>
</dd>
<dt><kbd>--script_dir</kbd></dt>
<dd><p>The directory containing the local PyTorch model script file.See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information">https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information</a> for more informsation.</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: quantized-model</p>
</dd>
<dt><kbd>--algorithm</kbd></dt>
<dd><p>Possible choices: awq, dynamic, gptq, hqq, rtn</p>
<p>List of quantization algorithms to run.</p>
</dd>
<dt><kbd>--precision</kbd></dt>
<dd><p>Possible choices: int4, int8, int16, uint4, uint8, uint16, fp4, fp8, fp16, nf4</p>
<p>The precision of the quantized model.</p>
<p>Default: “int4”</p>
</dd>
<dt><kbd>--implementation</kbd></dt>
<dd><p>Possible choices: awq, bnb4, gptq, inc_dynamic, matmul4, mnb_to_qdq, nvmo, onnx_dynamic, quarot</p>
<p>The specific implementation of quantization algorithms to use.</p>
</dd>
<dt><kbd>--enable-qdq-encoding</kbd></dt>
<dd><p>Use QDQ encoding in ONNX model for the quantized nodes.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--quarot_rotate</kbd></dt>
<dd><p>Apply QuaRot/Hadamard rotation to the model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>-d, --data_name</kbd></dt>
<dd><p>The dataset name.</p>
</dd>
<dt><kbd>--subset</kbd></dt>
<dd><p>The subset of the dataset to use.</p>
</dd>
<dt><kbd>--split</kbd></dt>
<dd><p>The dataset split to use.</p>
</dd>
<dt><kbd>--data_files</kbd></dt>
<dd><p>The dataset files. If multiple files, separate by comma.</p>
</dd>
<dt><kbd>--text_field</kbd></dt>
<dd><p>The text field to use for fine-tuning.</p>
</dd>
<dt><kbd>--text_template</kbd></dt>
<dd><p>Template to generate text field from. E.g. ‘### Question: {prompt} n### Answer: {response}’</p>
</dd>
<dt><kbd>--max_seq_len</kbd></dt>
<dd><p>Maximum sequence length for the data.</p>
<p>Default: 1024</p>
</dd>
<dt><kbd>--add_special_tokens</kbd></dt>
<dd><p>Whether to add special tokens during preprocessing.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--max_samples</kbd></dt>
<dd><p>Maximum samples to select from the dataset.</p>
<p>Default: 256</p>
</dd>
<dt><kbd>--batch_size</kbd></dt>
<dd><p>Batch size.</p>
<p>Default: 1</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
</dl>
</section>
</section>
<section id="capture-onnx-graph">
<h1>Capture Onnx Graph<a class="headerlink" href="#capture-onnx-graph" title="Link to this heading">#</a></h1>
<p>Capture ONNX graph using PyTorch Exporter or Model Builder from the Huggingface model or PyTorch model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">capture</span><span class="o">-</span><span class="n">onnx</span><span class="o">-</span><span class="n">graph</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TASK</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">trust_remote_code</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">a</span> <span class="n">ADAPTER_PATH</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">model_script</span> <span class="n">MODEL_SCRIPT</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">script_dir</span> <span class="n">SCRIPT_DIR</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">conversion_device</span> <span class="p">{</span><span class="n">cpu</span><span class="p">,</span><span class="n">gpu</span><span class="p">}]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">use_dynamo_exporter</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">past_key_value_name</span> <span class="n">PAST_KEY_VALUE_NAME</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">torch_dtype</span> <span class="n">TORCH_DTYPE</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">target_opset</span> <span class="n">TARGET_OPSET</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">use_model_builder</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">precision</span> <span class="p">{</span><span class="n">fp16</span><span class="p">,</span><span class="n">fp32</span><span class="p">,</span><span class="n">int4</span><span class="p">}]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">int4_block_size</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">}]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">int4_accuracy_level</span> <span class="n">INT4_ACCURACY_LEVEL</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">exclude_embeds</span> <span class="n">EXCLUDE_EMBEDS</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">exclude_lm_head</span> <span class="n">EXCLUDE_LM_HEAD</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">enable_cuda_graph</span> <span class="n">ENABLE_CUDA_GRAPH</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">use_ort_genai</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>-t, --task</kbd></dt>
<dd><p>Task for which the huggingface model is used.</p>
</dd>
<dt><kbd>--trust_remote_code</kbd></dt>
<dd><p>Trust remote code when loading a huggingface model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>-a, --adapter_path</kbd></dt>
<dd><p>Path to the adapters weights saved after peft fine-tuning. Local folder or huggingface id.</p>
</dd>
<dt><kbd>--model_script</kbd></dt>
<dd><p>The script file containing the model definition. Required for the local PyTorch model.</p>
</dd>
<dt><kbd>--script_dir</kbd></dt>
<dd><p>The directory containing the local PyTorch model script file.See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information">https://microsoft.github.io/Olive/reference/cli.html#model-script-file-information</a> for more informsation.</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: onnx-model</p>
</dd>
<dt><kbd>--conversion_device</kbd></dt>
<dd><p>Possible choices: cpu, gpu</p>
<p>The device used to run the model to capture the ONNX graph.</p>
<p>Default: “cpu”</p>
</dd>
<dt><kbd>--use_ort_genai</kbd></dt>
<dd><p>Use OnnxRuntime generate() API to run the model</p>
<p>Default: False</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
</dl>
</section>
<section id="pytorch-exporter-options">
<h2>PyTorch Exporter options<a class="headerlink" href="#pytorch-exporter-options" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--use_dynamo_exporter</kbd></dt>
<dd><p>Whether to use dynamo_export API to export ONNX model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--past_key_value_name</kbd></dt>
<dd><p>The arguments name to point to past key values. For model loaded from huggingface, it is ‘past_key_values’. Basically, it is used only when <cite>use_dynamo_exporter</cite> is True.</p>
<p>Default: “past_key_values”</p>
</dd>
<dt><kbd>--torch_dtype</kbd></dt>
<dd><p>The dtype to cast the model to before capturing the ONNX graph, e.g., ‘float32’ or ‘float16’. If not specified will use the model as is.</p>
</dd>
<dt><kbd>--target_opset</kbd></dt>
<dd><p>The target opset version for the ONNX model. Default is 17.</p>
<p>Default: 17</p>
</dd>
</dl>
</section>
<section id="model-builder-options">
<h2>Model Builder options<a class="headerlink" href="#model-builder-options" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--use_model_builder</kbd></dt>
<dd><p>Whether to use Model Builder to capture ONNX model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--precision</kbd></dt>
<dd><p>Possible choices: fp16, fp32, int4</p>
<p>The precision of the ONNX model. This is used by Model Builder</p>
<p>Default: “fp16”</p>
</dd>
<dt><kbd>--int4_block_size</kbd></dt>
<dd><p>Possible choices: 16, 32, 64, 128, 256</p>
<p>Specify the block_size for int4 quantization. Acceptable values: 16/32/64/128/256.</p>
</dd>
<dt><kbd>--int4_accuracy_level</kbd></dt>
<dd><p>Specify the minimum accuracy level for activation of MatMul in int4 quantization.</p>
</dd>
<dt><kbd>--exclude_embeds</kbd></dt>
<dd><p>Remove embedding layer from your ONNX model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--exclude_lm_head</kbd></dt>
<dd><p>Remove language modeling head from your ONNX model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--enable_cuda_graph</kbd></dt>
<dd><p>The model can use CUDA graph capture for CUDA execution provider. If enabled, all nodes being placed on the CUDA EP is the prerequisite for the CUDA graph to be used correctly.</p>
</dd>
</dl>
</section>
</section>
<section id="generate-adapters">
<h1>Generate Adapters<a class="headerlink" href="#generate-adapters" title="Link to this heading">#</a></h1>
<p>Generate ONNX model with adapters as inputs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">generate</span><span class="o">-</span><span class="n">adapter</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">adapter_format</span> <span class="p">{</span><span class="n">pt</span><span class="p">,</span><span class="n">numpy</span><span class="p">,</span><span class="n">safetensors</span><span class="p">,</span><span class="n">onnx_adapter</span><span class="p">}]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: optimized-model</p>
</dd>
<dt><kbd>--adapter_format</kbd></dt>
<dd><p>Possible choices: pt, numpy, safetensors, onnx_adapter</p>
<p>Format to save the weights in. Default is onnx_adapter.</p>
<p>Default: “onnx_adapter”</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
</dl>
</section>
</section>
<section id="convert-adapters">
<h1>Convert Adapters<a class="headerlink" href="#convert-adapters" title="Link to this heading">#</a></h1>
<p>Convert LoRA adapter weights to a file that will be consumed by ONNX models generated by Olive ExtractedAdapters pass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">convert</span><span class="o">-</span><span class="n">adapters</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">-</span><span class="n">a</span> <span class="n">ADAPTER_PATH</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">adapter_format</span> <span class="p">{</span><span class="n">pt</span><span class="p">,</span><span class="n">numpy</span><span class="p">,</span><span class="n">safetensors</span><span class="p">,</span><span class="n">onnx_adapter</span><span class="p">}]</span>
                              <span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span> <span class="p">[</span><span class="o">--</span><span class="n">dtype</span> <span class="p">{</span><span class="n">float32</span><span class="p">,</span><span class="n">float16</span><span class="p">}]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">quantize_int4</span><span class="p">]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">int4_block_size</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">}]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">int4_quantization_mode</span> <span class="p">{</span><span class="n">symmetric</span><span class="p">,</span><span class="n">asymmetric</span><span class="p">}]</span>
                              <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-a, --adapter_path</kbd></dt>
<dd><p>Path to the adapters weights saved after peft fine-tuning. Can be a local folder or huggingface id.</p>
</dd>
<dt><kbd>--adapter_format</kbd></dt>
<dd><p>Possible choices: pt, numpy, safetensors, onnx_adapter</p>
<p>Format to save the weights in. Default is onnx_adapter.</p>
<p>Default: “onnx_adapter”</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the exported weights. Will be saved in the <cite>adapter_format</cite> format.</p>
</dd>
<dt><kbd>--dtype</kbd></dt>
<dd><p>Possible choices: float32, float16</p>
<p>Data type to save float adapter weights as. If quantize_int4 is True, this is the data type of the quantization scales. Default is float32.</p>
<p>Default: “float32”</p>
</dd>
<dt><kbd>--quantize_int4</kbd></dt>
<dd><p>Quantize the adapter weights to int4 using blockwise quantization.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--int4_block_size</kbd></dt>
<dd><p>Possible choices: 16, 32, 64, 128, 256</p>
<p>Block size for int4 quantization of adapter weights. Default is 32.</p>
<p>Default: 32</p>
</dd>
<dt><kbd>--int4_quantization_mode</kbd></dt>
<dd><p>Possible choices: symmetric, asymmetric</p>
<p>Quantization mode for int4 quantization of adapter weights. Default is symmetric.</p>
<p>Default: “symmetric”</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
</dl>
</section>
</section>
<section id="tune-onnxruntime-session-params">
<h1>Tune OnnxRuntime Session Params<a class="headerlink" href="#tune-onnxruntime-session-params" title="Link to this heading">#</a></h1>
<p>Automatically tune the OnnxRuntime session parameters for a given onnx model. Currently, for onnx model converted from huggingface model and used for generative tasks, user can simply provide the –model onnx_model_path –hf_model_name hf_model_name –device device_type to get the tuned session parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">tune</span><span class="o">-</span><span class="n">session</span><span class="o">-</span><span class="n">params</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">cpu_cores</span> <span class="n">CPU_CORES</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">io_bind</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">enable_cuda_graph</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">execution_mode_list</span> <span class="p">[</span><span class="n">EXECUTION_MODE_LIST</span> <span class="o">...</span><span class="p">]]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">opt_level_list</span> <span class="p">[</span><span class="n">OPT_LEVEL_LIST</span> <span class="o">...</span><span class="p">]]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">trt_fp16_enable</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">intra_thread_num_list</span> <span class="p">[</span><span class="n">INTRA_THREAD_NUM_LIST</span> <span class="o">...</span><span class="p">]]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">inter_thread_num_list</span> <span class="p">[</span><span class="n">INTER_THREAD_NUM_LIST</span> <span class="o">...</span><span class="p">]]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">extra_session_config</span> <span class="n">EXTRA_SESSION_CONFIG</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">disable_force_evaluate_other_eps</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">enable_profiling</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">predict_with_kv_cache</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">device</span> <span class="p">{</span><span class="n">gpu</span><span class="p">,</span><span class="n">cpu</span><span class="p">,</span><span class="n">npu</span><span class="p">}]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">providers_list</span> <span class="p">[{</span><span class="n">CPUExecutionProvider</span><span class="p">,</span><span class="n">CUDAExecutionProvider</span><span class="p">,</span><span class="n">DmlExecutionProvider</span><span class="p">,</span><span class="n">JsExecutionProvider</span><span class="p">,</span><span class="n">MIGraphXExecutionProvider</span><span class="p">,</span><span class="n">OpenVINOExecutionProvider</span><span class="p">,</span><span class="n">QNNExecutionProvider</span><span class="p">,</span><span class="n">ROCMExecutionProvider</span><span class="p">,</span><span class="n">TensorrtExecutionProvider</span><span class="p">,</span><span class="n">VitisAIExecutionProvider</span><span class="p">}</span> <span class="o">...</span><span class="p">]]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">memory</span> <span class="n">MEMORY</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">keyvault_name</span> <span class="n">KEYVAULT_NAME</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">aml_compute</span> <span class="n">AML_COMPUTE</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">log_level</span> <span class="n">LOG_LEVEL</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">account_name</span> <span class="n">ACCOUNT_NAME</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">container_name</span> <span class="n">CONTAINER_NAME</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: tuned-inference-settings</p>
</dd>
<dt><kbd>--cpu_cores</kbd></dt>
<dd><p>CPU cores used for thread tuning.</p>
</dd>
<dt><kbd>--io_bind</kbd></dt>
<dd><p>Whether enable IOBinding Search for ONNX Runtime inference.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--enable_cuda_graph</kbd></dt>
<dd><p>Whether enable CUDA Graph for CUDA execution provider.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--execution_mode_list</kbd></dt>
<dd><p>Parallelism list between operators.</p>
</dd>
<dt><kbd>--opt_level_list</kbd></dt>
<dd><p>Optimization level list for ONNX Model.</p>
</dd>
<dt><kbd>--trt_fp16_enable</kbd></dt>
<dd><p>Enable TensorRT FP16 mode.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--intra_thread_num_list</kbd></dt>
<dd><p>List of intra thread number for test.</p>
</dd>
<dt><kbd>--inter_thread_num_list</kbd></dt>
<dd><p>List of inter thread number for test.</p>
</dd>
<dt><kbd>--extra_session_config</kbd></dt>
<dd><p>Extra customized session options during tuning process. It should be a json string.E.g. –extra_session_config ‘{“key1”: “value1”, “key2”: “value2”}’</p>
</dd>
<dt><kbd>--disable_force_evaluate_other_eps</kbd></dt>
<dd><p>Whether force to evaluate all execution providers which are different with the associated execution provider.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--enable_profiling</kbd></dt>
<dd><p>Whether enable profiling for ONNX Runtime inference.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--predict_with_kv_cache</kbd></dt>
<dd><p>Whether to use key-value cache for ORT session parameter tuning</p>
<p>Default: False</p>
</dd>
<dt><kbd>--device</kbd></dt>
<dd><p>Possible choices: gpu, cpu, npu</p>
<p>Target device to run the model. Default is cpu.</p>
<p>Default: “cpu”</p>
</dd>
<dt><kbd>--providers_list</kbd></dt>
<dd><p>Possible choices: CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider, JsExecutionProvider, MIGraphXExecutionProvider, OpenVINOExecutionProvider, QNNExecutionProvider, ROCMExecutionProvider, TensorrtExecutionProvider, VitisAIExecutionProvider</p>
<p>List of execution providers to use for ONNX model. They are case sensitive. If not provided, all available providers will be used.</p>
</dd>
<dt><kbd>--memory</kbd></dt>
<dd><p>Memory limit for the accelerator in bytes. Default is None.</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Resource group for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Workspace name for the AzureML workspace to run the workflow remotely.</p>
</dd>
<dt><kbd>--keyvault_name</kbd></dt>
<dd><p>The azureml keyvault name with huggingface token to use for remote run. Refer to <a class="reference external" href="https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login">https://microsoft.github.io/Olive/how-to/configure-workflows/huggingface-integration.html#huggingface-login</a> for more details.</p>
</dd>
<dt><kbd>--aml_compute</kbd></dt>
<dd><p>The compute name to run the workflow on.</p>
</dd>
<dt><kbd>--log_level</kbd></dt>
<dd><p>Logging level. Default is 3. level 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL</p>
<p>Default: 3</p>
</dd>
<dt><kbd>--account_name</kbd></dt>
<dd><p>Azure storage account name for shared cache.</p>
</dd>
<dt><kbd>--container_name</kbd></dt>
<dd><p>Azure storage container name for shared cache.</p>
</dd>
</dl>
</section>
</section>
<section id="generate-cost-model-for-model-splitting">
<h1>Generate Cost Model for Model Splitting<a class="headerlink" href="#generate-cost-model-for-model-splitting" title="Link to this heading">#</a></h1>
<p>Generate a cost model for a given model and save it as a csv file. This cost model is consumed by the CaptureSplitInfo pass. Only supports HfModel.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">generate</span><span class="o">-</span><span class="n">cost</span><span class="o">-</span><span class="n">model</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="n">MODEL_NAME_OR_PATH</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TASK</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">trust_remote_code</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">--</span><span class="n">is_generative_model</span> <span class="n">IS_GENERATIVE_MODEL</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">-</span><span class="n">o</span> <span class="n">OUTPUT_PATH</span><span class="p">]</span>
                                 <span class="p">[</span><span class="o">-</span><span class="n">p</span> <span class="p">{</span><span class="n">fp32</span><span class="p">,</span><span class="n">fp16</span><span class="p">,</span><span class="n">fp8</span><span class="p">,</span><span class="n">int32</span><span class="p">,</span><span class="n">uint32</span><span class="p">,</span><span class="n">int16</span><span class="p">,</span><span class="n">uint16</span><span class="p">,</span><span class="n">int8</span><span class="p">,</span><span class="n">uint8</span><span class="p">,</span><span class="n">int4</span><span class="p">,</span><span class="n">uint4</span><span class="p">,</span><span class="n">nf4</span><span class="p">,</span><span class="n">fp4</span><span class="p">}]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>-m, --model_name_or_path</kbd></dt>
<dd><p>Path to the input model. See <a class="reference external" href="https://microsoft.github.io/Olive/reference/cli.html#providing-input-models">https://microsoft.github.io/Olive/reference/cli.html#providing-input-models</a> for more informsation.</p>
</dd>
<dt><kbd>-t, --task</kbd></dt>
<dd><p>Task for which the huggingface model is used.</p>
</dd>
<dt><kbd>--trust_remote_code</kbd></dt>
<dd><p>Trust remote code when loading a huggingface model.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--is_generative_model</kbd></dt>
<dd><p>Is this a generative model?</p>
<p>Default: True</p>
</dd>
<dt><kbd>-o, --output_path</kbd></dt>
<dd><p>Path to save the command output.</p>
<p>Default: “cost-model.csv”</p>
</dd>
<dt><kbd>-p, --weight_precision</kbd></dt>
<dd><p>Possible choices: fp32, fp16, fp8, int32, uint32, int16, uint16, int8, uint8, int4, uint4, nf4, fp4</p>
<p>Weight precision</p>
<p>Default: “fp16”</p>
</dd>
</dl>
</section>
</section>
<section id="qualcomm-sdk">
<h1>Qualcomm SDK<a class="headerlink" href="#qualcomm-sdk" title="Link to this heading">#</a></h1>
<p>Configure Qualcomm SDK.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">configure</span><span class="o">-</span><span class="n">qualcomm</span><span class="o">-</span><span class="n">sdk</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">--</span><span class="n">py_version</span> <span class="p">{</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">3.8</span><span class="p">}</span> <span class="o">--</span><span class="n">sdk</span>
                                    <span class="p">{</span><span class="n">snpe</span><span class="p">,</span><span class="n">qnn</span><span class="p">}</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--py_version</kbd></dt>
<dd><p>Possible choices: 3.6, 3.8</p>
<p>Python version: Use 3.6 for tensorflow 1.15 and 3.8 otherwise</p>
</dd>
<dt><kbd>--sdk</kbd></dt>
<dd><p>Possible choices: snpe, qnn</p>
<p>Qualcomm SDK: snpe or qnn</p>
</dd>
</dl>
</section>
</section>
<section id="azureml">
<h1>AzureML<a class="headerlink" href="#azureml" title="Link to this heading">#</a></h1>
<p>Manage the AzureML Compute resources.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">manage</span><span class="o">-</span><span class="n">aml</span><span class="o">-</span><span class="n">compute</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">(</span><span class="o">--</span><span class="n">create</span> <span class="o">|</span> <span class="o">--</span><span class="n">delete</span><span class="p">)</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">subscription_id</span> <span class="n">SUBSCRIPTION_ID</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">resource_group</span> <span class="n">RESOURCE_GROUP</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">workspace_name</span> <span class="n">WORKSPACE_NAME</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">aml_config_path</span> <span class="n">AML_CONFIG_PATH</span><span class="p">]</span>
                                <span class="o">--</span><span class="n">compute_name</span> <span class="n">COMPUTE_NAME</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">vm_size</span> <span class="n">VM_SIZE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">location</span> <span class="n">LOCATION</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">min_nodes</span> <span class="n">MIN_NODES</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">max_nodes</span> <span class="n">MAX_NODES</span><span class="p">]</span>
                                <span class="p">[</span><span class="o">--</span><span class="n">idle_time_before_scale_down</span> <span class="n">IDLE_TIME_BEFORE_SCALE_DOWN</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--create, -c</kbd></dt>
<dd><p>Create new compute</p>
<p>Default: False</p>
</dd>
<dt><kbd>--delete, -d</kbd></dt>
<dd><p>Delete existing compute</p>
<p>Default: False</p>
</dd>
<dt><kbd>--subscription_id</kbd></dt>
<dd><p>Azure subscription ID</p>
</dd>
<dt><kbd>--resource_group</kbd></dt>
<dd><p>Name of the Azure resource group</p>
</dd>
<dt><kbd>--workspace_name</kbd></dt>
<dd><p>Name of the AzureML workspace</p>
</dd>
<dt><kbd>--aml_config_path</kbd></dt>
<dd><p>Path to AzureML config file. If provided, subscription_id, resource_group and workspace_name are ignored</p>
</dd>
<dt><kbd>--compute_name</kbd></dt>
<dd><p>Name of the new compute</p>
</dd>
<dt><kbd>--vm_size</kbd></dt>
<dd><p>VM size of the new compute. This is required if you are creating a compute instance</p>
</dd>
<dt><kbd>--location</kbd></dt>
<dd><p>Location of the new compute. This is required if you are creating a compute instance</p>
</dd>
<dt><kbd>--min_nodes</kbd></dt>
<dd><p>Minimum number of nodes</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--max_nodes</kbd></dt>
<dd><p>Maximum number of nodes</p>
<p>Default: 2</p>
</dd>
<dt><kbd>--idle_time_before_scale_down</kbd></dt>
<dd><p>Idle seconds before scaledown</p>
<p>Default: 120</p>
</dd>
</dl>
</section>
</section>
<section id="shared-cache">
<h1>Shared Cache<a class="headerlink" href="#shared-cache" title="Link to this heading">#</a></h1>
<p>Delete Olive model cache stored in the cloud.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">olive</span> <span class="n">shared</span><span class="o">-</span><span class="n">cache</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">delete</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="nb">all</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">y</span><span class="p">]</span> <span class="o">--</span><span class="n">account</span> <span class="n">ACCOUNT</span>
                          <span class="o">--</span><span class="n">container</span> <span class="n">CONTAINER</span> <span class="p">[</span><span class="o">--</span><span class="n">model_hash</span> <span class="n">MODEL_HASH</span><span class="p">]</span>
</pre></div>
</div>
<section id="named-arguments">
<h2>Named Arguments<a class="headerlink" href="#named-arguments" title="Link to this heading">#</a></h2>
<dl class="option-list">
<dt><kbd>--delete</kbd></dt>
<dd><p>Delete a model cache from the shared cache.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--all</kbd></dt>
<dd><p>Delete all model cache from the cloud cache.</p>
<p>Default: False</p>
</dd>
<dt><kbd>-y, --yes</kbd></dt>
<dd><p>Confirm the deletion without prompting for confirmation.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--account</kbd></dt>
<dd><p>The account name for the shared cache.</p>
</dd>
<dt><kbd>--container</kbd></dt>
<dd><p>The container name for the shared cache.</p>
</dd>
<dt><kbd>--model_hash</kbd></dt>
<dd><p>The model hash to remove from the shared cache.</p>
</dd>
</dl>
</section>
</section>
<section id="providing-input-models">
<h1>Providing Input Models<a class="headerlink" href="#providing-input-models" title="Link to this heading">#</a></h1>
<p>There are more than one way to supply input model to the Olive commands.</p>
<ol class="arabic simple">
<li><p>HuggingFace model can be directly used as an input model. For example <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">microsoft/Phi-3-mini-4k-instruct</span></code>.</p></li>
<li><p>A model produced by a Olive command can be directly used as an input model. You can specify the model file path using the <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">&lt;output_model&gt;</span></code> option, where <code class="docutils literal notranslate"><span class="pre">&lt;output_model&gt;</span></code> is the output folder defined by <code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">&lt;output_model&gt;</span></code> in the previous Olive command.</p></li>
<li><p>Olive commands also accept a local PyTorch model as an input model. You can specify the model file path using the <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">model.pt</span></code> option, and the associated model script using the <code class="docutils literal notranslate"><span class="pre">--model_script</span> <span class="pre">script.py</span></code> option. For example, <code class="docutils literal notranslate"><span class="pre">olive</span> <span class="pre">capture-onnx-graph</span> <span class="pre">-m</span> <span class="pre">model.pt</span> <span class="pre">--model_script</span> <span class="pre">script.py</span></code>.</p></li>
<li><p>A model from AzureML registry can be directly used as an input model. For example <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">azureml://registries/&lt;registry_name&gt;/models/&lt;model_name&gt;/versions/&lt;version&gt;</span></code>.</p></li>
<li><p>An ONNX model available locally can also be used as an input for the Olive commands that accept ONNX model as an input.</p></li>
</ol>
</section>
<section id="model-script-file-information">
<h1>Model Script File Information<a class="headerlink" href="#model-script-file-information" title="Link to this heading">#</a></h1>
<p>Olive commands support custom PyTorch model as an input. Olive requires users to define specific functions to load and process the custom PyTorch model.
These functions should be defined in your model script you provide.</p>
<ul>
<li><p><strong>Model Loader Function (`_model_loader`)</strong>:
Loads the PyTorch model. If the model file path is provided using the <cite>-m</cite> option, it takes higher priority than the model loader function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_model_loader</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</li>
<li><p><strong>IO Config Function (`_io_config`)</strong>:
Returns the IO configuration for the model. Either <cite>_io_config</cite> or <cite>_dummy_inputs</cite> is required for the <cite>capture-onnx-graph</cite> CLI command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_io_config</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">PyTorchModelHandler</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">io_config</span>
</pre></div>
</div>
</li>
<li><p><strong>Dummy Inputs Function (`_dummy_inputs`)</strong>:
Provides dummy input tensors for the model. Either <cite>_io_config</cite> or <cite>_dummy_inputs</cite> is required for the <cite>capture-onnx-graph</cite> CLI command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_dummy_inputs</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">PyTorchModelHandler</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">dummy_inputs</span>
</pre></div>
</div>
</li>
<li><p><strong>Model Format Function (`_model_file_format`)</strong>:
Specifies the format of the model. The default value is <cite>PyTorch.EntireModel</cite>. For more available options, refer to <a class="reference external" href="https://github.com/microsoft/Olive/blob/main/olive/constants.py#L23-L26">this</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_model_file_format</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">model_file_format</span>
</pre></div>
</div>
</li>
</ul>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">API reference</p>
      </div>
    </a>
    <a class="right-next"
       href="engine.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Engine</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Command Line Tools</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#run">Run</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#finetune">Finetune</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-options">LoRA options</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-optimization">Auto-Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization">Quantization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#capture-onnx-graph">Capture Onnx Graph</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-exporter-options">PyTorch Exporter options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-builder-options">Model Builder options</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-adapters">Generate Adapters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-adapters">Convert Adapters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-onnxruntime-session-params">Tune OnnxRuntime Session Params</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-cost-model-for-model-splitting">Generate Cost Model for Model Splitting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#qualcomm-sdk">Qualcomm SDK</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#azureml">AzureML</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-cache">Shared Cache</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#named-arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#providing-input-models">Providing Input Models</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-script-file-information">Model Script File Information</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2023-2025, Olive Dev team.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.0.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>